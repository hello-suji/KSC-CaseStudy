{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mjpe7STyxkS"
      },
      "source": [
        "# KSC: LLM-Driven **Composite** Test Generation (GPT‚Äë4o, Python)\n",
        "**Repo:** `temporalio/money-transfer-project-template-python`  \n",
        "**LLM:** OpenAI GPT‚Äë4o  \n",
        "**Notebook Î™©Ï†Å:** *Î∂ÑÍ∏∞ ÏùòÎ¨¥ + Def‚ÄëUse Ï≤¥Ïù∏ + ÏòàÏô∏ Í≤ΩÎ°ú*Î•º ÌÜµÌï©Ìïú ÌÖåÏä§Ìä∏Î•º ÏûêÎèô ÏÉùÏÑ±¬∑Ïã§Ìñâ¬∑Ï¶ùÎ∂ÑÌï©ÎãàÎã§."
      ],
      "id": "0mjpe7STyxkS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-0) Îü∞ÌÉÄÏûÑ & ÏùòÏ°¥ÏÑ± Ï§ÄÎπÑ\n",
        "import os, sys, subprocess, pathlib, getpass, shutil\n",
        "\n",
        "# 1) Í∏∞Î≥∏ ÏÉÅÏàò/Í≤ΩÎ°ú\n",
        "REPO_URL = \"https://github.com/temporalio/money-transfer-project-template-python.git\"\n",
        "PROJECT_NAME = REPO_URL.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "ROOT = pathlib.Path(\".\").resolve()\n",
        "PROJ = ROOT / PROJECT_NAME\n",
        "\n",
        "def sh(cmd: str, check: bool = True, cwd: pathlib.Path | None = None) -> None:\n",
        "    print(\"> \", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True, cwd=str(cwd) if cwd else None)\n",
        "    if check and rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "\n",
        "print(f\"Python {sys.version}\")\n",
        "print(\"ROOT:\", ROOT)\n",
        "\n",
        "# 2) Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏\n",
        "if PROJ.exists():\n",
        "    print(f\"Repo exists at {PROJ}. Pulling latest‚Ä¶\")\n",
        "    sh(f\"git -C {PROJ} fetch --all --prune\")\n",
        "    sh(f\"git -C {PROJ} reset --hard origin/main\")\n",
        "else:\n",
        "    sh(f\"git clone --depth=1 {REPO_URL}\")\n",
        "print(\"‚úÖ Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å\")\n",
        "\n",
        "# 3) pip ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\n",
        "def pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs\n",
        "    print(\"> \", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "pkgs_core = [\n",
        "    \"pytest==7.4.4\", \"pytest-asyncio==0.23.7\", \"pytest-mock==3.11.1\", \"pytest-cov==4.1.0\",\n",
        "    \"coverage[toml]==7.6.1\",\n",
        "    \"temporalio==1.7.0\",\n",
        "    \"lxml==5.2.2\",\n",
        "    \"asttokens==2.4.1\", \"libcst==1.4.0\", \"networkx==3.3\",\n",
        "    \"rich==13.7.1\", \"pyyaml==6.0.2\",\n",
        "]\n",
        "if sys.version_info < (3,11):\n",
        "    pkgs_core.append(\"tomli\")\n",
        "\n",
        "pkgs_llm = [\n",
        "    \"openai==1.43.0\",\n",
        "    \"httpx==0.27.2\",\n",
        "    \"backoff==2.2.1\",\n",
        "    \"tiktoken==0.7.0\",\n",
        "    \"aiolimiter==1.1.0\",\n",
        "    \"anyio==4.4.0\",\n",
        "    \"nest_asyncio==1.6.0\",\n",
        "    \"python-dotenv==1.0.1\",\n",
        "    \"tqdm==4.66.5\",\n",
        "]\n",
        "\n",
        "pip_install([\"pip\"])\n",
        "pip_install(pkgs_core)\n",
        "pip_install(pkgs_llm)\n",
        "print(\"‚úÖ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÏôÑÎ£å\")\n",
        "\n",
        "# 4) .env ÏÉùÏÑ±\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except:\n",
        "    pip_install([\"python-dotenv>=1.0.1\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "openai_api_key = getpass.getpass(\"Enter OPENAI_API_KEY (ÌïÑÏàò): \").strip()\n",
        "env_path = ROOT / \".env\"\n",
        "env_path.write_text(f\"OPENAI_API_KEY={openai_api_key}\\n\", encoding=\"utf-8\")\n",
        "load_dotenv(env_path)\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "print(\"‚úÖ .env Ï†ÄÏû• Î∞è ÌôòÍ≤Ω Ï£ºÏûÖ ÏôÑÎ£å\")\n",
        "\n",
        "# 5) PYTHONPATH ÏÑ§Ï†ï\n",
        "os.environ[\"PROJECT_PATH\"] = str(PROJ)\n",
        "py_paths = [str(PROJ)]\n",
        "if (PROJ / \"src\").exists():\n",
        "    py_paths.insert(0, str(PROJ / \"src\"))\n",
        "prev_pp = os.environ.get(\"PYTHONPATH\",\"\")\n",
        "os.environ[\"PYTHONPATH\"] = \":\".join(py_paths + ([prev_pp] if prev_pp else []))\n",
        "\n",
        "print(\"‚úÖ PYTHONPATH =\", os.environ[\"PYTHONPATH\"])\n",
        "\n",
        "# 6) AÏïà: coverage source Ï†ÑÏ≤¥(.) Í≥†Ï†ïÌïòÏó¨ .coveragerc ÏÉùÏÑ±\n",
        "coveragerc_text = \"\"\"\n",
        "[run]\n",
        "branch = True\n",
        "source =\n",
        "    .\n",
        "omit =\n",
        "    tests/*\n",
        "    generated_tests/*\n",
        "    run_artifacts/*\n",
        "    htmlcov/*\n",
        "    .venv/*\n",
        "    */site-packages/*\n",
        "\n",
        "[report]\n",
        "show_missing = True\n",
        "skip_covered = True\n",
        "\"\"\"\n",
        "(PROJ / \".coveragerc\").write_text(coveragerc_text.strip() + \"\\n\", encoding=\"utf-8\")\n",
        "os.environ[\"COVERAGE_RCFILE\"] = str(PROJ / \".coveragerc\")\n",
        "\n",
        "print(\"‚úÖ .coveragerc ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(\"  - source = . (ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥)\")\n",
        "print(\"  - omit   = tests/, generated_tests/, run_artifacts/, htmlcov/, .venv, site-packages\")\n",
        "\n",
        "# 7) Í≤∞Í≥º ÎîîÎ†âÌÑ∞Î¶¨ Ï§ÄÎπÑ\n",
        "for d in [\"run_artifacts\", \"htmlcov\", \"reports\"]:\n",
        "    (PROJ / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ ÏôÑÎ£å\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJGvcId9bzF6",
        "outputId": "cf7cd243-d950-40a5-8c85-f8fc53e02405"
      },
      "id": "yJGvcId9bzF6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "ROOT: /content\n",
            ">  git clone --depth=1 https://github.com/temporalio/money-transfer-project-template-python.git\n",
            "‚úÖ Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å\n",
            ">  /usr/bin/python3 -m pip install -U pip\n",
            ">  /usr/bin/python3 -m pip install -U pytest==7.4.4 pytest-asyncio==0.23.7 pytest-mock==3.11.1 pytest-cov==4.1.0 coverage[toml]==7.6.1 temporalio==1.7.0 lxml==5.2.2 asttokens==2.4.1 libcst==1.4.0 networkx==3.3 rich==13.7.1 pyyaml==6.0.2\n",
            ">  /usr/bin/python3 -m pip install -U openai==1.43.0 httpx==0.27.2 backoff==2.2.1 tiktoken==0.7.0 aiolimiter==1.1.0 anyio==4.4.0 nest_asyncio==1.6.0 python-dotenv==1.0.1 tqdm==4.66.5\n",
            "‚úÖ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÏôÑÎ£å\n",
            "Enter OPENAI_API_KEY (ÌïÑÏàò): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ .env Ï†ÄÏû• Î∞è ÌôòÍ≤Ω Ï£ºÏûÖ ÏôÑÎ£å\n",
            "‚úÖ PYTHONPATH = /content/money-transfer-project-template-python:/env/python\n",
            "‚úÖ .coveragerc ÏÉùÏÑ± ÏôÑÎ£å\n",
            "  - source = . (ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥)\n",
            "  - omit   = tests/, generated_tests/, run_artifacts/, htmlcov/, .venv, site-packages\n",
            "‚úÖ Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-1) Í∏∞Ï§ÄÏÑ† Ï∏°Ï†ï (Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Î∞ò)\n",
        "\n",
        "import os, sys, json, subprocess, shutil, re\n",
        "from pathlib import Path\n",
        "from lxml import etree\n",
        "\n",
        "# ====== 0) Í≤ΩÎ°ú/ÌôòÍ≤Ω ======\n",
        "assert \"PROJ\" in globals(), \"3-0 Îã®Í≥ÑÍ∞Ä Î®ºÏ†Ä Ïã§ÌñâÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "HTML_DIR = PROJ / \"htmlcov\"\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "\n",
        "def sh(cmd, check=False):\n",
        "    print(\"> \", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True, cwd=str(PROJ))\n",
        "    if check and rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "    return rc\n",
        "\n",
        "def norm(fp):\n",
        "    p = Path(fp)\n",
        "    if not p.is_absolute():\n",
        "        p = (PROJ / p).resolve()\n",
        "    return str(p)\n",
        "\n",
        "def in_proj(abs_path):\n",
        "    return str(PROJ) in str(Path(abs_path).resolve())\n",
        "\n",
        "# ====== 1) baseline ÌÖåÏä§Ìä∏ Ïã§Ìñâ ======\n",
        "print(\"üß™ BASELINE ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë‚Ä¶\")\n",
        "sh(\"coverage erase\" + rc_opt)\n",
        "sh(f\"{sys.executable} -m coverage run{rc_opt} -m pytest -q\")\n",
        "sh(f\"coverage json -o coverage_base.json{rc_opt}\")\n",
        "sh(f\"coverage xml  -o coverage_base.xml{rc_opt}\")\n",
        "sh(\"coverage html\" + rc_opt)\n",
        "\n",
        "# Î≥µÏÇ¨\n",
        "json_path = PROJ / \"coverage_base.json\"\n",
        "xml_path  = PROJ / \"coverage_base.xml\"\n",
        "shutil.copy2(json_path, ART_DIR / \"coverage_base.json\")\n",
        "shutil.copy2(xml_path, ART_DIR / \"coverage_base.xml\")\n",
        "\n",
        "# ====== 2) baseline JSON ÌååÏã± ======\n",
        "cov = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "files = cov.get(\"files\", {}) or {}\n",
        "\n",
        "# Ïã§Ìñâ Í∞ÄÎä• ÎùºÏù∏ Í≥ÑÏÇ∞ = executed + missing\n",
        "def sum_len(key):\n",
        "    return sum(len((files.get(f, {}) or {}).get(key, []) or []) for f in files)\n",
        "\n",
        "base_exec = sum_len(\"executed_lines\")\n",
        "base_miss = sum_len(\"missing_lines\")\n",
        "total_executable_lines = base_exec + base_miss\n",
        "\n",
        "# ====== 3) branch coverage (coverage XML ‚Üí arc Í∏∞Î∞ò) ======\n",
        "full = half = zero = 0\n",
        "observed_outcomes = {}\n",
        "\n",
        "xml_root = etree.parse(str(xml_path)).getroot()\n",
        "for cls in xml_root.findall(\".//class\"):\n",
        "    filename = cls.get(\"filename\")\n",
        "    abs_f = norm(filename)\n",
        "    if not in_proj(abs_f):\n",
        "        continue\n",
        "\n",
        "    for line in cls.findall(\"./lines/line\"):\n",
        "        if line.get(\"branch\") != \"true\":\n",
        "            continue\n",
        "\n",
        "        num = int(line.get(\"number\"))\n",
        "        cond = line.get(\"condition-coverage\")\n",
        "        m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cond) if cond else None\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        covered, total = int(m.group(1)), int(m.group(2))\n",
        "        observed_outcomes.setdefault(abs_f, {})[num] = {\n",
        "            \"covered\": covered,\n",
        "            \"total\": total,\n",
        "            \"ratio\": round(covered / total, 3)\n",
        "        }\n",
        "\n",
        "        if covered == 0:\n",
        "            zero += 1\n",
        "        elif covered == total:\n",
        "            full += 1\n",
        "        else:\n",
        "            half += 1\n",
        "\n",
        "# Ïã§Ìñâ Í∞ÄÎä•Ìïú branch Ïàò\n",
        "branch_points = full + half + zero\n",
        "\n",
        "# ====== 4) uncovered_map ÏÉùÏÑ± ======\n",
        "uncovered_map = {}\n",
        "for f, finfo in files.items():\n",
        "    abs_f = norm(f)\n",
        "    if not in_proj(abs_f):\n",
        "        continue\n",
        "    if any(seg in abs_f for seg in [\"tests/\", \"generated_tests/\", \"run_artifacts/\", \"htmlcov/\"]):\n",
        "        continue\n",
        "\n",
        "    miss = finfo.get(\"missing_lines\", []) or []\n",
        "    if miss:\n",
        "        uncovered_map[abs_f] = sorted(set(miss))\n",
        "\n",
        "(ART_DIR / \"uncovered_map_base.json\").write_text(\n",
        "    json.dumps(uncovered_map, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "(ART_DIR / \"observed_outcomes_base.json\").write_text(\n",
        "    json.dumps(observed_outcomes, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ====== 5) baseline_info Ï†ÄÏû• (Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Ï§Ä) ======\n",
        "baseline_info = {\n",
        "    \"total_executable_lines\": total_executable_lines,\n",
        "    \"total_executable_branches\": branch_points,\n",
        "    \"baseline_executed_lines\": base_exec,\n",
        "    \"baseline_missing_lines\": base_miss,\n",
        "    \"baseline_branch_full\": full,\n",
        "    \"baseline_branch_half\": half,\n",
        "    \"baseline_branch_zero\": zero,\n",
        "}\n",
        "\n",
        "(ART_DIR / \"baseline_info.json\").write_text(\n",
        "    json.dumps(baseline_info, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ====== 6) Ï∂úÎ†• ======\n",
        "print(\"‚úÖ Í∏∞Ï§ÄÏÑ†(3-1) ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(\" - uncovered_map_base.json ÏÉùÏÑ±Îê®\")\n",
        "print(\" - observed_outcomes_base.json ÏÉùÏÑ±Îê®\")\n",
        "print(\" - baseline_info.json ÏÉùÏÑ±Îê®\")\n",
        "\n",
        "print(\"\\nüìò [Baseline Summary]\")\n",
        "print(f\" ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏Ïàò      : {total_executable_lines}\")\n",
        "print(f\" ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú Î∂ÑÍ∏∞ Ïàò     : {branch_points}\")\n",
        "\n",
        "print(\"\\nüìä [Baseline Coverage]\")\n",
        "print(f\" ‚Ä¢ executed={base_exec}, missing={base_miss}\")\n",
        "pct = round((base_exec / (base_exec + base_miss)) * 100, 2) if (base_exec + base_miss) else 0\n",
        "print(f\"   ‚Üí ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : {pct}%\")\n",
        "\n",
        "print(\"\\nüîÄ [Baseline Branch Coverage]\")\n",
        "print(f\" ‚Ä¢ full-hit  : {full}\")\n",
        "print(f\" ‚Ä¢ half-hit  : {half}\")\n",
        "print(f\" ‚Ä¢ zero-hit  : {zero}\")\n",
        "pct_b = round((full / branch_points) * 100, 2) if branch_points else 0\n",
        "print(f\"   ‚Üí Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : {pct_b}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHp7XI6xd1DG",
        "outputId": "cc095b03-28cb-4d0a-b2cc-d7488240ac5c"
      },
      "id": "AHp7XI6xd1DG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ BASELINE ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë‚Ä¶\n",
            ">  coverage erase --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  /usr/bin/python3 -m coverage run --rcfile /content/money-transfer-project-template-python/.coveragerc -m pytest -q\n",
            ">  coverage json -o coverage_base.json --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  coverage xml  -o coverage_base.xml --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  coverage html --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            "‚úÖ Í∏∞Ï§ÄÏÑ†(3-1) ÏÉùÏÑ± ÏôÑÎ£å\n",
            " - uncovered_map_base.json ÏÉùÏÑ±Îê®\n",
            " - observed_outcomes_base.json ÏÉùÏÑ±Îê®\n",
            " - baseline_info.json ÏÉùÏÑ±Îê®\n",
            "\n",
            "üìò [Baseline Summary]\n",
            " ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏Ïàò      : 156\n",
            " ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú Î∂ÑÍ∏∞ Ïàò     : 20\n",
            "\n",
            "üìä [Baseline Coverage]\n",
            " ‚Ä¢ executed=102, missing=54\n",
            "   ‚Üí ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : 65.38%\n",
            "\n",
            "üîÄ [Baseline Branch Coverage]\n",
            " ‚Ä¢ full-hit  : 16\n",
            " ‚Ä¢ half-hit  : 0\n",
            " ‚Ä¢ zero-hit  : 4\n",
            "   ‚Üí Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : 80.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2a) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌîÑÎ£®Îãù ‚Äì ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ (Í∏∞Ï§ÄÏÑ† ÎåÄÎπÑ Í≥†Ïú† Œîcoverage + ÌÉêÏöïÏ†Å ÏÑ†ÌÉù; ÏàòÏßëÌïÑÌÑ∞/Ìè¥Î∞± Í∞ïÌôî)\n",
        "import subprocess, json, os, shutil, re, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- 0. Í≤ΩÎ°ú/Ï†ÑÏ†ú ----------\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌï¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "SRC_TESTS = PROJ / \"tests\"\n",
        "DST_PRUNED = PROJ / \"Pruned_Base_Tests\"\n",
        "DST_PRUNED.mkdir(exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "assert RCFILE.exists(), \".coveragercÍ∞Ä ÏóÜÏäµÎãàÎã§. 3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "rc_opt = f\"--rcfile={RCFILE}\"\n",
        "\n",
        "BASE_JSON = ART_DIR / \"coverage_base.json\"\n",
        "assert BASE_JSON.exists(), \"Í∏∞Ï§ÄÏÑ† coverage_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "# ---------- 1. Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ ÏßëÌï© ----------\n",
        "def load_executed_lines_from_json(json_path: Path) -> set[str]:\n",
        "    if not json_path.exists():\n",
        "        return set()\n",
        "    data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "    lines = set()\n",
        "    for f, finfo in (data.get(\"files\", {}) or {}).items():\n",
        "        for ln in (finfo.get(\"executed_lines\", []) or []):\n",
        "            lines.add(f\"{f}:{ln}\")\n",
        "    return lines\n",
        "\n",
        "baseline_lines = load_executed_lines_from_json(BASE_JSON)\n",
        "print(f\"üìä Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ Ïàò: {len(baseline_lines)}\")\n",
        "\n",
        "# ---------- 2. ÌÖåÏä§Ìä∏ Î™©Î°ù ÏàòÏßë (nodeid) ----------\n",
        "print(\"üìã pytest ÌÖåÏä§Ìä∏ ÏàòÏßë Ï§ë (--collect-only)...\")\n",
        "collect_out = subprocess.check_output(\n",
        "    [sys.executable, \"-m\", \"pytest\", \"--collect-only\", \"-q\"],\n",
        "    cwd=PROJ,\n",
        "    stderr=subprocess.STDOUT,\n",
        ").decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# nodeidÎ°ú Í∞ÑÏ£º: \"path::...\" Ìå®ÌÑ¥Îßå ÌóàÏö© (ÏïàÎÇ¥Î¨∏/Ï¥ùÍ≥Ñ ÎùºÏù∏ ÌïÑÌÑ∞)\n",
        "nodeid_pat = re.compile(r\".+::.+\")\n",
        "test_items = [ln.strip() for ln in collect_out.splitlines()\n",
        "              if nodeid_pat.match(ln.strip())]\n",
        "print(f\"Ï¥ù ÏàòÏßëÎêú ÌÖåÏä§Ìä∏ Ïàò: {len(test_items)}\")\n",
        "\n",
        "if not test_items:\n",
        "    print(\"‚ÑπÔ∏è ÏàòÏßëÎêú ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§. ÌîÑÎ£®ÎãùÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
        "    (ART_DIR / \"pruning_summary.json\").write_text(\n",
        "        json.dumps({\n",
        "            \"total_tests\": 0, \"retained\": 0, \"removed\": 0,\n",
        "            \"retained_tests\": [], \"removed_tests\": [],\n",
        "        }, indent=2, ensure_ascii=False),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "else:\n",
        "    # ---------- 3. Í∞úÎ≥Ñ ÌÖåÏä§Ìä∏ Ïã§Ìñâ ‚Üí Ïã§Ìñâ ÎùºÏù∏ ÏßëÌï© ----------\n",
        "    def get_executed_lines_tmp() -> set[str]:\n",
        "        tmp_json = PROJ / \"coverage_tmp.json\"\n",
        "        return load_executed_lines_from_json(tmp_json)\n",
        "\n",
        "    def run_test_item(item: str) -> tuple[bool, set[str]]:\n",
        "        subprocess.run(f\"coverage erase {rc_opt}\", shell=True, cwd=PROJ)\n",
        "        rc = subprocess.call(\n",
        "            f\"{sys.executable} -m coverage run {rc_opt} -m pytest -q {item}\",\n",
        "            shell=True, cwd=PROJ,\n",
        "        )\n",
        "        subprocess.run(f\"coverage json -o coverage_tmp.json {rc_opt}\", shell=True, cwd=PROJ)\n",
        "        return (rc == 0), get_executed_lines_tmp()\n",
        "\n",
        "    per_test_exec: list[tuple[str, bool, set[str]]] = []\n",
        "    for i, item in enumerate(test_items, 1):\n",
        "        print(f\"[{i}/{len(test_items)}] ‚ñ∂ Ïã§Ìñâ Ï§ë: {item}\")\n",
        "        ok, exec_set = run_test_item(item)\n",
        "        per_test_exec.append((item, ok, exec_set))\n",
        "        print(f\"   ‚Üí {'‚úÖPASS' if ok else '‚ùåFAIL'}, executed_lines={len(exec_set)}\")\n",
        "\n",
        "    # ---------- 4. ÌÉêÏöïÏ†Å ÏÑ†ÌÉù (set-cover Ïú†ÏÇ¨) ----------\n",
        "    covered_so_far = set(baseline_lines)\n",
        "    candidates = [(it, s) for (it, ok, s) in per_test_exec if ok]\n",
        "\n",
        "    THRESHOLD_MIN_GAIN = 1  # Í≥†Ïú† Í∏∞Ïó¨ ÎùºÏù∏ ‚â• 1\n",
        "    selected: list[str] = []\n",
        "\n",
        "    while True:\n",
        "        best = None\n",
        "        best_gain_val = 0\n",
        "        for it, s in candidates:\n",
        "            gain_val = len(s - covered_so_far)\n",
        "            if gain_val > best_gain_val:\n",
        "                best_gain_val = gain_val\n",
        "                best = (it, s)\n",
        "        if not best or best_gain_val < THRESHOLD_MIN_GAIN:\n",
        "            break\n",
        "        it, s = best\n",
        "        selected.append(it)\n",
        "        covered_so_far |= s\n",
        "        candidates = [(iit, ss) for (iit, ss) in candidates if iit != it]\n",
        "\n",
        "    # ---- Ìè¥Î∞±: Î™®Îëê 0Ïù¥Î©¥ PASS Ï§ëÏóêÏÑú ÏµúÎåÄ executed_lines 1Í∞ú Ïú†ÏßÄ\n",
        "    if not selected:\n",
        "        passables = [(it, s) for (it, ok, s) in per_test_exec if ok]\n",
        "        if passables:\n",
        "            it, s = max(passables, key=lambda x: len(x[1]))\n",
        "            selected = [it]\n",
        "            covered_so_far |= s\n",
        "            print(f\"‚ÑπÔ∏è Ìè¥Î∞± Ï†ÅÏö©: '{it}' 1Í∞ú Ïú†ÏßÄ (Ïã§Ìñâ ÎùºÏù∏ {len(s)}).\")\n",
        "\n",
        "    # ---------- 5. Í≤∞Í≥º ÏöîÏïΩ ----------\n",
        "    selected_set = set(selected)\n",
        "    retained = [{\"name\": it, \"success\": True} for it in selected]\n",
        "    removed = [{\"name\": it, \"success\": bool(ok)}\n",
        "               for (it, ok, _s) in per_test_exec if it not in selected_set]\n",
        "\n",
        "    summary = {\n",
        "        \"total_tests\": len(test_items),\n",
        "        \"retained\": len(retained),\n",
        "        \"removed\": len(removed),\n",
        "        \"retained_tests\": retained,\n",
        "        \"removed_tests\": removed,\n",
        "        \"baseline_lines\": len(baseline_lines),\n",
        "        \"final_covered_lines\": len(covered_so_far),\n",
        "        \"incremental_gain_lines\": len(covered_so_far - baseline_lines),\n",
        "        \"threshold_min_gain\": THRESHOLD_MIN_GAIN,\n",
        "    }\n",
        "    (ART_DIR / \"pruning_summary.json\").write_text(\n",
        "        json.dumps(summary, indent=2, ensure_ascii=False),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    # ---------- 6. Ïú†ÏßÄÎêú ÌÖåÏä§Ìä∏Îßå Î≥µÏÇ¨ ----------\n",
        "    retained_files = set()\n",
        "    for nodeid in selected:\n",
        "        file_part = nodeid.split(\"::\", 1)[0]\n",
        "        path_obj = (PROJ / file_part).resolve()\n",
        "        if path_obj.exists():\n",
        "            retained_files.add(path_obj)\n",
        "\n",
        "    if SRC_TESTS.exists():\n",
        "        copied = 0\n",
        "        for test_file in sorted(SRC_TESTS.rglob(\"test_*.py\")):\n",
        "            if test_file.resolve() in retained_files:\n",
        "                rel = test_file.relative_to(SRC_TESTS)\n",
        "                dst = DST_PRUNED / rel\n",
        "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(test_file, dst)\n",
        "                copied += 1\n",
        "        print(f\"üì¶ Ïú†ÏßÄ ÌååÏùº Î≥µÏÇ¨ ÏôÑÎ£å: {copied}Í∞ú ÌååÏùº ‚Üí {DST_PRUNED}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è tests/ Ìè¥ÎçîÍ∞Ä ÏóÜÏñ¥ ÌååÏùº Î≥µÏÇ¨Îäî Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
        "\n",
        "    (ART_DIR / \"retained_nodeids.txt\").write_text(\"\\n\".join(selected), encoding=\"utf-8\")\n",
        "\n",
        "    print(\"‚úÖ ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ ÌîÑÎ£®Îãù ÏôÑÎ£å\")\n",
        "    print(\" - pruning_summary.json  :\", ART_DIR / \"pruning_summary.json\")\n",
        "    print(\" - retained_nodeids.txt  :\", ART_DIR / \"retained_nodeids.txt\")\n",
        "    print(\" - Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨(ÏÑ†ÌÉù ÌååÏùº):\", DST_PRUNED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCyuGBQWExjy",
        "outputId": "3d11765f-c237-4b4e-da6a-da03cd243d28"
      },
      "id": "qCyuGBQWExjy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ Ïàò: 102\n",
            "üìã pytest ÌÖåÏä§Ìä∏ ÏàòÏßë Ï§ë (--collect-only)...\n",
            "Ï¥ù ÏàòÏßëÎêú ÌÖåÏä§Ìä∏ Ïàò: 3\n",
            "[1/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer\n",
            "   ‚Üí ‚úÖPASS, executed_lines=91\n",
            "[2/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer_withdraw_insufficient_funds\n",
            "   ‚Üí ‚úÖPASS, executed_lines=83\n",
            "[3/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer_withdraw_invalid_account\n",
            "   ‚Üí ‚úÖPASS, executed_lines=79\n",
            "‚ÑπÔ∏è Ìè¥Î∞± Ï†ÅÏö©: 'tests/test_run_worker.py::test_money_transfer' 1Í∞ú Ïú†ÏßÄ (Ïã§Ìñâ ÎùºÏù∏ 91).\n",
            "üì¶ Ïú†ÏßÄ ÌååÏùº Î≥µÏÇ¨ ÏôÑÎ£å: 1Í∞ú ÌååÏùº ‚Üí /content/money-transfer-project-template-python/Pruned_Base_Tests\n",
            "‚úÖ ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ ÌîÑÎ£®Îãù ÏôÑÎ£å\n",
            " - pruning_summary.json  : /content/money-transfer-project-template-python/run_artifacts/run1/pruning_summary.json\n",
            " - retained_nodeids.txt  : /content/money-transfer-project-template-python/run_artifacts/run1/retained_nodeids.txt\n",
            " - Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨(ÏÑ†ÌÉù ÌååÏùº): /content/money-transfer-project-template-python/Pruned_Base_Tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2b) Î≥µÌï© Î™©Ìëú ÏÉùÏÑ±\n",
        "import ast\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set, Optional\n",
        "\n",
        "# ---------- Í≤ΩÎ°ú ----------\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR    = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "UNCOVERED_JSON = ART_DIR / \"uncovered_map_base.json\"\n",
        "OBSERVED_JSON  = ART_DIR / \"observed_outcomes_base.json\"\n",
        "\n",
        "assert UNCOVERED_JSON.exists(),  \"uncovered_map_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "assert OBSERVED_JSON.exists(),   \"observed_outcomes_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "# ===================== Ïú†Ìã∏ =====================\n",
        "def norm_abs(p: str) -> str:\n",
        "    q = Path(p)\n",
        "    if not q.is_absolute():\n",
        "        q = (PROJ_PATH / q).resolve()\n",
        "    return str(q.resolve())\n",
        "\n",
        "def rel_from_proj(abs_path: str) -> str:\n",
        "    try:\n",
        "        return str(Path(abs_path).resolve().relative_to(PROJ_PATH))\n",
        "    except Exception:\n",
        "        return abs_path\n",
        "\n",
        "def is_source(abs_path: str) -> bool:\n",
        "    try:\n",
        "        rel = Path(abs_path).resolve().relative_to(PROJ_PATH)\n",
        "    except Exception:\n",
        "        return False\n",
        "    s = str(rel).replace(\"\\\\\", \"/\")\n",
        "    if not s.endswith(\".py\"):\n",
        "        return False\n",
        "    if s.startswith((\"generated_tests/\", \"tests/\", \".venv/\", \"venv/\", \"run_artifacts/\", \"htmlcov/\")):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def try_unparse(node: ast.AST) -> Optional[str]:\n",
        "    try:\n",
        "        if hasattr(ast, \"unparse\"):\n",
        "            return ast.unparse(node)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ===================== baseline ÏûÖÎ†• Î°úÎìú =====================\n",
        "raw_uncovered = json.loads(UNCOVERED_JSON.read_text(encoding=\"utf-8\"))\n",
        "uncovered_map: Dict[str, List[int]] = {\n",
        "    norm_abs(k): sorted(set(v))\n",
        "    for k, v in raw_uncovered.items()\n",
        "    if is_source(norm_abs(k))\n",
        "}\n",
        "\n",
        "raw_observed = json.loads(OBSERVED_JSON.read_text(encoding=\"utf-8\"))\n",
        "observed_outcomes: Dict[str, Dict[int, Dict[str, float]]] = {}\n",
        "\n",
        "for k, mp in raw_observed.items():\n",
        "    abs_k = norm_abs(k)\n",
        "    if not is_source(abs_k):\n",
        "        continue\n",
        "    fixed = {}\n",
        "    for ln_str, meta in mp.items():\n",
        "        try:\n",
        "            fixed[int(ln_str)] = meta\n",
        "        except:\n",
        "            continue\n",
        "    observed_outcomes[abs_k] = fixed\n",
        "\n",
        "# half-hit\n",
        "half_hit_map: Dict[str, Set[int]] = {}\n",
        "for fp, mapping in observed_outcomes.items():\n",
        "    half = {ln for ln, meta in mapping.items()\n",
        "            if 0 < int(meta.get(\"covered\", 0)) < int(meta.get(\"total\", 0))}\n",
        "    if half:\n",
        "        half_hit_map[fp] = half\n",
        "\n",
        "\n",
        "# ===================== AST Ïä§Ï∫î =====================\n",
        "MOD_REQS = {\"requests\"}\n",
        "MOD_HTTPX = {\"httpx\"}\n",
        "MOD_TEMPORAL = {\"temporalio\"}\n",
        "MOD_SUBPROCESS = {\"subprocess\"}\n",
        "TEMPORAL_PREFIXES = (\"temporalio.client.\", \"temporalio.worker.\", \"temporalio.workflow.\")\n",
        "\n",
        "class FunctionInfo:\n",
        "    def __init__(self, name, lineno):\n",
        "        self.name = name or \"<module>\"\n",
        "        self.lineno = lineno\n",
        "        self.branches: List[int] = []\n",
        "        self.defs: Dict[str, List[int]] = {}\n",
        "        self.uses: Dict[str, List[int]] = {}\n",
        "        self.exceptions: List[Tuple[str, int, dict]] = []\n",
        "        self.side_effect_calls: List[Tuple[str, int]] = []\n",
        "\n",
        "    def add_def(self, v, ln): self.defs.setdefault(v, []).append(ln)\n",
        "    def add_use(self, v, ln): self.uses.setdefault(v, []).append(ln)\n",
        "\n",
        "\n",
        "class ASTVisitor(ast.NodeVisitor):\n",
        "    def __init__(self, file_rel):\n",
        "        self.file_rel = file_rel\n",
        "        self.stack: List[FunctionInfo] = []\n",
        "        self.funcs: List[FunctionInfo] = []\n",
        "        self.alias_to_module = {}\n",
        "        self.symbol_to_module = {}\n",
        "\n",
        "    def current(self):\n",
        "        if not self.stack:\n",
        "            if not self.funcs or self.funcs[0].name != \"<module>\":\n",
        "                fi = FunctionInfo(\"<module>\", 1)\n",
        "                self.funcs.insert(0, fi)\n",
        "            return self.funcs[0]\n",
        "        return self.stack[-1]\n",
        "\n",
        "    # imports\n",
        "    def visit_Import(self, node):\n",
        "        for alias in node.names:\n",
        "            mod = alias.name\n",
        "            asname = alias.asname or mod.split(\".\")[0]\n",
        "            self.alias_to_module[asname] = mod\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_ImportFrom(self, node):\n",
        "        mod = node.module or \"\"\n",
        "        for alias in node.names:\n",
        "            self.symbol_to_module[alias.asname or alias.name] = mod\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    # function defs\n",
        "    def visit_FunctionDef(self, node):\n",
        "        fi = FunctionInfo(node.name, node.lineno)\n",
        "        self.stack.append(fi); self.generic_visit(node); self.stack.pop()\n",
        "        self.funcs.append(fi)\n",
        "\n",
        "    def visit_AsyncFunctionDef(self, node):\n",
        "        self.visit_FunctionDef(node)\n",
        "\n",
        "    # branches\n",
        "    def visit_If(self, node):        self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_While(self, node):     self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_For(self, node):       self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_AsyncFor(self, node):  self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_With(self, node):      self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "\n",
        "    def visit_Try(self, node):\n",
        "        self.current().branches.append(node.lineno)\n",
        "        self.current().exceptions.append((\"try\", node.lineno, {}))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_ExceptHandler(self, node):\n",
        "        hint = {}\n",
        "        if node.type is not None:\n",
        "            typ = try_unparse(node.type)\n",
        "            if not typ and hasattr(node.type, \"id\"):\n",
        "                typ = node.type.id\n",
        "            if typ: hint[\"exception_type\"] = typ\n",
        "        self.current().branches.append(node.lineno)\n",
        "        self.current().exceptions.append((\"except\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Raise(self, node):\n",
        "        hint = {}\n",
        "        if node.exc is not None:\n",
        "            text = try_unparse(node.exc)\n",
        "            if text: hint[\"expr\"] = text\n",
        "            if isinstance(node.exc, ast.Call):\n",
        "                fn = node.exc.func\n",
        "                if isinstance(fn, ast.Name):\n",
        "                    hint[\"exception_type\"] = fn.id\n",
        "                elif isinstance(fn, ast.Attribute):\n",
        "                    hint[\"exception_type\"] = fn.attr\n",
        "                if node.exc.args:\n",
        "                    a0 = node.exc.args[0]\n",
        "                    if isinstance(a0, ast.Constant) and isinstance(a0.value, str):\n",
        "                        hint[\"message_contains\"] = a0.value[:80]\n",
        "        self.current().exceptions.append((\"raise\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Assert(self, node):\n",
        "        hint = {}\n",
        "        if node.msg and isinstance(node.msg, ast.Constant) and isinstance(node.msg.value, str):\n",
        "            hint[\"message_contains\"] = node.msg.value[:80]\n",
        "        self.current().exceptions.append((\"assert\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Name(self, node):\n",
        "        if isinstance(node.ctx, ast.Store):\n",
        "            self.current().add_def(node.id, node.lineno)\n",
        "        elif isinstance(node.ctx, ast.Load):\n",
        "            self.current().add_use(node.id, node.lineno)\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    # dotted name\n",
        "    def _dotted_name(self, node):\n",
        "        if isinstance(node, ast.Name):\n",
        "            nm = node.id\n",
        "            if nm in self.alias_to_module:\n",
        "                return self.alias_to_module[nm]\n",
        "            if nm in self.symbol_to_module:\n",
        "                return f\"{self.symbol_to_module[nm]}.{nm}\"\n",
        "            return nm\n",
        "        if isinstance(node, ast.Attribute):\n",
        "            base = self._dotted_name(node.value)\n",
        "            if base: return f\"{base}.{node.attr}\"\n",
        "        return None\n",
        "\n",
        "    # calls\n",
        "    def visit_Call(self, node):\n",
        "        qn = self._dotted_name(node.func) or \"\"\n",
        "        if qn in (\"open\", \"builtins.open\"):\n",
        "            self.current().side_effect_calls.append((\"io_open\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_REQS):\n",
        "            self.current().side_effect_calls.append((\"net_requests\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_HTTPX):\n",
        "            self.current().side_effect_calls.append((\"net_httpx\", node.lineno))\n",
        "        if any(qn.startswith(pref) for pref in TEMPORAL_PREFIXES) \\\n",
        "           or any(qn.startswith(m + \".\") for m in MOD_TEMPORAL):\n",
        "            self.current().side_effect_calls.append((\"temporal\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_SUBPROCESS):\n",
        "            self.current().side_effect_calls.append((\"subprocess\", node.lineno))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "\n",
        "# ===================== ÎåÄÏÉÅ ÌååÏùº =====================\n",
        "candidate_files = sorted(set([*uncovered_map.keys(), *observed_outcomes.keys()]))\n",
        "candidate_files = [fp for fp in candidate_files if is_source(fp)]\n",
        "\n",
        "\n",
        "# ---------------- Í∞ÄÏ§ëÏπò/ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ----------------\n",
        "ALPHA_BRANCH = 0.45\n",
        "BETA_DU      = 0.25\n",
        "GAMMA_EXC    = 0.30\n",
        "LAMBDA_1_CONST   = 0.20\n",
        "LAMBDA_2_CONTEXT = 0.60\n",
        "LAMBDA_3_TARGETS = 0.20\n",
        "TOP_K = 10\n",
        "OVERLAP_THETA = 0.50\n",
        "\n",
        "assert abs((ALPHA_BRANCH + BETA_DU + GAMMA_EXC) - 1.0) < 1e-6\n",
        "assert abs((LAMBDA_1_CONST + LAMBDA_2_CONTEXT + LAMBDA_3_TARGETS) - 1.0) < 1e-6\n",
        "\n",
        "\n",
        "def coverage_gain_structural(b_cnt, du_cnt, exc_cnt):\n",
        "    return ALPHA_BRANCH*b_cnt + BETA_DU*du_cnt + GAMMA_EXC*exc_cnt\n",
        "\n",
        "def coverage_gain_total(target_lines, b_cnt, du_cnt, exc_cnt):\n",
        "    return len(target_lines) + coverage_gain_structural(b_cnt, du_cnt, exc_cnt)\n",
        "\n",
        "def generation_cost(context_size, target_count):\n",
        "    return (LAMBDA_1_CONST*1.0) + (LAMBDA_2_CONTEXT*context_size) + (LAMBDA_3_TARGETS*target_count)\n",
        "\n",
        "\n",
        "# ===================== AST ‚Üí file_infos =====================\n",
        "file_infos = {}\n",
        "MAX_NEAR_GAP = 8\n",
        "\n",
        "for file_abs in candidate_files:\n",
        "    try:\n",
        "        source = Path(file_abs).read_text(encoding=\"utf-8\")\n",
        "        tree = ast.parse(source)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è AST parse Ïã§Ìå®: {file_abs}, {e}\")\n",
        "        continue\n",
        "\n",
        "    rel = rel_from_proj(file_abs)\n",
        "    visitor = ASTVisitor(rel)\n",
        "    visitor.visit(tree)\n",
        "\n",
        "    miss = set(uncovered_map.get(file_abs, []))\n",
        "    half = set(half_hit_map.get(file_abs, []))\n",
        "\n",
        "    for fi in visitor.funcs:\n",
        "        tb = [ln for ln in fi.branches if (ln in miss or ln in half)]\n",
        "        tu_pairs = []\n",
        "        for var, defs in fi.defs.items():\n",
        "            uses = sorted(fi.uses.get(var, []))\n",
        "            defs_sorted = sorted(defs)\n",
        "            for u in uses:\n",
        "                if u not in miss:\n",
        "                    continue\n",
        "                d_le = [d for d in defs_sorted if d <= u]\n",
        "                if not d_le:\n",
        "                    continue\n",
        "                d = max(d_le)\n",
        "                tu_pairs.append((var, d, u))\n",
        "\n",
        "        te = [(kind, line, hint)\n",
        "              for (kind, line, hint) in fi.exceptions if line in miss]\n",
        "\n",
        "        ctx = len({ln for ls in fi.defs.values() for ln in ls} |\n",
        "                  {ln for ls in fi.uses.values() for ln in ls})\n",
        "\n",
        "        if tb or tu_pairs or te:\n",
        "            file_infos[(rel, fi.name)] = {\n",
        "                \"branches\": sorted(tb),\n",
        "                \"def_uses\": sorted(tu_pairs, key=lambda x: (x[2], x[1], x[0])),\n",
        "                \"exceptions\": sorted(te, key=lambda x: (x[1], x[0])),\n",
        "                \"func_lineno\": fi.lineno,\n",
        "                \"context_size\": ctx,\n",
        "            }\n",
        "\n",
        "\n",
        "# ===================== ÌõÑÎ≥¥ goal ÏÉùÏÑ± =====================\n",
        "def make_goal(file_rel, func, func_lineno, branches, def_uses, exceptions, context_size):\n",
        "    target_lines = set(branches)\n",
        "    for _, d, u in def_uses: target_lines.update([d, u])\n",
        "    for _, line, _ in exceptions: target_lines.add(line)\n",
        "\n",
        "    b_cnt = len(branches)\n",
        "    du_cnt = len(def_uses)\n",
        "    exc_cnt = len(exceptions)\n",
        "\n",
        "    gain = coverage_gain_total(target_lines, b_cnt, du_cnt, exc_cnt)\n",
        "    cost = generation_cost(context_size, len(target_lines))\n",
        "\n",
        "    return {\n",
        "        \"file\": file_rel,\n",
        "        \"function\": {\"name\": func, \"lineno\": func_lineno},\n",
        "        \"components\": {\n",
        "            \"branches\": [{\"line\": b} for b in branches],\n",
        "            \"def_uses\": [{\"var\": v, \"def_line\": d, \"use_line\": u} for (v, d, u) in def_uses],\n",
        "            \"exceptions\": [{\"kind\": k, \"line\": ln, **hint} for (k, ln, hint) in exceptions],\n",
        "        },\n",
        "        \"target_lines\": sorted(target_lines),\n",
        "        \"coverage_gain\": float(gain),\n",
        "        \"generation_cost\": float(cost),\n",
        "        \"context_size\": context_size,\n",
        "    }\n",
        "\n",
        "\n",
        "candidates = []\n",
        "\n",
        "for (file_rel, func), info in file_infos.items():\n",
        "    tb = info[\"branches\"]\n",
        "    tu = info[\"def_uses\"]\n",
        "    te = info[\"exceptions\"]\n",
        "    ctx = info[\"context_size\"]\n",
        "    func_lineno = info[\"func_lineno\"]\n",
        "\n",
        "    # Îã®Ïùº ÏöîÏÜå\n",
        "    for b in tb:              candidates.append(make_goal(file_rel, func, func_lineno, [b], [], [], ctx))\n",
        "    for (v, d, u) in tu:      candidates.append(make_goal(file_rel, func, func_lineno, [], [(v, d, u)], [], ctx))\n",
        "    for (k, ln, h) in te:     candidates.append(make_goal(file_rel, func, func_lineno, [], [], [(k, ln, h)], ctx))\n",
        "\n",
        "    # Í∑ºÏ†ë Ï°∞Ìï©\n",
        "    for b in tb:\n",
        "        for (v, d, u) in tu:\n",
        "            if max(b, u) - min(b, d) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [b], [(v, d, u)], [], ctx))\n",
        "\n",
        "    for b in tb:\n",
        "        for (k, ln, h) in te:\n",
        "            if abs(b - ln) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [b], [], [(k, ln, h)], ctx))\n",
        "\n",
        "    for (v, d, u) in tu:\n",
        "        for (k, ln, h) in te:\n",
        "            if max(u, ln) - min(d, ln) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [], [(v, d, u)], [(k, ln, h)], ctx))\n",
        "\n",
        "    for b in tb:\n",
        "        for (v, d, u) in tu:\n",
        "            for (k, ln, h) in te:\n",
        "                lines = [b, d, u, ln]\n",
        "                if max(lines) - min(lines) <= MAX_NEAR_GAP:\n",
        "                    candidates.append(make_goal(file_rel, func, func_lineno,\n",
        "                                                [b], [(v, d, u)], [(k, ln, h)], ctx))\n",
        "\n",
        "\n",
        "# ===================== Algorithm 1 (19Ï§Ñ) Íµ¨ÌòÑ =====================\n",
        "\n",
        "def overlap_basic(g, h):\n",
        "    tg = set(g[\"target_lines\"])\n",
        "    th = set(h[\"target_lines\"])\n",
        "    if not tg:\n",
        "        return 0.0\n",
        "    return len(tg & th) / len(tg)\n",
        "\n",
        "# goc Í≥ÑÏÇ∞\n",
        "a = 0.7\n",
        "for g in candidates:\n",
        "    gain = g[\"coverage_gain\"]\n",
        "    cost = g[\"generation_cost\"]\n",
        "    g[\"goc\"] = a * gain + (1 - a) * (1 / (cost + 1))\n",
        "\n",
        "# ---- filtering (line 12~16) ----\n",
        "filtered = []\n",
        "for g in candidates:\n",
        "    if g[\"goc\"] <= 0:\n",
        "        continue\n",
        "    bad = False\n",
        "    for h in filtered:\n",
        "        if overlap_basic(g, h) > OVERLAP_THETA and h[\"goc\"] >= g[\"goc\"]:\n",
        "            bad = True\n",
        "            break\n",
        "    if not bad:\n",
        "        filtered.append(g)\n",
        "\n",
        "# ---- Ï†ïÎ†¨ (line 17) ----\n",
        "filtered_sorted = sorted(filtered, key=lambda x: x[\"goc\"], reverse=True)\n",
        "\n",
        "# ---- top-k ÏÑ†ÌÉù (line 18) ----\n",
        "selected = filtered_sorted[:TOP_K]\n",
        "\n",
        "# ---- Ï†ïÏ†ú Î∞è Ï†ÄÏû• ----\n",
        "for i, g in enumerate(selected, 1):\n",
        "    g[\"id\"] = f\"{i:04d}\"\n",
        "    g[\"coverage_gain\"] = round(g[\"coverage_gain\"], 6)\n",
        "    g[\"generation_cost\"] = round(g[\"generation_cost\"], 6)\n",
        "    g[\"score\"] = round(g[\"goc\"], 6)\n",
        "\n",
        "(ART_DIR / \"goals_raw.json\").write_text(json.dumps(selected, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "ranked = [\n",
        "    {\n",
        "        \"id\": g[\"id\"],\n",
        "        \"file\": g[\"file\"],\n",
        "        \"function\": g[\"function\"],\n",
        "        \"components\": g[\"components\"],\n",
        "        \"target_lines\": g[\"target_lines\"],\n",
        "        \"coverage_gain\": g[\"coverage_gain\"],\n",
        "        \"generation_cost\": g[\"generation_cost\"],\n",
        "        \"score\": g[\"score\"],\n",
        "        \"context_size\": g[\"context_size\"],\n",
        "    }\n",
        "    for g in selected\n",
        "]\n",
        "\n",
        "(ART_DIR / \"goals_ranked.json\").write_text(json.dumps(ranked, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"‚úÖ Î≥µÌï© Î™©Ìëú ÏÉùÏÑ± ÏôÑÎ£å (Algorithm1, top-k={TOP_K})\")\n",
        "print(f\" - ÏõêÎ≥∏ ÌõÑÎ≥¥ Ïàò: {len(candidates)}\")\n",
        "print(f\" - filtered ÌõÑÎ≥¥ Ïàò: {len(filtered)}\")\n",
        "print(f\" - ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú Î™©Ìëú Ïàò: {len(selected)}\")\n",
        "print(\" - Ï†ÄÏû•: goals_raw.json, goals_ranked.json\")\n"
      ],
      "metadata": {
        "id": "q3TIL3otgCzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b08008-237d-44c0-aa8e-d042f8981a1a"
      },
      "id": "q3TIL3otgCzm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Î≥µÌï© Î™©Ìëú ÏÉùÏÑ± ÏôÑÎ£å (Algorithm1, top-k=10)\n",
            " - ÏõêÎ≥∏ ÌõÑÎ≥¥ Ïàò: 94\n",
            " - filtered ÌõÑÎ≥¥ Ïàò: 50\n",
            " - ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú Î™©Ìëú Ïàò: 10\n",
            " - Ï†ÄÏû•: goals_raw.json, goals_ranked.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2c) ÏûÖÎ†• Ï†úÏïΩ ÏûêÎèô Ï∂îÏ∂úÍ∏∞\n",
        "\n",
        "import ast\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "\n",
        "GOALS_RANKED = ART_DIR / \"goals_ranked.json\"\n",
        "OUT_FILE = ART_DIR / \"goals_enriched.json\"\n",
        "\n",
        "assert GOALS_RANKED.exists(), \"goals_ranked.json ÏóÜÏñ¥Ïöî. 3-2b Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "goals = json.loads(GOALS_RANKED.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Í≥µÏö© Ïú†Ìã∏\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def load_ast(path: Path):\n",
        "    try:\n",
        "        return ast.parse(path.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def annotate_parent(root):\n",
        "    for node in ast.walk(root):\n",
        "        for child in ast.iter_child_nodes(node):\n",
        "            setattr(child, \"parent\", node)\n",
        "\n",
        "def u(node):\n",
        "    try:\n",
        "        return ast.unparse(node)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ÌÉÄÏûÖ/ÌååÎùºÎØ∏ÌÑ∞ Ï†úÏïΩ\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def extract_param_constraints(func_node: ast.FunctionDef):\n",
        "    cons = []\n",
        "    args = func_node.args.args\n",
        "\n",
        "    for a in args:\n",
        "        name = a.arg\n",
        "        if a.annotation:\n",
        "            ann = u(a.annotation) or \"\"\n",
        "            ann_l = ann.lower()\n",
        "            if \"int\" in ann_l:\n",
        "                cons.append(f\"{name}: integer\")\n",
        "            elif \"float\" in ann_l:\n",
        "                cons.append(f\"{name}: float\")\n",
        "            elif \"str\" in ann_l:\n",
        "                cons.append(f\"{name}: string\")\n",
        "            elif \"bool\" in ann_l:\n",
        "                cons.append(f\"{name}: boolean\")\n",
        "            else:\n",
        "                cons.append(f\"{name}: type={ann}\")\n",
        "\n",
        "        # Í∏∞Î≥∏Í∞í Í∏∞Î∞ò Ï†úÏïΩ\n",
        "        if func_node.args.defaults:\n",
        "            idx = args.index(a) - (len(args) - len(func_node.args.defaults))\n",
        "            if idx >= 0:\n",
        "                dv = func_node.args.defaults[idx]\n",
        "                cons.append(f\"{name}: default={u(dv)}\")\n",
        "\n",
        "    return cons\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Ïã¨Î≥ºÎ¶≠ Ï°∞Í±¥Ïãù(parser) ‚Üí value-domain constraint\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def _translate_compare(node):\n",
        "    \"\"\"\n",
        "    x > 0 ‚Üí \"x > 0\"\n",
        "    0 < x < 10 ‚Üí \"0 < x < 10\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return u(node)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_branch_constraints(func_node: ast.FunctionDef):\n",
        "    cons = []\n",
        "\n",
        "    for node in ast.walk(func_node):\n",
        "\n",
        "        # if Ï°∞Í±¥\n",
        "        if isinstance(node, ast.If):\n",
        "            cond = _translate_compare(node.test)\n",
        "            if cond:\n",
        "                cons.append(f\"condition: {cond}\")\n",
        "\n",
        "        # assert Ï°∞Í±¥\n",
        "        if isinstance(node, ast.Assert):\n",
        "            cond = _translate_compare(node.test)\n",
        "            if cond:\n",
        "                cons.append(f\"assert: {cond}\")\n",
        "\n",
        "        # Ï°∞Í±¥ ÏïàÏóê Í∞í ÎπÑÍµêÍ∞Ä ÏûàÏúºÎ©¥ value-domain Ï∂îÎ°†\n",
        "        if isinstance(node, ast.Compare):\n",
        "            left = u(node.left)\n",
        "            rights = [u(r) for r in node.comparators]\n",
        "            ops = node.ops\n",
        "\n",
        "            if left and all(rights):\n",
        "                if isinstance(ops[0], ast.Gt):\n",
        "                    cons.append(f\"{left} > {rights[0]}\")\n",
        "                if isinstance(ops[0], ast.Lt):\n",
        "                    cons.append(f\"{left} < {rights[0]}\")\n",
        "                if isinstance(ops[0], ast.GtE):\n",
        "                    cons.append(f\"{left} >= {rights[0]}\")\n",
        "                if isinstance(ops[0], ast.LtE):\n",
        "                    cons.append(f\"{left} <= {rights[0]}\")\n",
        "                if isinstance(ops[0], ast.Eq):\n",
        "                    cons.append(f\"{left} == {rights[0]}\")\n",
        "                if isinstance(ops[0], ast.NotEq):\n",
        "                    cons.append(f\"{left} != {rights[0]}\")\n",
        "\n",
        "    return cons\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# def-use Í∏∞Î∞ò value-flow Ï†úÏïΩ\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def extract_def_use_constraints(func_node: ast.FunctionDef, goal):\n",
        "    pairs = goal[\"components\"][\"def_uses\"]\n",
        "    cons = []\n",
        "\n",
        "    for pair in pairs:\n",
        "        var = pair[\"var\"]\n",
        "        d = pair[\"def_line\"]\n",
        "        u_line = pair[\"use_line\"]\n",
        "        cons.append(f\"value-flow: {var} defined@{d} ‚Üí used@{u_line}\")\n",
        "\n",
        "    return cons\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# raise/except Í∏∞Î∞ò ÏòàÏô∏ Ï°∞Í±¥ Ï∂îÎ°†\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def extract_exception_constraints(func_node: ast.FunctionDef):\n",
        "    cons = []\n",
        "    for node in ast.walk(func_node):\n",
        "        if isinstance(node, ast.Raise):\n",
        "            exc = u(node.exc)\n",
        "            if exc:\n",
        "                cons.append(f\"exception: raises {exc}\")\n",
        "\n",
        "        if isinstance(node, ast.ExceptHandler):\n",
        "            typ = u(node.type) if node.type else None\n",
        "            if typ:\n",
        "                cons.append(f\"exception-handler: {typ}\")\n",
        "\n",
        "    return cons\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ÌïÑÎìú Ï†ëÍ∑º / subscript Í∏∞Î∞ò Íµ¨Ï°∞ Ï†úÏïΩ\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def extract_field_constraints(func_node: ast.FunctionDef):\n",
        "    cons = []\n",
        "    for node in ast.walk(func_node):\n",
        "\n",
        "        # obj.attr\n",
        "        if isinstance(node, ast.Attribute):\n",
        "            expr = u(node)\n",
        "            if expr:\n",
        "                cons.append(f\"needs field: {expr}\")\n",
        "\n",
        "        # obj[\"key\"]\n",
        "        if isinstance(node, ast.Subscript):\n",
        "            expr = u(node)\n",
        "            if expr:\n",
        "                cons.append(f\"needs key: {expr}\")\n",
        "\n",
        "    return cons\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# relevance filtering: target_lines Í∑ºÏ≤òÏùò Ï†úÏïΩÎßå ÎÇ®Í∏∞Í∏∞\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def filter_relevant(constraints, func_node, goal):\n",
        "    target = set(goal[\"target_lines\"])\n",
        "    if not target:\n",
        "        return constraints\n",
        "\n",
        "    relevant = []\n",
        "\n",
        "    for c in constraints:\n",
        "        # Îß§Ïö∞ Îã®ÏàúÌïòÍ≤å: Ïà´Ïûê ÎùºÏù∏Ïù¥ Ìè¨Ìï®ÎêòÎ©¥ ÌïÑÌÑ∞ÎßÅ\n",
        "        found = False\n",
        "        for t in target:\n",
        "            if f\"@{t}\" in c or f\"{t}\" in c:\n",
        "                found = True\n",
        "                break\n",
        "        if found:\n",
        "            relevant.append(c)\n",
        "        else:\n",
        "            # branch/assertÎäî Ï†ÑÏ≤¥ Ìï®ÏàòÏóê ÏòÅÌñ• ‚Üí keep\n",
        "            if c.startswith(\"condition\") or c.startswith(\"assert\"):\n",
        "                relevant.append(c)\n",
        "\n",
        "    return list(dict.fromkeys(relevant))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Î©îÏù∏\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "goals_out = []\n",
        "\n",
        "for g in goals:\n",
        "    file_rel = g[\"file\"]\n",
        "    fname = g[\"function\"][\"name\"]\n",
        "\n",
        "    src = (PROJ_PATH / file_rel).resolve()\n",
        "    tree = load_ast(src)\n",
        "\n",
        "    if tree is None:\n",
        "        g[\"input_constraints\"] = []\n",
        "        goals_out.append(g)\n",
        "        continue\n",
        "\n",
        "    annotate_parent(tree)\n",
        "\n",
        "    func_node = None\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.FunctionDef) and node.name == fname:\n",
        "            func_node = node\n",
        "            break\n",
        "\n",
        "    if not func_node:\n",
        "        g[\"input_constraints\"] = []\n",
        "        goals_out.append(g)\n",
        "        continue\n",
        "\n",
        "    # ---- Ï∂îÎ°† Î™®Îìà Ìï©ÏπòÍ∏∞ ----\n",
        "    c_param = extract_param_constraints(func_node)\n",
        "    c_branch = extract_branch_constraints(func_node)\n",
        "    c_du = extract_def_use_constraints(func_node, g)\n",
        "    c_exc = extract_exception_constraints(func_node)\n",
        "    c_field = extract_field_constraints(func_node)\n",
        "\n",
        "    all_c = c_param + c_branch + c_du + c_exc + c_field\n",
        "    all_c = list(dict.fromkeys(all_c))\n",
        "\n",
        "    # relevance filter\n",
        "    all_c = filter_relevant(all_c, func_node, g)\n",
        "\n",
        "    g[\"input_constraints\"] = all_c\n",
        "    goals_out.append(g)\n",
        "\n",
        "# ---------------- Ï†ÄÏû• ----------------\n",
        "OUT_FILE.write_text(\n",
        "    json.dumps(goals_out, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ïã¨Ï∏µ ÏûÖÎ†• Ï†úÏïΩ Ï∂îÏ∂ú ÏôÑÎ£å ‚Üí\", OUT_FILE)\n",
        "print(\"Ï¥ù Î™©Ìëú:\", len(goals_out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no-RwijrFe7_",
        "outputId": "364e297b-3d68-4808-ff9b-5b8a164f1238"
      },
      "id": "no-RwijrFe7_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ïã¨Ï∏µ ÏûÖÎ†• Ï†úÏïΩ Ï∂îÏ∂ú ÏôÑÎ£å ‚Üí /content/money-transfer-project-template-python/run_artifacts/run1/goals_enriched.json\n",
            "Ï¥ù Î™©Ìëú: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-3) LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "\n",
        "GOALS_RANKED = ART_DIR / \"goals_ranked.json\"\n",
        "GOALS_ENRICHED = ART_DIR / \"goals_enriched.json\"\n",
        "LLM_OUT = ART_DIR / \"llm_prompts.jsonl\"\n",
        "\n",
        "assert GOALS_RANKED.exists(), \"goals_ranked.json ÏóÜÏùå (3-2b Î®ºÏ†Ä Ïã§Ìñâ)\"\n",
        "assert GOALS_ENRICHED.exists(), \"goals_enriched.json ÏóÜÏùå (3-2c Î®ºÏ†Ä Ïã§Ìñâ)\"\n",
        "\n",
        "ranked = json.loads(GOALS_RANKED.read_text(encoding=\"utf-8\"))\n",
        "enriched = json.loads(GOALS_ENRICHED.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# --- enriched Î•º id Í∏∞Ï§ÄÏúºÎ°ú Lookup\n",
        "E = {g[\"id\"]: g for g in enriched}\n",
        "\n",
        "def to_mod(file):\n",
        "    s = file.replace(\"\\\\\", \"/\")\n",
        "    if s.startswith(\"src/\"): s = s[4:]\n",
        "    if s.endswith(\".py\"): s = s[:-3]\n",
        "    return s.replace(\"/\", \".\")\n",
        "\n",
        "def suggest(goal):\n",
        "    mod = to_mod(goal[\"file\"]).split(\".\")[-1]\n",
        "    func = goal[\"function\"][\"name\"]\n",
        "    safe = lambda x: re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", str(x))\n",
        "    return f\"test_gen_{safe(goal['id'])}_{safe(mod)}_{safe(func)}.py\"\n",
        "\n",
        "# --- ÏãúÏä§ÌÖú Î©îÏãúÏßÄ (Í∞ïÌôî)\n",
        "SYSTEM = (\n",
        "\"Ïó≠Ìï†: ÎãπÏã†ÏùÄ Ï£ºÏñ¥ÏßÑ Ïã§Ìñâ Î™©Ìëú(Î∂ÑÍ∏∞/Ï†ïÏùò-ÏÇ¨Ïö©/ÏòàÏô∏)Î•º ÌÉÄÍ≤©ÌïòÎäî pytest ÌÖåÏä§Ìä∏ ÏΩîÎìúÎ•º ÏÉùÏÑ±ÌïòÎäî ÏóîÏßÑÏûÖÎãàÎã§.\\n\"\n",
        "\"Ï∂úÎ†•: Ïò§ÏßÅ ÌïòÎÇòÏùò JSON Í∞ùÏ≤¥Îßå Î∞òÌôòÌïòÍ≥† Ï£ºÏÑù/markdown Í∏àÏßÄ.\\n\"\n",
        "\"{\\n\"\n",
        "'  \"filename\": \"test_*.py\",\\n'\n",
        "'  \"tests\": [ {\"name\": \"test_*\", \"code\": \"<pytest code>\"} ]\\n'\n",
        "\"}\\n\"\n",
        "\"\\n\"\n",
        "\"ÏóÑÍ≤© Í∑úÏπô:\\n\"\n",
        "\"‚Ä¢ importlib.import_module + getattr Î°úÎßå Ïã¨Î≥º Ï†ëÍ∑º (ÏóÜÏùÑ ÎïåÎßå guarded skip ÌóàÏö©)\\n\"\n",
        "\"‚Ä¢ Î™®Îìà Î†àÎ≤® skip Í∏àÏßÄ. skipÏùÄ ÌÖåÏä§Ìä∏ Ìï®Ïàò ÎÇ¥Î∂Ä + Ïã¨Î≥º ÏóÜÏùå ÏÉÅÌô©ÏóêÏÑúÎßå.\\n\"\n",
        "\"‚Ä¢ ÏµúÏÜå 1Í∞ú assert ÎòêÎäî pytest.raises ÌïÑÏàò.\\n\"\n",
        "\"‚Ä¢ Î∂ÑÍ∏∞Îäî Ïñë Í≤ΩÎ°úÎ•º Î≥ÑÎèÑ ÌÖåÏä§Ìä∏Î°ú ÏàòÌñâ(half-hit Ïãú 2Í∞ú ÌïÑÏöî).\\n\"\n",
        "\"‚Ä¢ def-use Í≤ΩÎ°úÎäî Ïã§Ï†ú ÏÉÅÌÉú/Î∞òÌôòÏóê Î∞òÏòÅÎêòÎèÑÎ°ù assert.\\n\"\n",
        "\"‚Ä¢ ÏòàÏô∏Îäî pytest.raisesÎ°ú ÌÉÄÏûÖ(+Í∞ÄÎä•ÌïòÎ©¥ Î©îÏãúÏßÄ) Í≤ÄÏ¶ù.\\n\"\n",
        "\"‚Ä¢ IO/ÏãúÍ∞Ñ/env/net/Temporal Îì± Ïô∏Î∂ÄÎäî Î™®Îëê monkeypatch.\\n\"\n",
        "\"‚Ä¢ async ÎåÄÏÉÅÏùÄ Î∞òÎìúÏãú async ÌÖåÏä§Ìä∏ + await. asyncio.run Í∏àÏßÄ.\\n\"\n",
        "\"‚Ä¢ TemporalÏùÄ Client/Worker/Workflow Îì± Í≥µÏãù ÏóîÌä∏Î¶¨Ìè¨Ïù∏Ìä∏Îßå ÎçîÎØ∏ Ìå®Ïπò.\\n\"\n",
        ")\n",
        "\n",
        "def priority(g):\n",
        "    return float(g.get(\"score\", g.get(\"coverage_gain\", 0.0)))\n",
        "\n",
        "ranked_sorted = sorted(ranked, key=lambda x: -priority(x))\n",
        "\n",
        "# --- ÌååÏùº ÏÉùÏÑ± ---\n",
        "with LLM_OUT.open(\"w\", encoding=\"utf-8\") as outf:\n",
        "    for rank, g in enumerate(ranked_sorted, start=1):\n",
        "\n",
        "        e = E[g[\"id\"]]\n",
        "        mod = to_mod(g[\"file\"])\n",
        "        filename = suggest(g)\n",
        "\n",
        "        # --- 3-2c ÏûÖÎ†• Ï†úÏïΩ Î∞òÏòÅ\n",
        "        input_constraints = e.get(\"input_constraints\", [])\n",
        "\n",
        "        # --- ÌÖåÏä§Ìä∏ ÏûÖÎ†•Í∞í ÏÉùÏÑ± Í∑úÏπô (target-line ÌûàÌä∏Ïö©)\n",
        "        hit_recipe = [\n",
        "            \"‚Ä¢ target_linesÏôÄ ÏßÅÏ†ë Ïó∞Í≤∞Îêú Ï°∞Í±¥Ïãù/ÎπÑÍµêÏãùÏùÑ Ï∂©Ï°±ÌïòÎäî ÏûÖÎ†•Í∞íÏùÑ Íµ¨ÏÑ±Ìï† Í≤É.\",\n",
        "            \"‚Ä¢ Ï°∞Í±¥Ïãù(Ïòà: amount > 0, 0 < x < 10)ÏùÑ ÎßåÏ°±ÌïòÎäî ÏµúÏÜå/Í≤ΩÍ≥ÑÍ∞í Ïö∞ÏÑ† ÏÉùÏÑ±.\",\n",
        "            \"‚Ä¢ def-use ÌùêÎ¶Ñ(v defined@d ‚Üí used@u)ÏùÄ use-lineÏóêÏÑú Í∞íÏù¥ Ïã§Ï†ú Î∞òÏòÅÎêòÎèÑÎ°ù ÏûÖÎ†• Íµ¨ÏÑ±.\",\n",
        "            \"‚Ä¢ needs field: obj.x / needs key: data['x'] Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Ìï¥Îãπ Íµ¨Ï°∞ Í∞ÄÏßÑ dict/object Ï†úÍ≥µ.\",\n",
        "            \"‚Ä¢ exception Î™©ÌëúÍ∞Ä ÏûàÏùÑ Í≤ΩÏö∞ raise Î∞úÏÉù Ï°∞Í±¥ÏùÑ Ï†ïÌôïÌûà Ï∂©Ï°±ÏãúÌÇ§Îäî ÏûÖÎ†• Íµ¨ÏÑ±.\",\n",
        "        ]\n",
        "\n",
        "        USER = {\n",
        "            \"schema_version\": \"v2\",\n",
        "            \"identifier\": {\n",
        "                \"id\": g[\"id\"],\n",
        "                \"rank\": rank,\n",
        "                \"priority\": priority(g),\n",
        "                \"basis\": \"score\" if \"score\" in g else \"coverage_gain\"\n",
        "            },\n",
        "            \"project\": {\n",
        "                \"root\": str(PROJ_PATH),\n",
        "                \"module\": mod\n",
        "            },\n",
        "            \"goal\": {\n",
        "                \"file\": g[\"file\"],\n",
        "                \"function\": g[\"function\"],\n",
        "                \"components\": g[\"components\"],\n",
        "                \"target_lines\": g[\"target_lines\"],\n",
        "                \"input_constraints\": input_constraints,\n",
        "            },\n",
        "            \"input_synthesis_recipe\": hit_recipe,\n",
        "            \"constraints\": {\n",
        "                \"filename_suggestion\": filename,\n",
        "                \"import_policy\": {\n",
        "                    \"strategy\": \"importlib_only\",\n",
        "                    \"on_missing\": \"pytest.skip\"\n",
        "                },\n",
        "                \"execution_contract\": {\n",
        "                    \"must_hit_at_least_n_target_lines\": 1,\n",
        "                    \"require_two_tests_for_half_hit\": bool(g[\"components\"][\"branches\"]),\n",
        "                    \"test_name_must_include_hit_lines\": True\n",
        "                },\n",
        "                \"isolation_policy\": {\n",
        "                    \"no_fs_no_net\": True,\n",
        "                    \"patch_time\": True,\n",
        "                    \"patch_env\": True,\n",
        "                    \"forbid_asyncio_run\": True,\n",
        "                    \"forbid_temporal_real_runs\": True\n",
        "                },\n",
        "                \"assert_policy\": {\n",
        "                    \"prefer_pytest_raises\": True,\n",
        "                    \"prefer_explicit_asserts\": True,\n",
        "                    \"require_state_assert\": True,\n",
        "                    \"no_unrelated_asserts\": True\n",
        "                },\n",
        "                \"async_policy\": {\n",
        "                    \"mark_asyncio_and_await_coroutines\": True,\n",
        "                    \"require_async_test_if_coroutine\": True,\n",
        "                    \"forbid_sync_call_of_coroutines\": True,\n",
        "                    \"forbid_asyncio_run\": True\n",
        "                },\n",
        "                \"temporal_policy\": {\n",
        "                    \"patch_entrypoints_only\": True,\n",
        "                    \"require_temporal_mocks\": True,\n",
        "                    \"preferred_targets\": [\n",
        "                        \"temporalio.client.Client\",\n",
        "                        \"temporalio.worker.Worker\",\n",
        "                        \"temporalio.workflow.Workflow\"\n",
        "                    ]\n",
        "                }\n",
        "            },\n",
        "            \"instructions\": [\n",
        "                \"JSON ÌòïÌÉúÎßå Î∞òÌôòÌï† Í≤É.\",\n",
        "                \"ÏûÖÎ†• Ï†úÏïΩ Î∞è branch/def-use/exceptionsÎ•º Í∏∞Î∞òÏúºÎ°ú target_linesÎ•º Î∞òÎìúÏãú ÌÉÄÍ≤©Ìï† Í≤É.\",\n",
        "                \"value-range, Ï°∞Í±¥Ïãù, Íµ¨Ï°∞Ï†Å Ï†úÏïΩÏùÑ Ï∂©Ï°±ÌïòÎäî ÏûÖÎ†•Í∞íÏùÑ ÏßÅÏ†ë Íµ¨ÏÑ±Ìï† Í≤É.\",\n",
        "                \"async/Temporal/test isolation Í∑úÏπô Ï§ÄÏàò.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        rec = {\n",
        "            \"meta\": {\n",
        "                \"id\": g[\"id\"],\n",
        "                \"rank\": rank,\n",
        "                \"priority\": priority(g),\n",
        "                \"file\": g[\"file\"],\n",
        "                \"function\": g[\"function\"][\"name\"],\n",
        "                \"coverage_gain\": g[\"coverage_gain\"],\n",
        "                \"score\": g[\"score\"],\n",
        "                \"suggested_filename\": filename,\n",
        "                \"input_constraints_count\": len(input_constraints),\n",
        "            },\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(USER, ensure_ascii=False, indent=2)}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        outf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å ‚Üí\", LLM_OUT)\n",
        "print(\"Ï¥ù Î™©Ìëú:\", len(ranked_sorted))\n",
        "print(\"ÏòàÏãú ÌååÏùºÎ™Ö:\", suggest(ranked_sorted[0]))\n"
      ],
      "metadata": {
        "id": "dI-iQnUDhBwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9612b168-9fc3-404d-c481-23373f8a11df"
      },
      "id": "dI-iQnUDhBwy",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å ‚Üí /content/money-transfer-project-template-python/run_artifacts/run1/llm_prompts.jsonl\n",
            "Ï¥ù Î™©Ìëú: 10\n",
            "ÏòàÏãú ÌååÏùºÎ™Ö: test_gen_0001_activities_refund.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-4) ÌÖåÏä§Ìä∏ ÏΩîÎìú ÏÉùÏÑ±\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import ast\n",
        "from pathlib import Path\n",
        "import httpx\n",
        "import backoff\n",
        "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
        "\n",
        "# ---------- Í≤ΩÎ°ú ÏÑ§Ï†ï ----------\n",
        "ART_DIR = Path(PROJ) / \"run_artifacts\" / \"run1\"\n",
        "LLM_PROMPTS_PATH = ART_DIR / \"llm_prompts.jsonl\"\n",
        "GEN_DIR = Path(PROJ) / \"generated_tests\"\n",
        "RAW_DIR = ART_DIR / \"_raw\"\n",
        "ERR_DIR = ART_DIR / \"_errors\"\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ERR_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- OpenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ----------\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. 3-0 Îã®Í≥ÑÏóêÏÑú .envÎ•º Î°úÎìúÌñàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")\n",
        "\n",
        "http_client = httpx.Client(\n",
        "    timeout=180.0,\n",
        "    follow_redirects=True,\n",
        "    limits=httpx.Limits(max_connections=1, max_keepalive_connections=0),\n",
        "    transport=httpx.HTTPTransport(retries=5),\n",
        ")\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    http_client=http_client,\n",
        ")\n",
        "\n",
        "# ---------- Ïú†Ìã∏ ----------\n",
        "_slug_re = re.compile(r\"[^a-z0-9_]+\")\n",
        "def slugify(s: str, maxlen: int = 40) -> str:\n",
        "    s = s.lower().strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
        "    s = _slug_re.sub(\"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return s[:maxlen] or \"t\"\n",
        "\n",
        "def strip_fences(s: str) -> str:\n",
        "    s = re.sub(r\"^```[a-zA-Z0-9]*\\s*\", \"\", s.strip())\n",
        "    s = re.sub(r\"\\s*```$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def ensure_unique_path(base: Path) -> Path:\n",
        "    p = base\n",
        "    i = 2\n",
        "    while p.exists():\n",
        "        p = base.with_name(f\"{base.stem}_{i}{base.suffix}\")\n",
        "        i += 1\n",
        "    return p\n",
        "\n",
        "def write_error(goal_id: str, kind: str, payload: dict, idx: int | None = None):\n",
        "    tag = f\"goal_{goal_id}_{kind}\" if idx is None else f\"goal_{goal_id}_t{idx}_{kind}\"\n",
        "    (ERR_DIR / f\"{tag}.json\").write_text(\n",
        "        json.dumps(payload, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "# ---------- PATCH (1): Î≥¥Ï†ïÍ∏∞ - ÌïÑÏàò import ÏûêÎèô Ï£ºÏûÖ Í∞ïÌôî ----------\n",
        "def auto_fix_imports(code: str) -> str:\n",
        "    \"\"\"\n",
        "    Í∏∞Ï°¥ Í∏∞Îä• + Í∞ïÌôîÎêú import ÏûêÎèô ÏÇΩÏûÖ\n",
        "    - pytest\n",
        "    - importlib\n",
        "    - SimpleNamespace\n",
        "    - asyncio\n",
        "    \"\"\"\n",
        "    lines = code.splitlines()\n",
        "    text = \"\\n\".join(lines)\n",
        "\n",
        "    need_pytest = (\"pytest.\" in text) and not re.search(r\"^\\s*import\\s+pytest\\b\", text, re.M)\n",
        "    need_importlib = (\"importlib.\" in text) and not re.search(r\"^\\s*import\\s+importlib\\b\", text, re.M)\n",
        "    need_simple = (\"SimpleNamespace\" in text) and not re.search(r\"from\\s+types\\s+import\\s+SimpleNamespace\", text, re.M)\n",
        "    need_asyncio = (\"asyncio.\" in text) and not re.search(r\"^\\s*import\\s+asyncio\\b\", text, re.M)\n",
        "\n",
        "    prepend = []\n",
        "    if need_pytest: prepend.append(\"import pytest\")\n",
        "    if need_importlib: prepend.append(\"import importlib\")\n",
        "    if need_simple: prepend.append(\"from types import SimpleNamespace\")\n",
        "    if need_asyncio: prepend.append(\"import asyncio\")\n",
        "\n",
        "    if not prepend:\n",
        "        return code\n",
        "\n",
        "    # from __future__ import ... Îí§Ïóê ÎÑ£Í∏∞\n",
        "    new_lines = []\n",
        "    inserted = False\n",
        "    for ln in lines:\n",
        "        new_lines.append(ln)\n",
        "        if (not inserted) and not ln.strip().startswith(\"from __future__\"):\n",
        "            new_lines = prepend + new_lines\n",
        "            inserted = True\n",
        "            break\n",
        "\n",
        "    if not inserted:\n",
        "        new_lines = prepend + lines\n",
        "\n",
        "    return \"\\n\".join(new_lines) + (\"\\n\" if not new_lines[-1].endswith(\"\\n\") else \"\")\n",
        "\n",
        "# ---------- Î≥¥Ï†ïÍ∏∞: runner Ï†úÍ±∞ ----------\n",
        "def sanitize_test_code(code: str) -> str:\n",
        "    patterns = [\n",
        "        re.compile(r\"(?ms)^\\s*if\\s+__name__\\s*==\\s*['\\\"]__main__['\\\"]\\s*:\\s*\\n(?:\\s+.*\\n?)+$\"),\n",
        "        re.compile(r\"(?m)^\\s*pytest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "        re.compile(r\"(?m)^\\s*unittest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "    ]\n",
        "    new = code\n",
        "    for pat in patterns:\n",
        "        new = pat.sub(\"\", new)\n",
        "    return new.strip() + \"\\n\"\n",
        "\n",
        "# ---------- Í≤ÄÏ¶ù Î°úÏßÅ ----------\n",
        "RE_IMPORTLIB = re.compile(r\"\\bimportlib\\.import_module\\s*\\(\")\n",
        "RE_PYTEST_RAISES = re.compile(r\"\\bpytest\\.raises\\s*\\(\")\n",
        "\n",
        "def validate_json_schema(result: dict) -> tuple[bool, list[str]]:\n",
        "    reasons = []\n",
        "    if not isinstance(result, dict):\n",
        "        return False, [\"not_a_json_object\"]\n",
        "    if \"tests\" not in result:\n",
        "        reasons.append(\"missing_tests\")\n",
        "    else:\n",
        "        if not isinstance(result[\"tests\"], list) or len(result[\"tests\"]) == 0:\n",
        "            reasons.append(\"tests_empty_or_not_list\")\n",
        "        else:\n",
        "            for i, t in enumerate(result[\"tests\"], start=1):\n",
        "                if not isinstance(t, dict):\n",
        "                    reasons.append(f\"test_{i}_not_object\"); continue\n",
        "                if \"code\" not in t:\n",
        "                    reasons.append(f\"test_{i}_missing_code\")\n",
        "                if \"name\" not in t:\n",
        "                    reasons.append(f\"test_{i}_missing_name\")\n",
        "    if \"filename\" in result:\n",
        "        fn = str(result[\"filename\"])\n",
        "        if not fn.endswith(\".py\"):\n",
        "            reasons.append(\"filename_not_py\")\n",
        "    return (len(reasons) == 0), reasons\n",
        "\n",
        "def parse_ast_or_error(code: str):\n",
        "    try:\n",
        "        return ast.parse(code), None\n",
        "    except SyntaxError as e:\n",
        "        return None, f\"syntax_error:{e.msg}@L{e.lineno}\"\n",
        "\n",
        "def extract_test_funcs(tree: ast.AST) -> list[ast.FunctionDef]:\n",
        "    return [n for n in ast.walk(tree)\n",
        "            if isinstance(n, ast.FunctionDef) and n.name.startswith(\"test_\")]\n",
        "\n",
        "def has_assert_or_raises(tree: ast.AST, code: str) -> bool:\n",
        "    has_assert_stmt = any(isinstance(n, ast.Assert) for n in ast.walk(tree))\n",
        "    has_pytest_raises = bool(RE_PYTEST_RAISES.search(code))\n",
        "    return has_assert_stmt or has_pytest_raises\n",
        "\n",
        "def uses_importlib(code: str) -> bool:\n",
        "    return bool(RE_IMPORTLIB.search(code))\n",
        "\n",
        "# ---------- skip Í∞ÄÎìú ----------\n",
        "def _parent_map(tree: ast.AST):\n",
        "    parent = {}\n",
        "    for node in ast.walk(tree):\n",
        "        for child in ast.iter_child_nodes(node):\n",
        "            parent[child] = node\n",
        "    return parent\n",
        "\n",
        "def _is_guarded_skip(call: ast.Call, parent_map, src: str) -> bool:\n",
        "    cur = call\n",
        "    while cur in parent_map:\n",
        "        cur = parent_map[cur]\n",
        "        if isinstance(cur, ast.If):\n",
        "            test = cur.test\n",
        "            if (\n",
        "                isinstance(test, ast.Compare)\n",
        "                and len(test.ops) == 1\n",
        "                and isinstance(test.ops[0], ast.Is)\n",
        "                and len(test.comparators) == 1\n",
        "                and isinstance(test.comparators[0], ast.Constant)\n",
        "                and test.comparators[0].value is None\n",
        "            ):\n",
        "                return True\n",
        "        if isinstance(cur, ast.ExceptHandler):\n",
        "            t = cur.type\n",
        "            if isinstance(t, ast.Name) and t.id in {\"ImportError\", \"NameError\"}:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def has_unconditional_skip(code: str) -> tuple[bool, list[int]]:\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "    except SyntaxError:\n",
        "        return (False, [])\n",
        "    parent = _parent_map(tree)\n",
        "    bad_lines = []\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.Call):\n",
        "            f = node.func\n",
        "            if (\n",
        "                isinstance(f, ast.Attribute)\n",
        "                and isinstance(f.value, ast.Name)\n",
        "                and f.value.id == \"pytest\"\n",
        "                and f.attr == \"skip\"\n",
        "            ):\n",
        "                if not _is_guarded_skip(node, parent, code):\n",
        "                    bad_lines.append(getattr(node, \"lineno\", -1))\n",
        "    return (len(bad_lines) > 0, bad_lines)\n",
        "\n",
        "# ---------- PATCH (2): ÏµúÏÜå Ïã§Ìñâ ÏöîÍ±¥ ÏôÑÌôî ----------\n",
        "def minimal_viability_checks(code: str) -> tuple[bool, list[str], dict]:\n",
        "    reasons = []\n",
        "    meta = {\"warnings\": []}\n",
        "\n",
        "    # ÏïÑÏ£º ÏßßÏùÄ ÏΩîÎìú Í≤ΩÍ≥†Îßå\n",
        "    if len(code.strip()) < 50:\n",
        "        meta[\"warnings\"].append(\"very_short_code\")\n",
        "\n",
        "    tree, synerr = parse_ast_or_error(code)\n",
        "    if synerr:\n",
        "        reasons.append(synerr)\n",
        "        return False, reasons, meta\n",
        "\n",
        "    tests = extract_test_funcs(tree)\n",
        "    if not tests:\n",
        "        reasons.append(\"no_test_functions\")\n",
        "\n",
        "    if not has_assert_or_raises(tree, code):\n",
        "        reasons.append(\"no_assert_or_raises\")\n",
        "\n",
        "    # importlib ÏóÜÎäî Í≤ΩÏö∞ Í≤ΩÍ≥†Îßå\n",
        "    if not uses_importlib(code):\n",
        "        meta[\"warnings\"].append(\"no_importlib_import_module\")\n",
        "\n",
        "    # forbidden import from direct absolute usage (ÏÉàÎ°úÏö¥ fail Ï°∞Í±¥)\n",
        "    if re.search(r\"from\\s+[\\w\\.]+\\s+import\\s+\\w+\", code):\n",
        "        meta[\"warnings\"].append(\"direct_from_import_detected\")\n",
        "\n",
        "    has_bad_skip, bad_lines = has_unconditional_skip(code)\n",
        "    if has_bad_skip:\n",
        "        reasons.append(f\"unconditional_skip_detected@{bad_lines}\")\n",
        "\n",
        "    # runner Í∏àÏßÄ\n",
        "    if re.search(r\"pytest\\.main\\s*\\(\", code):\n",
        "        reasons.append(\"forbidden_runner_invocation:pytest.main\")\n",
        "    if re.search(r\"unittest\\.main\\s*\\(\", code):\n",
        "        reasons.append(\"forbidden_runner_invocation:unittest.main\")\n",
        "\n",
        "    return (len(reasons) == 0), reasons, meta\n",
        "\n",
        "# ---------- OpenAI ----------\n",
        "@backoff.on_exception(\n",
        "    backoff.expo,\n",
        "    (APIConnectionError, APIError, RateLimitError),\n",
        "    max_tries=8,\n",
        "    max_time=300\n",
        ")\n",
        "def call_openai_with_retry(messages):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        timeout=180.0,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "# ---------- PATCH (3): filename Ï†ïÏ±Ö Í∞ïÌôî ----------\n",
        "def choose_filename(base_name: str | None, goal_id: str, idx: int, tests_len: int):\n",
        "    \"\"\"\n",
        "    ÏÉàÎ°úÏö¥ filename Ï†ïÏ±Ö:\n",
        "    - result.filename ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú Ïö∞ÏÑ† ÏÇ¨Ïö©\n",
        "    - Îã®, Îã§Ï§ë ÌÖåÏä§Ìä∏Ïù∏ Í≤ΩÏö∞ suffix Î∂ôÏûÑ\n",
        "    \"\"\"\n",
        "    if base_name:\n",
        "        base = Path(base_name).stem\n",
        "        if tests_len > 1:\n",
        "            return f\"{base}_{idx}.py\"\n",
        "        else:\n",
        "            return f\"{base}.py\"\n",
        "    else:\n",
        "        slug = slugify(f\"goal_{goal_id}\")\n",
        "        if tests_len > 1:\n",
        "            return f\"test_{goal_id}_{slug}_{idx}.py\"\n",
        "        return f\"test_{goal_id}_{slug}.py\"\n",
        "\n",
        "\n",
        "# ---------- Î©îÏù∏ Î£®ÌîÑ ----------\n",
        "gen_log_path = ART_DIR / \"gen_log.jsonl\"\n",
        "ok_count = 0\n",
        "fail_count = 0\n",
        "\n",
        "with LLM_PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f_in, gen_log_path.open(\"w\", encoding=\"utf-8\") as f_log:\n",
        "    for line in f_in:\n",
        "        rec = json.loads(line)\n",
        "        goal_id = rec[\"meta\"][\"id\"]\n",
        "        messages = rec[\"messages\"]\n",
        "        print(f\"\\nüöÄ Goal {goal_id} ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\")\n",
        "\n",
        "        try:\n",
        "            out_text = call_openai_with_retry(messages)\n",
        "        except Exception as e:\n",
        "            fail_count += 1\n",
        "            write_error(goal_id, \"request_error\", {\"error\": str(e)})\n",
        "            print(f\"‚ùå Goal {goal_id} ÏöîÏ≤≠ Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        (RAW_DIR / f\"goal_{goal_id}_raw.json\").write_text(out_text, encoding=\"utf-8\")\n",
        "\n",
        "        try:\n",
        "            cleaned = strip_fences(out_text)\n",
        "            result = json.loads(cleaned)\n",
        "        except Exception as e:\n",
        "            fail_count += 1\n",
        "            write_error(goal_id, \"json_parse_error\", {\"error\": str(e), \"raw\": out_text[:2000]})\n",
        "            print(f\"‚ùå Goal {goal_id} JSON ÌååÏã± Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        ok_schema, schema_reasons = validate_json_schema(result)\n",
        "        if not ok_schema:\n",
        "            fail_count += 1\n",
        "            write_error(goal_id, \"schema_error\", {\"reasons\": schema_reasons, \"result\": result})\n",
        "            print(f\"‚ùå Goal {goal_id} Ïä§ÌÇ§Îßà Ïò§Î•ò: {schema_reasons}\")\n",
        "            continue\n",
        "\n",
        "        tests = result.get(\"tests\", [])\n",
        "        base_name = result.get(\"filename\")\n",
        "\n",
        "        saved_files = []\n",
        "        excluded = []\n",
        "\n",
        "        for idx, t in enumerate(tests, start=1):\n",
        "            code = strip_fences(t.get(\"code\", \"\"))\n",
        "            code = sanitize_test_code(code)\n",
        "            code = auto_fix_imports(code)\n",
        "\n",
        "            ok_min, reasons, meta = minimal_viability_checks(code)\n",
        "            if not ok_min:\n",
        "                excluded.append({\"index\": idx, \"name\": t.get(\"name\"), \"reasons\": reasons, **meta})\n",
        "                write_error(goal_id, \"min_viability\", {\"index\": idx, \"name\": t.get(\"name\"), \"reasons\": reasons, **meta}, idx)\n",
        "                print(f\"‚ö†Ô∏è Goal {goal_id} ÌÖåÏä§Ìä∏ #{idx} Ï†úÏô∏: {reasons}\")\n",
        "                continue\n",
        "\n",
        "            out_filename = choose_filename(base_name, goal_id, idx, len(tests))\n",
        "            out_path = ensure_unique_path(GEN_DIR / out_filename)\n",
        "            out_path.write_text(code, encoding=\"utf-8\")\n",
        "            saved_files.append(out_path.name)\n",
        "            print(f\"‚úÖ Ï†ÄÏû•: {out_path.name} (warnings: {','.join(meta.get('warnings', [])) or 'ÏóÜÏùå'})\")\n",
        "\n",
        "        if saved_files:\n",
        "            ok_count += 1\n",
        "            f_log.write(json.dumps({\"goal_id\": goal_id, \"saved_files\": saved_files, \"excluded_tests\": excluded}, ensure_ascii=False) + \"\\n\")\n",
        "        else:\n",
        "            fail_count += 1\n",
        "            write_error(goal_id, \"no_valid_tests\", {\"result_head\": result, \"excluded_tests\": excluded})\n",
        "            print(f\"‚ùå Goal {goal_id} Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\")\n",
        "\n",
        "print(f\"\\n‚úÖ ÏÉùÏÑ± Îã®Í≥Ñ Ï¢ÖÎ£å: ÏÑ±Í≥µ {ok_count} / Ïã§Ìå® {fail_count}\")\n",
        "print(f\"   ‚Ä¢ Ï†ÄÏû• Ìè¥Îçî : {GEN_DIR}\")\n",
        "print(f\"   ‚Ä¢ ÏõêÎ¨∏ Î≥¥Í¥Ä : {RAW_DIR}\")\n",
        "print(f\"   ‚Ä¢ ÏóêÎü¨/Ï†úÏô∏ : {ERR_DIR}\")\n",
        "print(f\"   ‚Ä¢ Î°úÍ∑∏ ÌååÏùº : {gen_log_path}\")\n"
      ],
      "metadata": {
        "id": "pYNqW94ih1mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2ff878-e62a-42a1-cee2-d7c6672241e0"
      },
      "id": "pYNqW94ih1mr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Goal 0001 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0001_activities_refund_1.py (warnings: ÏóÜÏùå)\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0001_activities_refund_2.py (warnings: ÏóÜÏùå)\n",
            "\n",
            "üöÄ Goal 0002 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0002_activities_refund_1.py (warnings: ÏóÜÏùå)\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0002_activities_refund_2.py (warnings: ÏóÜÏùå)\n",
            "\n",
            "üöÄ Goal 0003 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0003_activities_refund_1.py (warnings: ÏóÜÏùå)\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0003_activities_refund_2.py (warnings: ÏóÜÏùå)\n",
            "\n",
            "üöÄ Goal 0004 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0004 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions', 'unconditional_skip_detected@[12]']\n",
            "‚ö†Ô∏è Goal 0004 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['no_test_functions', 'unconditional_skip_detected@[12]']\n",
            "‚ùå Goal 0004 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0005 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0005 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions', 'unconditional_skip_detected@[12]']\n",
            "‚ö†Ô∏è Goal 0005 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['no_test_functions', 'unconditional_skip_detected@[12]']\n",
            "‚ùå Goal 0005 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0006 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0006 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['unconditional_skip_detected@[9]']\n",
            "‚ö†Ô∏è Goal 0006 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['unconditional_skip_detected@[9]']\n",
            "‚ùå Goal 0006 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0007 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0007 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['unconditional_skip_detected@[11]']\n",
            "‚ö†Ô∏è Goal 0007 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['unconditional_skip_detected@[11]']\n",
            "‚ùå Goal 0007 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0008 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0008 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ö†Ô∏è Goal 0008 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ùå Goal 0008 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0009 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 0009 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ö†Ô∏è Goal 0009 ÌÖåÏä§Ìä∏ #2 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ùå Goal 0009 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå ‚Üí Í∏∞Î°ùÎßå ÎÇ®ÍπÄ\n",
            "\n",
            "üöÄ Goal 0010 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0010_activities_deposit_1.py (warnings: ÏóÜÏùå)\n",
            "‚úÖ Ï†ÄÏû•: test_gen_0010_activities_deposit_2.py (warnings: ÏóÜÏùå)\n",
            "\n",
            "‚úÖ ÏÉùÏÑ± Îã®Í≥Ñ Ï¢ÖÎ£å: ÏÑ±Í≥µ 4 / Ïã§Ìå® 6\n",
            "   ‚Ä¢ Ï†ÄÏû• Ìè¥Îçî : /content/money-transfer-project-template-python/generated_tests\n",
            "   ‚Ä¢ ÏõêÎ¨∏ Î≥¥Í¥Ä : /content/money-transfer-project-template-python/run_artifacts/run1/_raw\n",
            "   ‚Ä¢ ÏóêÎü¨/Ï†úÏô∏ : /content/money-transfer-project-template-python/run_artifacts/run1/_errors\n",
            "   ‚Ä¢ Î°úÍ∑∏ ÌååÏùº : /content/money-transfer-project-template-python/run_artifacts/run1/gen_log.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-5) Í∏∞Ï°¥ tests(ÌîÑÎ£®Îãù) + ÏÉùÏÑ± tests Í≤©Î¶¨ Ïã§Ìñâ ¬∑ Î°úÍ∑∏ ÏàòÏßë ¬∑ ÏÉ§Îìú Í≤∞Ìï© ¬∑ Ìñ•ÏÉÅÏπò Í≥ÑÏÇ∞ (autosetup Ìè¨Ìï®)\n",
        "import os, sys, json, re, time, subprocess, shutil, shlex\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from lxml import etree\n",
        "\n",
        "# ==== Í≤ΩÎ°ú/ÏÉÅÏàò ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "TESTS_DIR = PROJ / \"tests\"                   # ÏõêÎ≥∏(Î∞±ÏóÖÏö©)\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"      # ÌîÑÎ£®Îãù ÏßëÌï©(Ïö∞ÏÑ†)\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "COV_SHARDS_DIR = ART_DIR / \"cov_shards\"\n",
        "HTML_DIR_GEN = PROJ / \"htmlcov_gen\"\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "COV_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR_GEN.mkdir(parents=True, exist_ok=True)\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "\n",
        "# Ïã§Ìñâ ÌååÎùºÎØ∏ÌÑ∞\n",
        "PY_EXE = sys.executable\n",
        "TIMEOUT_SEC_GEN = 45            # ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº 1Í∞úÎãπ ÌÉÄÏûÑÏïÑÏõÉ (ÎäòÎ¶º)\n",
        "TIMEOUT_SEC_BASE = 45           # (ÌîÑÎ£®ÎãùÎêú) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº 1Í∞úÎãπ ÌÉÄÏûÑÏïÑÏõÉ\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "ENV_BASE = os.environ.copy()\n",
        "\n",
        "# goal_id Ï∂îÏ∂ú\n",
        "RE_GOAL = re.compile(r\"(?:^|[_-])(?P<gid>\\d{4})(?:[_-]|$)\")\n",
        "\n",
        "# ==== Autosetup: pytest.ini + generated_tests/conftest.py ====\n",
        "def ensure_pytest_ini():\n",
        "    ini = PROJ / \"pytest.ini\"\n",
        "    base = []\n",
        "    if ini.exists():\n",
        "        base = ini.read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "    def set_or_append(lines, key, value):\n",
        "        if not any(l.strip().startswith(\"[pytest]\") for l in lines):\n",
        "            lines = [\"[pytest]\"] + lines\n",
        "        found = False\n",
        "        for i, l in enumerate(lines):\n",
        "            if l.strip().startswith(key):\n",
        "                lines[i] = f\"{key} = {value}\"\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            try:\n",
        "                idx = next(i for i,l in enumerate(lines) if l.strip().startswith(\"[pytest]\"))\n",
        "            except StopIteration:\n",
        "                idx = -1\n",
        "            insert_at = idx+1 if idx>=0 else len(lines)\n",
        "            lines.insert(insert_at, f\"{key} = {value}\")\n",
        "        return lines\n",
        "\n",
        "    lines = base[:]\n",
        "    lines = set_or_append(lines, \"asyncio_mode\", \"auto\")  # pytest-asyncio ÏûêÎèô Î™®Îìú\n",
        "    if not any(l.strip().startswith(\"addopts\") for l in lines):\n",
        "        lines.append(\"addopts = -q -s\")\n",
        "    ini.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"‚úÖ pytest.ini ensured ‚Üí\", ini)\n",
        "\n",
        "def ensure_conftest_autouse():\n",
        "    cf = GEN_DIR / \"conftest.py\"\n",
        "    payload = (\n",
        "        \"import os, asyncio, pytest, importlib, inspect\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _no_real_net_env(monkeypatch):\\n\"\n",
        "        \"    monkeypatch.setenv('NO_PROXY', '*')\\n\"\n",
        "        \"    for k in ['HTTP_PROXY','HTTPS_PROXY','http_proxy','https_proxy','ALL_PROXY','all_proxy']:\\n\"\n",
        "        \"        monkeypatch.delenv(k, raising=False)\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _patch_time(monkeypatch):\\n\"\n",
        "        \"    import time\\n\"\n",
        "        \"    monkeypatch.setattr(time, 'sleep', lambda *_: None, raising=False)\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _patch_temporal(monkeypatch):\\n\"\n",
        "        \"    try:\\n\"\n",
        "        \"        import temporalio  # noqa: F401\\n\"\n",
        "        \"    except Exception:\\n\"\n",
        "        \"        return\\n\"\n",
        "        \"    class _Dummy:\\n\"\n",
        "        \"        def __init__(self, *a, **k): pass\\n\"\n",
        "        \"        async def __aenter__(self): return self\\n\"\n",
        "        \"        async def __aexit__(self, *a): pass\\n\"\n",
        "        \"        def __call__(self, *a, **k): return self\\n\"\n",
        "        \"        async def run(self, *a, **k): return None\\n\"\n",
        "        \"        async def start(self, *a, **k): return None\\n\"\n",
        "        \"    for path in ['temporalio.client.Client','temporalio.worker.Worker','temporalio.workflow.Workflow']:\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            mod_name, attr = path.rsplit('.', 1)\\n\"\n",
        "        \"            mod = importlib.import_module(mod_name)\\n\"\n",
        "        \"            setattr(mod, attr, _Dummy)\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            pass\\n\"\n",
        "        \"\\n\"\n",
        "        \"def _wrap_async(func):\\n\"\n",
        "        \"    if inspect.iscoroutinefunction(func):\\n\"\n",
        "        \"        def sync_wrapper(*a, **k):\\n\"\n",
        "        \"            try:\\n\"\n",
        "        \"                loop = asyncio.get_event_loop()\\n\"\n",
        "        \"            except RuntimeError:\\n\"\n",
        "        \"                loop = asyncio.new_event_loop()\\n\"\n",
        "        \"                asyncio.set_event_loop(loop)\\n\"\n",
        "        \"            return loop.run_until_complete(func(*a, **k))\\n\"\n",
        "        \"        return sync_wrapper\\n\"\n",
        "        \"    return func\\n\"\n",
        "        \"\\n\"\n",
        "        \"_TARGET_MODULES = [\\n\"\n",
        "        \"    'run_worker_module',\\n\"\n",
        "        \"    'run_workflow_module',\\n\"\n",
        "        \"    'run_workflow_main',\\n\"\n",
        "        \"]\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _wrap_async_entrypoints(monkeypatch):\\n\"\n",
        "        \"    for mod_name in _TARGET_MODULES:\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            mod = importlib.import_module(mod_name)\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            continue\\n\"\n",
        "        \"        for attr in ('main','run','start'):\\n\"\n",
        "        \"            if hasattr(mod, attr):\\n\"\n",
        "        \"                try:\\n\"\n",
        "        \"                    monkeypatch.setattr(mod, attr, _wrap_async(getattr(mod, attr)), raising=False)\\n\"\n",
        "        \"                except Exception:\\n\"\n",
        "        \"                    pass\\n\"\n",
        "        \"    def _safe_run(coro):\\n\"\n",
        "        \"        if inspect.iscoroutine(coro):\\n\"\n",
        "        \"            try:\\n\"\n",
        "        \"                loop = asyncio.get_event_loop()\\n\"\n",
        "        \"            except RuntimeError:\\n\"\n",
        "        \"                loop = asyncio.new_event_loop()\\n\"\n",
        "        \"                asyncio.set_event_loop(loop)\\n\"\n",
        "        \"            return loop.run_until_complete(coro)\\n\"\n",
        "        \"        return coro\\n\"\n",
        "        \"    monkeypatch.setattr(asyncio, 'run', _safe_run, raising=False)\\n\"\n",
        "    )\n",
        "    if cf.exists():\n",
        "        text = cf.read_text(encoding=\"utf-8\")\n",
        "        if \"_wrap_async_entrypoints\" not in text:\n",
        "            text = text.rstrip() + \"\\n\\n\" + payload\n",
        "            cf.write_text(text, encoding=\"utf-8\")\n",
        "            print(\"‚úÖ conftest.py augmented ‚Üí\", cf)\n",
        "        else:\n",
        "            print(\"‚úÖ conftest.py already has autosetup ‚Üí\", cf)\n",
        "    else:\n",
        "        cf.write_text(payload, encoding=\"utf-8\")\n",
        "        print(\"‚úÖ conftest.py created ‚Üí\", cf)\n",
        "\n",
        "ensure_pytest_ini()\n",
        "ensure_conftest_autouse()\n",
        "\n",
        "# ==== Ïú†Ìã∏ ====\n",
        "def goal_id_from_name(name: str) -> str | None:\n",
        "    m = RE_GOAL.search(name)\n",
        "    return m.group(\"gid\") if m else None\n",
        "\n",
        "def sh(cmd: str, cwd: Path | None = None, timeout: int | None = None, env: dict | None = None):\n",
        "    try:\n",
        "        p = subprocess.run(\n",
        "            cmd, cwd=str(cwd or PROJ), env=env or ENV_BASE,\n",
        "            shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "            timeout=timeout, text=True\n",
        "        )\n",
        "        return p.returncode, p.stdout, p.stderr, False\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        return 124, e.stdout or \"\", e.stderr or \"\", True\n",
        "\n",
        "def list_test_files(dir_path: Path) -> list[Path]:\n",
        "    if not dir_path.exists():\n",
        "        return []\n",
        "    files = []\n",
        "    files += list(dir_path.glob(\"test*.py\"))\n",
        "    files += list(dir_path.glob(\"*_test.py\"))\n",
        "    return sorted(set(p for p in files if p.is_file()))\n",
        "\n",
        "def list_generated_test_files() -> list[Path]:\n",
        "    if not GEN_DIR.exists():\n",
        "        return []\n",
        "    # conftest.pyÎäî ÏàòÏßë ÎåÄÏÉÅÏóêÏÑú Ï†úÏô∏\n",
        "    return sorted([p for p in GEN_DIR.glob(\"*.py\") if p.is_file() and p.name != \"conftest.py\"])\n",
        "\n",
        "def rel_to_proj(p: Path) -> str:\n",
        "    try:\n",
        "        return str(p.resolve().relative_to(PROJ))\n",
        "    except Exception:\n",
        "        return str(p.resolve())\n",
        "\n",
        "# ==== 0) ÏÇ∞Ï∂úÎ¨º ÌååÏùº Í≤ΩÎ°ú ====\n",
        "results_jsonl = ART_DIR / \"results.jsonl\"\n",
        "manifest_path = ART_DIR / \"manifest.json\"\n",
        "coverage_json_path = ART_DIR / \"coverage_gen.json\"     # Ïù¥Î≤à ÎùºÏö¥Îìú ÌÜµÌï©(ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±)\n",
        "coverage_xml_path  = ART_DIR / \"coverage_gen.xml\"\n",
        "coverage_base_json = ART_DIR / \"coverage_base.json\"    # 3-1 Í∏∞Ï§ÄÏÑ†(ÏõêÎ≥∏ Í∏∞Ï°¥ ÌÖåÏä§Ìä∏)\n",
        "\n",
        "for old in [results_jsonl, coverage_json_path, coverage_xml_path]:\n",
        "    if old.exists():\n",
        "        old.unlink()\n",
        "\n",
        "runs = []\n",
        "ok = fail = to_cnt = 0\n",
        "\n",
        "# ==== 1) (ÌîÑÎ£®ÎãùÎêú) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ Í∞úÎ≥Ñ Í≤©Î¶¨ Ïã§Ìñâ ====\n",
        "base_root = PRUNED_DIR if PRUNED_DIR.exists() else TESTS_DIR\n",
        "base_files = list_test_files(base_root)\n",
        "\n",
        "if base_files:\n",
        "    print(f\"üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: {len(base_files)}Í∞ú @ {rel_to_proj(base_root)}\")\n",
        "    for tf in base_files:\n",
        "        name = tf.name\n",
        "        shard = COV_SHARDS_DIR / f\".coverage.__base__.{name}\"\n",
        "\n",
        "        env = ENV_BASE.copy()\n",
        "        env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "        env[\"COVERAGE_FILE\"] = str(shard)\n",
        "        env.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "        target = rel_to_proj(tf)\n",
        "        cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "\n",
        "        start = time.time()\n",
        "        ts_start = datetime.now(timezone.utc).isoformat()\n",
        "        rc, out, err, timed_out = sh(cmd, cwd=PROJ, timeout=TIMEOUT_SEC_BASE, env=env)\n",
        "        dur = round(time.time() - start, 3)\n",
        "        ts_end = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "        (LOG_DIR / f\"__base__{name}.out.txt\").write_text(out, encoding=\"utf-8\")\n",
        "        (LOG_DIR / f\"__base__{name}.err.txt\").write_text(err, encoding=\"utf-8\")\n",
        "\n",
        "        runs.append({\n",
        "            \"suite\": \"BASE\",\n",
        "            \"test_file\": name,\n",
        "            \"goal_id\": None,\n",
        "            \"start_utc\": ts_start,\n",
        "            \"end_utc\": ts_end,\n",
        "            \"duration_sec\": dur,\n",
        "            \"returncode\": rc,\n",
        "            \"timed_out\": timed_out,\n",
        "            \"stdout_len\": len(out),\n",
        "            \"stderr_len\": len(err),\n",
        "            \"shard_path\": str(shard),\n",
        "            \"invoked_path\": target,\n",
        "        })\n",
        "\n",
        "        if timed_out:\n",
        "            to_cnt += 1\n",
        "            print(f\"‚è±Ô∏è TIMEOUT [BASE] {name} ({dur}s)\")\n",
        "        elif rc == 0:\n",
        "            ok += 1\n",
        "            print(f\"‚úÖ PASS   [BASE] {name} ({dur}s)\")\n",
        "        else:\n",
        "            fail += 1\n",
        "            first_err = (err.strip().splitlines() or [''])[0]\n",
        "            print(f\"‚ùå FAIL   [BASE] {name} (rc={rc}, {dur}s) :: {first_err}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ BASE Ïã§ÌñâÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§. (Pruned_Base_Tests/ ÎòêÎäî tests/ ÎπÑÏñ¥ÏûàÏùå)\")\n",
        "\n",
        "# ==== 2) ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº Í∞úÎ≥Ñ Í≤©Î¶¨ Ïã§Ìñâ ====\n",
        "test_files = list_generated_test_files()\n",
        "print(f\"üß™ ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº: {len(test_files)}Í∞ú @ {rel_to_proj(GEN_DIR)}\")\n",
        "for tf in test_files:\n",
        "    name = tf.name\n",
        "    gid = goal_id_from_name(name) or \"----\"\n",
        "    shard = COV_SHARDS_DIR / f\".coverage.{name}\"\n",
        "\n",
        "    env = ENV_BASE.copy()\n",
        "    env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "    env.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "    target = rel_to_proj(tf)   # e.g. \"generated_tests/test_0001_...py\"\n",
        "    cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "\n",
        "    start = time.time()\n",
        "    ts_start = datetime.now(timezone.utc).isoformat()\n",
        "    rc, out, err, timed_out = sh(cmd, cwd=PROJ, timeout=TIMEOUT_SEC_GEN, env=env)\n",
        "    dur = round(time.time() - start, 3)\n",
        "    ts_end = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "    (LOG_DIR / f\"{name}.out.txt\").write_text(out, encoding=\"utf-8\")\n",
        "    (LOG_DIR / f\"{name}.err.txt\").write_text(err, encoding=\"utf-8\")\n",
        "\n",
        "    runs.append({\n",
        "        \"suite\": \"GEN\",\n",
        "        \"test_file\": name,\n",
        "        \"goal_id\": gid,\n",
        "        \"start_utc\": ts_start,\n",
        "        \"end_utc\": ts_end,\n",
        "        \"duration_sec\": dur,\n",
        "        \"returncode\": rc,\n",
        "        \"timed_out\": timed_out,\n",
        "        \"stdout_len\": len(out),\n",
        "        \"stderr_len\": len(err),\n",
        "        \"shard_path\": str(shard),\n",
        "        \"invoked_path\": target,\n",
        "    })\n",
        "\n",
        "    if timed_out:\n",
        "        to_cnt += 1\n",
        "        print(f\"‚è±Ô∏è TIMEOUT [GEN] {name} ({dur}s)\")\n",
        "    elif rc == 0:\n",
        "        ok += 1\n",
        "        print(f\"‚úÖ PASS   [GEN] {name} ({dur}s)\")\n",
        "    else:\n",
        "        fail += 1\n",
        "        first_err = (err.strip().splitlines() or [''])[0]\n",
        "        print(f\"‚ùå FAIL   [GEN] {name} (rc={rc}, {dur}s) :: {first_err}\")\n",
        "\n",
        "# Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•\n",
        "results_jsonl.write_text(\n",
        "    \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in runs) + (\"\\n\" if runs else \"\"),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "manifest = {\n",
        "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"project\": str(PROJ),\n",
        "    \"run_dir\": str(ART_DIR),\n",
        "    \"tests_total\": len([r for r in runs if r['suite'] in {'BASE','GEN'}]),\n",
        "    \"pass\": ok,\n",
        "    \"fail\": fail,\n",
        "    \"timeout\": to_cnt,\n",
        "    \"logs_dir\": str(LOG_DIR),\n",
        "    \"cov_shards_dir\": str(COV_SHARDS_DIR),\n",
        "    \"base_root\": str(rel_to_proj(base_root)) if base_files else None,\n",
        "    \"gen_root\": str(rel_to_proj(GEN_DIR)),\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"\\nüì¶ Ïã§Ìñâ ÏöîÏïΩ:\", json.dumps({\"pass\": ok, \"fail\": fail, \"timeout\": to_cnt, \"total\": manifest['tests_total']}, ensure_ascii=False))\n",
        "\n",
        "# ==== 3) Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï©(JSON/XML/HTML) ====\n",
        "shards = sorted([p for p in COV_SHARDS_DIR.iterdir() if p.name.startswith(\".coverage.\")])\n",
        "if not shards:\n",
        "    print(\"‚ö†Ô∏è Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÉ§ÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§. ÌÖåÏä§Ìä∏Í∞Ä Ï¶âÏãú Ïã§Ìå®ÌñàÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\")\n",
        "else:\n",
        "    subprocess.call(f\"coverage erase{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    combine_cmd = \"coverage combine\" + rc_opt + \" \" + \" \".join(shlex.quote(str(p)) for p in shards)\n",
        "    print(\"> \", combine_cmd)\n",
        "    subprocess.call(combine_cmd, shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage json -o {coverage_json_path.name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o {coverage_xml_path.name}{rc_opt}\",  shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage html -d {HTML_DIR_GEN.name}{rc_opt}\",       shell=True, cwd=str(PROJ))\n",
        "\n",
        "    # Í≤∞Í≥º ÌååÏùºÏùÑ run_artifactsÏóê Î≥µÏÇ¨ Î≥¥Í¥Ä\n",
        "    src_json = PROJ / coverage_json_path.name\n",
        "    src_xml  = PROJ / coverage_xml_path.name\n",
        "    if src_json.exists(): shutil.copy2(src_json, coverage_json_path)\n",
        "    if src_xml.exists():  shutil.copy2(src_xml,  coverage_xml_path)\n",
        "\n",
        "    print(\"‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\")\n",
        "    print(\" - JSON :\", coverage_json_path)\n",
        "    print(\" - XML  :\", coverage_xml_path)\n",
        "    print(\" - HTML :\", HTML_DIR_GEN / \"index.html\")\n",
        "\n",
        "# ==== 4) Î∂ÑÍ∏∞ Í¥ÄÏ∏°/Î™©Ìëú Îã¨ÏÑ±Î•† Í≥ÑÏÇ∞ ====\n",
        "observed_outcomes_gen = {}\n",
        "branch_points = full_hit = half_hit = zero_hit = 0\n",
        "\n",
        "if coverage_xml_path.exists():\n",
        "    try:\n",
        "        xml_root = etree.parse(str(coverage_xml_path)).getroot()\n",
        "        for cls in xml_root.findall(\".//class\"):\n",
        "            filename = cls.get(\"filename\") or \"\"\n",
        "            if not filename:\n",
        "                continue\n",
        "            abs_path = (PROJ / filename).resolve() if not Path(filename).is_absolute() else Path(filename)\n",
        "            for line in cls.findall(\"./lines/line\"):\n",
        "                if line.get(\"branch\") != \"true\":\n",
        "                    continue\n",
        "                try:\n",
        "                    num = int(line.get(\"number\"))\n",
        "                except Exception:\n",
        "                    continue\n",
        "                cond = line.get(\"condition-coverage\")  # \"50% (1/2)\"\n",
        "                covered = total = 0\n",
        "                if cond:\n",
        "                    m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cond)\n",
        "                    if m:\n",
        "                        covered, total = int(m.group(1)), int(m.group(2))\n",
        "                if total == 0:\n",
        "                    continue\n",
        "                observed_outcomes_gen.setdefault(str(abs_path), {})[num] = {\n",
        "                    \"covered\": covered, \"total\": total, \"ratio\": round(covered/total, 3)\n",
        "                }\n",
        "                branch_points += 1\n",
        "                if covered == 0: zero_hit += 1\n",
        "                elif covered == total: full_hit += 1\n",
        "                else: half_hit += 1\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è coverage_xml ÌååÏã± Ïã§Ìå®:\", e)\n",
        "\n",
        "(ART_DIR / \"observed_outcomes_gen.json\").write_text(\n",
        "    json.dumps(observed_outcomes_gen, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"üßÆ Î∂ÑÍ∏∞ Í¥ÄÏ∏° ÏöîÏïΩ ‚Üí total:{branch_points}, full:{full_hit}, half:{half_hit}, zero:{zero_hit}\")\n",
        "\n",
        "# ==== 5) Î™©Ìëú Îã¨ÏÑ±Î•†(Í∏∞Î≥∏) ====\n",
        "GOALS_FILE = ART_DIR / \"goals_ranked.json\"\n",
        "if GOALS_FILE.exists() and coverage_json_path.exists():\n",
        "    cov_json = json.loads((coverage_json_path).read_text(encoding=\"utf-8\"))\n",
        "    files_map = cov_json.get(\"files\", {}) or {}\n",
        "\n",
        "    def line_hit(fpath: str, ln: int) -> bool:\n",
        "        finfo = files_map.get(fpath) or files_map.get(str(Path(fpath).resolve()))\n",
        "        if not finfo:\n",
        "            return False\n",
        "        executed = set(finfo.get(\"executed_lines\", []) or [])\n",
        "        return ln in executed\n",
        "\n",
        "    goals = json.loads(GOALS_FILE.read_text(encoding=\"utf-8\"))\n",
        "    goal_stats = []\n",
        "    for g in goals:\n",
        "        f = g[\"file\"]\n",
        "        abs1 = str((PROJ / f).resolve())\n",
        "        abs2 = f\n",
        "        hit = sum(1 for ln in g.get(\"target_lines\", []) if line_hit(abs1, ln) or line_hit(abs2, ln))\n",
        "        total = len(g.get(\"target_lines\", [])) or 1\n",
        "        goal_stats.append({\"id\": g[\"id\"], \"hit\": hit, \"total\": total, \"rate\": round(hit/total, 3)})\n",
        "\n",
        "    (ART_DIR / \"goal_achievements.json\").write_text(\n",
        "        json.dumps(goal_stats, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    hit_goals = sum(1 for s in goal_stats if s[\"hit\"] > 0)\n",
        "    print(f\"üéØ Î™©Ìëú Îã¨ÏÑ±Î•†: {hit_goals}/{len(goal_stats)} Î™©ÌëúÍ∞Ä ‚â•1 ÎùºÏù∏ ÎèÑÎã¨\")\n",
        "\n",
        "# ==== 6) Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò(delta) Í≥ÑÏÇ∞ ====\n",
        "def load_json(p: Path, default=None):\n",
        "    try:\n",
        "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "base = load_json(coverage_base_json, {\"files\": {}}) or {\"files\": {}}\n",
        "gen  = load_json(coverage_json_path, {\"files\": {}}) or {\"files\": {}}\n",
        "base_files = base.get(\"files\", {}) or {}\n",
        "gen_files  = gen.get(\"files\", {})  or {}\n",
        "\n",
        "def _sum_len(key, d):\n",
        "    return sum(len((d.get(f, {}) or {}).get(key, []) or []) for f in d.keys())\n",
        "\n",
        "base_exec = _sum_len(\"executed_lines\", base_files)\n",
        "base_miss = _sum_len(\"missing_lines\",  base_files)\n",
        "gen_exec  = _sum_len(\"executed_lines\", gen_files)\n",
        "gen_miss  = _sum_len(\"missing_lines\",  gen_files)\n",
        "\n",
        "delta = {\n",
        "    \"executed_lines_delta\": gen_exec - base_exec,\n",
        "    \"missing_lines_delta\":  base_miss - gen_miss,   # +Î©¥ ÎØ∏Ïã± Í∞êÏÜå\n",
        "    \"base_executed\": base_exec,\n",
        "    \"gen_executed\":  gen_exec,\n",
        "    \"base_missing\":  base_miss,\n",
        "    \"gen_missing\":   gen_miss,\n",
        "}\n",
        "(ART_DIR / \"coverage_delta.json\").write_text(json.dumps(delta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò:\", json.dumps(delta, ensure_ascii=False))\n",
        "\n",
        "print(\"‚úÖ 3-5 ÏôÑÎ£å: (ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±) autosetup + Í≤©Î¶¨ Ïã§Ìñâ/ÏÉ§Îìú Í≤∞Ìï©/Î∂ÑÍ∏∞¬∑Î™©Ìëú¬∑Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú\")\n"
      ],
      "metadata": {
        "id": "cxb2ea7Wi2Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3de616-9c4a-4343-cbb1-9faabb1bd63c"
      },
      "id": "cxb2ea7Wi2Zk",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ pytest.ini ensured ‚Üí /content/money-transfer-project-template-python/pytest.ini\n",
            "‚úÖ conftest.py created ‚Üí /content/money-transfer-project-template-python/generated_tests/conftest.py\n",
            "üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: 1Í∞ú @ Pruned_Base_Tests\n",
            "‚úÖ PASS   [BASE] test_run_worker.py (15.539s)\n",
            "üß™ ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº: 8Í∞ú @ generated_tests\n",
            "‚úÖ PASS   [GEN] test_gen_0001_activities_refund_1.py (14.203s)\n",
            "‚úÖ PASS   [GEN] test_gen_0001_activities_refund_2.py (14.145s)\n",
            "‚úÖ PASS   [GEN] test_gen_0002_activities_refund_1.py (14.451s)\n",
            "‚úÖ PASS   [GEN] test_gen_0002_activities_refund_2.py (14.444s)\n",
            "‚úÖ PASS   [GEN] test_gen_0003_activities_refund_1.py (14.48s)\n",
            "‚úÖ PASS   [GEN] test_gen_0003_activities_refund_2.py (15.281s)\n",
            "‚ùå FAIL   [GEN] test_gen_0010_activities_deposit_1.py (rc=1, 14.561s) :: \n",
            "‚ùå FAIL   [GEN] test_gen_0010_activities_deposit_2.py (rc=1, 14.568s) :: \n",
            "\n",
            "üì¶ Ïã§Ìñâ ÏöîÏïΩ: {\"pass\": 7, \"fail\": 2, \"timeout\": 0, \"total\": 9}\n",
            ">  coverage combine --rcfile /content/money-transfer-project-template-python/.coveragerc /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.__base__.test_run_worker.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0001_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0001_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0002_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0002_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0003_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0003_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0010_activities_deposit_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0010_activities_deposit_2.py\n",
            "‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\n",
            " - JSON : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.json\n",
            " - XML  : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.xml\n",
            " - HTML : /content/money-transfer-project-template-python/htmlcov_gen/index.html\n",
            "üßÆ Î∂ÑÍ∏∞ Í¥ÄÏ∏° ÏöîÏïΩ ‚Üí total:31, full:27, half:0, zero:4\n",
            "üéØ Î™©Ìëú Îã¨ÏÑ±Î•†: 0/10 Î™©ÌëúÍ∞Ä ‚â•1 ÎùºÏù∏ ÎèÑÎã¨\n",
            "üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò: {\"executed_lines_delta\": 37, \"missing_lines_delta\": 0, \"base_executed\": 102, \"gen_executed\": 139, \"base_missing\": 54, \"gen_missing\": 54}\n",
            "‚úÖ 3-5 ÏôÑÎ£å: (ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±) autosetup + Í≤©Î¶¨ Ïã§Ìñâ/ÏÉ§Îìú Í≤∞Ìï©/Î∂ÑÍ∏∞¬∑Î™©Ìëú¬∑Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-6) ÌÖåÏä§Ìä∏ Î≥¥ÏôÑ ‚Äì Improvement-based Adaptive Round (coverage_gen Í∏∞Î∞ò Ï†ÅÏùëÌòï ÏÑ†Î≥Ñ¬∑Î∂ÑÌè¨ Ï£ºÏûÖ)\n",
        "import os, sys, json, re\n",
        "from pathlib import Path\n",
        "\n",
        "# ==== Í≤ΩÎ°ú/ÏÉÅÏàò ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "HTML_GEN_DIR = PROJ / \"htmlcov_gen\"\n",
        "\n",
        "COV_BASE_JSON = ART_DIR / \"coverage_base.json\"       # 3-1 Í∏∞Ï§ÄÏÑ† (Ï∞∏Í≥†Ïö©)\n",
        "COV_GEN_JSON  = ART_DIR / \"coverage_gen.json\"        # 3-5/3-8 ÌÜµÌï© Ïª§Î≤ÑÎ¶¨ÏßÄ(ÌïÑÏàò)\n",
        "GOALS_FILE    = ART_DIR / \"goals_ranked.json\"        # 3-2 Î™©Ìëú\n",
        "UNCV_MAP_JSON = ART_DIR / \"uncovered_map_base.json\"  # 3-1 ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏(Ï∞∏Í≥†Ïö©)\n",
        "RESULTS_JL    = ART_DIR / \"results.jsonl\"            # 3-5/3-8 Ïã§Ìñâ Í≤∞Í≥º Î°úÍ∑∏ Ïù∏Îç±Ïä§(Ï∞∏Í≥†Ïö©)\n",
        "\n",
        "# ÏÑ†Î≥Ñ/Î∞∞Ïπò ÌååÎùºÎØ∏ÌÑ∞\n",
        "NEAR_MISS_WINDOW = 2\n",
        "BATCH_SIZE = 3\n",
        "MAX_ROUNDS = 5\n",
        "TOPK_FILES = 10   # Ï†ÑÏó≠ ÎØ∏Ïª§Î≤Ñ ÏÉÅÏúÑ ÌååÏùº ÏöîÏïΩ Í∞úÏàò\n",
        "\n",
        "# ==== Ï†ÅÏùëÌòï ÏûÑÍ≥Ñ/ÏàòÎ†¥ ÌååÎùºÎØ∏ÌÑ∞(ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Ï°∞Ï†ï Í∞ÄÎä•) ====\n",
        "TAU1_RATE = float(os.environ.get(\"ADAPT_TAU1_RATE\", \"0.25\"))  # Î™©ÌëúÎã¨ÏÑ±Î•† Ï¶ùÍ∞Ä ÏûÑÍ≥Ñ(Ïòà: 0.25=25%p)\n",
        "CONV_EPS  = float(os.environ.get(\"ADAPT_CONV_EPS\", \"0.01\"))   # (Ï∞∏Í≥†) Ï†ÑÏ≤¥ ÎùºÏù∏Ïª§Î≤ÑÎ¶¨ÏßÄ ÏàòÎ†¥ ÌåêÎã® 1%p\n",
        "CONV_PATIENCE = int(os.environ.get(\"ADAPT_CONV_PATIENCE\", \"1\"))\n",
        "\n",
        "# ==== ÎùºÏö¥Îìú ÎîîÎ†âÌÑ∞Î¶¨ ÏûêÎèô Ï¶ùÍ∞Ä ====\n",
        "def next_round_dir(base: Path) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        cand = base / f\"refine_round{i}\"\n",
        "        if not cand.exists():\n",
        "            cand.mkdir(parents=True, exist_ok=True)\n",
        "            return cand\n",
        "        i += 1\n",
        "\n",
        "REFINE_DIR = next_round_dir(ART_DIR)\n",
        "REFINE_PROMPTS = REFINE_DIR / \"llm_refine_prompts.jsonl\"\n",
        "REFINE_SUMMARY = REFINE_DIR / \"refine_selection.json\"\n",
        "REFINE_TEST_EXPORT = REFINE_DIR / \"selected_tests_dump.json\"\n",
        "\n",
        "# ==== Ïú†Ìã∏ ====\n",
        "def load_json(p: Path, default=None):\n",
        "    try:\n",
        "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def read_text_safe(p: Path) -> str:\n",
        "    try:\n",
        "        return p.read_text(encoding=\"utf-8\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def first_lines(s: str, n=2000):\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    head = s[:n]\n",
        "    if len(s) > n:\n",
        "        head += \"\\n...<truncated>...\"\n",
        "    return head\n",
        "\n",
        "RE_GID = re.compile(r\"(?:^|[_-])(?P<gid>\\d{4})(?:[_-]|$)\")\n",
        "def goal_id_from_name(name: str) -> str | None:\n",
        "    m = RE_GID.search(name)\n",
        "    return m.group(\"gid\") if m else None\n",
        "\n",
        "def list_generated_tests_for_gid(gid: str) -> list[Path]:\n",
        "    \"\"\"goal idÏóê Ìï¥ÎãπÌïòÎäî ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ ÌõÑÎ≥¥Îì§ÏùÑ ÏµúÏã†ÏàúÏúºÎ°ú Î∞òÌôò.\"\"\"\n",
        "    if not GEN_DIR.exists():\n",
        "        return []\n",
        "    cands = [p for p in GEN_DIR.glob(\"*.py\") if gid in p.name]\n",
        "    # Î≥¥Í∞ïÎ≥∏(*_rN.py)ÏùÑ Ïö∞ÏÑ†, Ïà´Ïûê ÌÅ∞ Í≤É Ïö∞ÏÑ† ‚Üí ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ ÌååÏùº\n",
        "    def rank(p: Path):\n",
        "        m = re.search(r\"_r(\\d+)\\.py$\", p.name)\n",
        "        r = int(m.group(1)) if m else -1\n",
        "        return (0 if r >= 0 else 1, -r, -p.stat().st_mtime)\n",
        "    return sorted(cands, key=rank)\n",
        "\n",
        "def pick_latest_test_for_gid(gid: str) -> Path | None:\n",
        "    lst = list_generated_tests_for_gid(gid)\n",
        "    return lst[0] if lst else None\n",
        "\n",
        "# ==== Ïù¥Ï†Ñ ÎùºÏö¥Îìú Ïä§ÎÉÖÏÉ∑/ÌûàÏä§ÌÜ†Î¶¨ Î°úÎçî ====\n",
        "def _load_latest_two_histories():\n",
        "    hist_path = ART_DIR / \"history.jsonl\"\n",
        "    if not hist_path.exists():\n",
        "        return None, None\n",
        "    rows = [json.loads(x) for x in hist_path.read_text(encoding=\"utf-8\").splitlines() if x.strip()]\n",
        "    if len(rows) == 0: return None, None\n",
        "    if len(rows) == 1: return rows[-1], None\n",
        "    return rows[-1], rows[-2]\n",
        "\n",
        "def _goal_rate_map(hist_row):\n",
        "    \"\"\"history.jsonlÏùò goal_achievementsÎ•º id->rate ÎßµÏúºÎ°ú Î≥ÄÌôò\"\"\"\n",
        "    if not hist_row or not hist_row.get(\"goal_achievements\"):\n",
        "        return {}\n",
        "    m = {}\n",
        "    for g in hist_row[\"goal_achievements\"]:\n",
        "        m[str(g[\"id\"])] = float(g.get(\"rate\", 0.0))\n",
        "    return m\n",
        "\n",
        "# ==== Ïã§Ìå®Ïú†Ìòï Î∂ÑÎ•òÍ∏∞(Î°úÍ∑∏ Ìó§Îìú Í∏∞Î∞ò) ====\n",
        "_ERR_PATTERNS = {\n",
        "    \"import_error\":  [r\"ModuleNotFoundError\", r\"ImportError\"],\n",
        "    \"timeout\":       [r\"TimeoutExpired\", r\"timed out\"],\n",
        "    \"crash\":         [r\"Segmentation fault\", r\"Fatal Python error\"],\n",
        "    \"assert_fail\":   [r\"AssertionError\", r\"assert .* failed\"],\n",
        "    \"runtime_warn\":  [r\"RuntimeWarning\", r\"coroutine .* was never awaited\"],\n",
        "}\n",
        "def classify_failure(stdout_head: str, stderr_head: str) -> str|None:\n",
        "    blob = (stderr_head or \"\") + \"\\n\" + (stdout_head or \"\")\n",
        "    for tag, pats in _ERR_PATTERNS.items():\n",
        "        for p in pats:\n",
        "            if re.search(p, blob):\n",
        "                return tag\n",
        "    return None\n",
        "\n",
        "# ==== Îç∞Ïù¥ÌÑ∞ Î°úÎìú ====\n",
        "cov_base = load_json(COV_BASE_JSON, {\"files\": {}}) or {\"files\": {}}\n",
        "cov_gen  = load_json(COV_GEN_JSON,  {\"files\": {}}) or {\"files\": {}}\n",
        "goals    = load_json(GOALS_FILE,    []) or []\n",
        "uncovered_map = load_json(UNCV_MAP_JSON, {}) or {}\n",
        "\n",
        "if not cov_gen or not goals:\n",
        "    raise SystemExit(\"ÌïÑÏàò ÏÇ∞Ï∂úÎ¨º(coverage_gen.json ÎòêÎäî goals_ranked.json)Ïù¥ ÏóÜÏäµÎãàÎã§. 3-2, 3-5/3-8 ÌõÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
        "\n",
        "gen_files = cov_gen.get(\"files\", {}) or {}\n",
        "\n",
        "def _info_for(file_rel: str):\n",
        "    \"\"\"coverage_gen.jsonÏóêÏÑú ÏÉÅÎåÄ/Ï†àÎåÄ ÌÇ§ Î™®Îëê ÌÉêÏÉâ.\"\"\"\n",
        "    return gen_files.get(file_rel) or gen_files.get(str((PROJ / file_rel).resolve()))\n",
        "\n",
        "def line_hit_in_gen(file_rel: str, ln: int) -> bool:\n",
        "    info = _info_for(file_rel)\n",
        "    if not info:\n",
        "        return False\n",
        "    return ln in set(info.get(\"executed_lines\", []) or [])\n",
        "\n",
        "def executed_set_in_gen(file_rel: str) -> set[int]:\n",
        "    info = _info_for(file_rel)\n",
        "    return set(info.get(\"executed_lines\", []) or []) if info else set()\n",
        "\n",
        "def missing_set_in_gen(file_rel: str) -> set[int]:\n",
        "    info = _info_for(file_rel)\n",
        "    return set(info.get(\"missing_lines\", []) or []) if info else set()\n",
        "\n",
        "# ==== A) coverage_gen.jsonÏóêÏÑú Ï†ÑÏó≠ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏ Î∂ÑÌè¨ Ï∂îÏ∂ú ====\n",
        "global_uncovered_map = {}\n",
        "total_missing = 0\n",
        "for fkey, finfo in gen_files.items():\n",
        "    miss = sorted(set((finfo or {}).get(\"missing_lines\", []) or []))\n",
        "    if miss:\n",
        "        global_uncovered_map[fkey] = miss\n",
        "        total_missing += len(miss)\n",
        "\n",
        "global_uncovered_summary = []\n",
        "for fkey, miss in sorted(global_uncovered_map.items(), key=lambda kv: len(kv[1]), reverse=True)[:TOPK_FILES]:\n",
        "    global_uncovered_summary.append({\n",
        "        \"file\": fkey,\n",
        "        \"missing_count\": len(miss),\n",
        "        \"missing_lines\": miss[:200],\n",
        "    })\n",
        "\n",
        "# ==== 1) ÎØ∏ÎèÑÎã¨ Î™©Ìëú ÌåêÏ†ï (coverage_gen Í∏∞Ï§Ä) ====\n",
        "miss_goals = []  # 1Ï∞® ÌõÑÎ≥¥\n",
        "for g in goals:\n",
        "    file_rel = g[\"file\"]\n",
        "    tlines = g.get(\"target_lines\", []) or []\n",
        "    hits = sum(1 for ln in tlines if line_hit_in_gen(file_rel, ln))\n",
        "    if hits == 0 and tlines:  # Î™©Ìëú ÎùºÏù∏ ‚â•1Ï§ÑÎèÑ Î™ª ÎßûÏ∂ò Í≤ΩÏö∞Îßå Î≥¥Í∞ï\n",
        "        exed = executed_set_in_gen(file_rel)\n",
        "        neigh = set()\n",
        "        for t in tlines:\n",
        "            for k in range(-NEAR_MISS_WINDOW, NEAR_MISS_WINDOW + 1):\n",
        "                neigh.add(t + k)\n",
        "        near = len(exed & neigh) > 0\n",
        "        miss_goals.append({\n",
        "            \"goal_id\": g[\"id\"],\n",
        "            \"file\": file_rel,\n",
        "            \"function\": g.get(\"function\", {}),\n",
        "            \"target_lines\": tlines,\n",
        "            \"near_miss\": near\n",
        "        })\n",
        "\n",
        "# ==== 1.5) Ï†ÅÏùëÌòï ÏÑ†Î≥Ñ: Ïù¥Ï†Ñ ÎùºÏö¥Îìú ÎåÄÎπÑ Î™©ÌëúÎã¨ÏÑ±Î•† ŒîÏôÄ Ïã§Ìå®Ïú†Ìòï ÌèâÍ∞Ä ====\n",
        "latest, prev = _load_latest_two_histories()\n",
        "rate_now = _goal_rate_map(latest)\n",
        "rate_prev = _goal_rate_map(prev)\n",
        "\n",
        "# results Ïù∏Îç±Ïä§ Î°úÎìú(ÎßàÏßÄÎßâ ÎùºÏö¥Îìú Ïã§Ìñâ Í≤∞Í≥º)\n",
        "results_idx = []\n",
        "if RESULTS_JL.exists():\n",
        "    results_idx = [json.loads(x) for x in RESULTS_JL.read_text(encoding=\"utf-8\").splitlines() if x.strip()]\n",
        "\n",
        "# test_file -> (stdout, stderr) Ìó§Îìú ÎØ∏Î¶¨ ÏùΩÍ∏∞\n",
        "def _load_heads(tname: str):\n",
        "    return (\n",
        "        read_text_safe(LOG_DIR / f\"{tname}.out.txt\"),\n",
        "        read_text_safe(LOG_DIR / f\"{tname}.err.txt\"),\n",
        "    )\n",
        "\n",
        "# goal_id ‚Üí ÏµúÏã† ÌÖåÏä§Ìä∏ ÌååÏùºÎ™Ö Ï∂îÏ†ï\n",
        "def _guess_last_test_name_for_gid(gid: str) -> str|None:\n",
        "    p = pick_latest_test_for_gid(gid)\n",
        "    return p.name if p else None\n",
        "\n",
        "adaptive_candidates = []\n",
        "for item in miss_goals:\n",
        "    gid = str(item[\"goal_id\"])\n",
        "    now = float(rate_now.get(gid, 0.0))\n",
        "    prv = float(rate_prev.get(gid, 0.0))\n",
        "    delta_rate = now - prv\n",
        "\n",
        "    # ÎßàÏßÄÎßâ Ïã§ÌñâÏùò Ïã§Ìå® Ïú†Ìòï ÌôïÏù∏(ÏûàÏúºÎ©¥ Ï†úÏô∏ Í∑úÏπô Ï†ÅÏö©)\n",
        "    last_tname = _guess_last_test_name_for_gid(gid)\n",
        "    fclass = None\n",
        "    if last_tname:\n",
        "        out_h, err_h = _load_heads(last_tname)\n",
        "        fclass = classify_failure(out_h, err_h)\n",
        "\n",
        "    fatal = fclass in {\"import_error\", \"timeout\", \"crash\"}  # Ï†úÏô∏ Í∏∞Ï§Ä\n",
        "\n",
        "    if (delta_rate >= TAU1_RATE) or item[\"near_miss\"]:\n",
        "        if not fatal:\n",
        "            item[\"delta_rate\"] = round(delta_rate, 3)\n",
        "            item[\"last_failure\"] = fclass\n",
        "            adaptive_candidates.append(item)\n",
        "\n",
        "# Ïù¥ÌõÑ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÄ adaptive_candidates ÏÇ¨Ïö©\n",
        "miss_goals = adaptive_candidates\n",
        "\n",
        "# ==== 2) Í∞Å ÎØ∏ÎèÑÎã¨ goal ‚Üí ÏµúÏã† ÌÖåÏä§Ìä∏ ÌååÏùº Îß§Ìïë & ÌîÑÎ°¨ÌîÑÌä∏ ÏûÖÎ†• Íµ¨ÏÑ± ====\n",
        "selected = []\n",
        "for item in miss_goals:\n",
        "    gid = item[\"goal_id\"]\n",
        "    tfile = pick_latest_test_for_gid(gid)\n",
        "    if not tfile:\n",
        "        # ÏÉùÏÑ±Îêú ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏúºÎ©¥ Ïä§ÌÇµ(Îã§Ïùå ÎùºÏö¥ÎìúÏóê ÏÉàÎ°ú ÏÉùÏÑ±)\n",
        "        continue\n",
        "\n",
        "    original_code = read_text_safe(tfile)\n",
        "    out_log = read_text_safe(LOG_DIR / f\"{tfile.name}.out.txt\")\n",
        "    err_log = read_text_safe(LOG_DIR / f\"{tfile.name}.err.txt\")\n",
        "\n",
        "    # Ìï¥Îãπ Î™©ÌëúÏóêÏÑú ÏïÑÏßÅ ÎØ∏ÎèÑÎã¨Ìïú ÌÉÄÍ≤ü ÎùºÏù∏(coverage_gen Í∏∞Ï§Ä)\n",
        "    still_missing = [ln for ln in item[\"target_lines\"] if not line_hit_in_gen(item[\"file\"], ln)]\n",
        "\n",
        "    # ÌòÑÏû¨ ÎùºÏö¥Îìú Í∏∞Ï§Ä Í∑∏ ÌååÏùºÏùò Ï†ÑÏ≤¥ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏(coverage_gen Í∏∞Ï§Ä)\n",
        "    file_uncovered_remaining_gen = sorted(missing_set_in_gen(item[\"file\"]))\n",
        "\n",
        "    # baselineÏùò ÌååÏùºÎ≥Ñ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏(Ï∞∏Í≥†Ïö©)\n",
        "    file_abs = str((PROJ / item[\"file\"]).resolve())\n",
        "    base_uncovered = uncovered_map.get(file_abs, uncovered_map.get(item[\"file\"], [])) or []\n",
        "\n",
        "    selected.append({\n",
        "        \"id\": f\"{gid}::{tfile.name}\",\n",
        "        \"goal_id\": gid,\n",
        "        \"target_file\": item[\"file\"],\n",
        "        \"target_lines\": item[\"target_lines\"],\n",
        "        \"near_miss\": item[\"near_miss\"],\n",
        "        \"delta_rate\": item.get(\"delta_rate\"),\n",
        "        \"last_failure\": item.get(\"last_failure\"),\n",
        "        \"uncovered_diff\": {\n",
        "            \"still_missing_target_lines\": sorted(still_missing),\n",
        "            \"file_uncovered_lines_baseline\": sorted(set(int(x) for x in base_uncovered)),\n",
        "            \"file_uncovered_remaining_gen\": file_uncovered_remaining_gen,   # ‚òÖ ÌòÑÏû¨ ÎØ∏Ïª§Î≤Ñ(Ìï¥Îãπ ÌååÏùº)\n",
        "        },\n",
        "        \"run\": {\n",
        "            \"stdout_head\": first_lines(out_log, 1500),\n",
        "            \"stderr_head\": first_lines(err_log, 1500),\n",
        "        },\n",
        "        \"original_test_code\": original_code,\n",
        "    })\n",
        "\n",
        "# ÏÑ†Î≥Ñ ÏóÜÏúºÎ©¥ Ï¢ÖÎ£å\n",
        "if not selected:\n",
        "    REFINE_PROMPTS.write_text(\"\", encoding=\"utf-8\")\n",
        "    REFINE_SUMMARY.write_text(json.dumps({\n",
        "        \"round_dir\": REFINE_DIR.name,\n",
        "        \"using_coverage_gen_only\": True,\n",
        "        \"selected\": 0,\n",
        "        \"reason\": \"Ï†ÅÏùëÌòï Í∏∞Ï§Ä(Œî-rate‚â•œÑ1 ÎòêÎäî near-miss) Ï∂©Ï°± Î™©Ìëú ÏóÜÏùå, ÌòπÏùÄ ÏµúÏã† ÌÖåÏä§Ìä∏ ÌååÏùº Î∂ÄÏû¨\",\n",
        "        \"params\": {\n",
        "            \"tau1_rate\": TAU1_RATE,\n",
        "            \"near_miss_window\": NEAR_MISS_WINDOW\n",
        "        },\n",
        "        \"global_uncovered\": {\n",
        "            \"total_missing_lines\": total_missing,\n",
        "            \"top_files\": global_uncovered_summary\n",
        "        }\n",
        "    }, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(\"‚ÑπÔ∏è Î≥¥Í∞ï ÎåÄÏÉÅÏù¥ ÏóÜÏäµÎãàÎã§. ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "# ==== 3) ÌååÏùºÎ≥Ñ Ï∂©Îèå ÌîºÌïòÍ∏∞(ÎèôÏùº ÌååÏùº Î™©ÌëúÎäî Ìïú Î∞∞ÏπòÏóê ÌïòÎÇòÎßå) ‚Üí Î∞∞Ïπò Íµ¨ÏÑ± ====\n",
        "batches, bucket, seen_files = [], [], set()\n",
        "for rec in selected:\n",
        "    f = rec[\"target_file\"]\n",
        "    if f in seen_files or len(bucket) >= BATCH_SIZE:\n",
        "        if bucket:\n",
        "            batches.append(bucket)\n",
        "        bucket, seen_files = [], set()\n",
        "    bucket.append(rec)\n",
        "    seen_files.add(f)\n",
        "if bucket:\n",
        "    batches.append(bucket)\n",
        "\n",
        "# ==== 4) LLM Î≥¥Í∞ï ÌîÑÎ°¨ÌîÑÌä∏(JSONL) ÏÉùÏÑ± ====\n",
        "SYSTEM_REFINE = (\n",
        "    \"ÎãπÏã†ÏùÄ Í∏∞Ï°¥ pytest ÌÖåÏä§Ìä∏Î•º Î≥¥Í∞ïÌïòÏó¨ ÎØ∏Ïª§Î≤Ñ ÏòÅÏó≠(Target Lines)Ïóê ÎèÑÎã¨ÌïòÎèÑÎ°ù ÏàòÏ†ïÌïòÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.\\n\"\n",
        "    \"Ï∂úÎ†•ÏùÄ ÎßàÌÅ¨Îã§Ïö¥ ÏóÜÏù¥ **ÏàúÏàò JSON Í∞ùÏ≤¥** ÌïòÎÇòÎ°úÎßå ÏùëÎãµÌï©ÎãàÎã§. Ïä§ÌÇ§ÎßàÎäî ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"edits\": [\\n'\n",
        "    '    {\"id\": \"<goal_id::filename>\", \"new_code\": \"<Î≥¥Í∞ïÎêú pytest ÌÖåÏä§Ìä∏ ÌååÏùº Ï†ÑÏ≤¥ Î¨∏ÏûêÏó¥>\"}\\n'\n",
        "    \"  ]\\n\"\n",
        "    \"}\\n\"\n",
        "    \"ÏßÄÏπ®:\\n\"\n",
        "    \"‚Ä¢ ÌÖåÏä§Ìä∏Ïùò Íµ¨Ï°∞Î•º Ïú†ÏßÄÌïòÎêò, ÏûÖÎ†•/Í≤ΩÍ≥ÑÏ°∞Í±¥/Ìò∏Ï∂ú ÏàúÏÑú/ÏòàÏô∏ Ìä∏Î¶¨Í±∞Î•º Ï°∞Ï†ïÌï¥ `still_missing_target_lines`Ïóê Ïã§Ï†úÎ°ú ÎèÑÎã¨ÌïòÍ≤å ÌïòÏÑ∏Ïöî.\\n\"\n",
        "    \"‚Ä¢ Ïô∏Î∂Ä Î∂ÄÏûëÏö© Í∏àÏßÄ(ÌååÏùº/ÎÑ§Ìä∏ÏõåÌÅ¨/ÏãúÍ∞Ñ/ÌôòÍ≤Ω/Temporal). ÌïÑÏöîÌïú Í≤ΩÏö∞ monkeypatch/ÎçîÎ∏îÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.\\n\"\n",
        "    \"‚Ä¢ importÎäî importlib + getattr Í≤ΩÎ°úÎ•º Ïú†ÏßÄÌïòÍ≥†, ÏÜçÏÑ±Ïù¥ ÏóÜÏùÑ ÎïåÎßå Í∞ÄÎìúÌòï Ï°∞Í±¥ÏúºÎ°ú pytest.skipÏùÑ ÌóàÏö©Ìï©ÎãàÎã§.\\n\"\n",
        "    \"‚Ä¢ Í∞Å ÌÖåÏä§Ìä∏Îäî ÏµúÏÜå 1Ï§Ñ Ïù¥ÏÉÅÏùò target_linesÎ•º Ïã§Ï†ú Ïã§ÌñâÌï¥Ïïº ÌïòÎ©∞, Í¥ÄÎ†®Îêú assert ÎòêÎäî pytest.raisesÎ•º Ìè¨Ìï®Ìï¥Ïïº Ìï©ÎãàÎã§.\\n\"\n",
        "    \"‚Ä¢ ÌÖåÏä§Ìä∏ Ïù¥Î¶ÑÏóê ÌÉÄÍ≤© ÎùºÏù∏ÏùÑ `hits_L<line>` ÌòïÌÉúÎ°ú Ìè¨Ìï®ÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.\\n\"\n",
        "    \"‚Ä¢ Ï∂úÎ†•ÏóêÎäî ÏΩîÎìú Ïô∏ ÏÑ§Î™Ö/Ï£ºÏÑù/ÎßàÌÅ¨Îã§Ïö¥ÏùÑ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî. **JSONÎßå** Î∞òÌôòÌïòÏÑ∏Ïöî.\\n\"\n",
        ")\n",
        "\n",
        "with REFINE_PROMPTS.open(\"w\", encoding=\"utf-8\") as outf:\n",
        "    for i, batch in enumerate(batches, start=1):\n",
        "        user_payload = {\n",
        "            \"schema_version\": \"refine-v1\",\n",
        "            \"round_dir\": REFINE_DIR.name,\n",
        "            \"selection_params\": {\n",
        "                \"tau1_rate\": TAU1_RATE,\n",
        "                \"near_miss_window\": NEAR_MISS_WINDOW,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"using_coverage_gen_only\": True,\n",
        "                \"max_rounds\": MAX_ROUNDS\n",
        "            },\n",
        "            # ‚òÖ Ï†ÑÏó≠ ÎØ∏Ïª§Î≤Ñ ÏöîÏïΩ(Î™®Îç∏Ïù¥ Ïö∞ÏÑ†ÏàúÏúÑ Í≥†Î†§ÌïòÎèÑÎ°ù ÌûåÌä∏)\n",
        "            \"global_uncovered\": {\n",
        "                \"total_missing_lines\": total_missing,\n",
        "                \"top_files\": global_uncovered_summary\n",
        "            },\n",
        "            \"batch_index\": i,\n",
        "            \"tests\": batch,\n",
        "        }\n",
        "        record = {\n",
        "            \"meta\": {\n",
        "                \"batch_index\": i,\n",
        "                \"num_tests\": len(batch),\n",
        "                \"ids\": [t[\"id\"] for t in batch],\n",
        "                \"round_dir\": REFINE_DIR.name,\n",
        "                \"using_coverage_gen_only\": True,\n",
        "            },\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_REFINE},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(user_payload, ensure_ascii=False, indent=2)},\n",
        "            ],\n",
        "        }\n",
        "        outf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ==== 5) ÏöîÏïΩ/Îç§ÌîÑ ====\n",
        "summary = {\n",
        "    \"round_dir\": REFINE_DIR.name,\n",
        "    \"using_coverage_gen_only\": True,\n",
        "    \"params\": {\n",
        "        \"tau1_rate\": TAU1_RATE,\n",
        "        \"near_miss_window\": NEAR_MISS_WINDOW,\n",
        "        \"batch_size\": BATCH_SIZE\n",
        "    },\n",
        "    \"selected\": len(selected),\n",
        "    \"batches\": [{\"batch_index\": i+1, \"num_tests\": len(b)} for i, b in enumerate(batches)],\n",
        "    \"global_uncovered\": {\n",
        "        \"total_missing_lines\": total_missing,\n",
        "        \"top_files\": global_uncovered_summary\n",
        "    }\n",
        "}\n",
        "REFINE_SUMMARY.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "REFINE_TEST_EXPORT.write_text(json.dumps(selected, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ Î≥¥Í∞ï ÎåÄÏÉÅ ÏÑ†Î≥Ñ ÏôÑÎ£å (Œî-rate‚â•œÑ1/near-miss + fatal Ïã§Ìå® Ï†úÏô∏ + Ï†ÑÏó≠ Î∂ÑÌè¨ Ï£ºÏûÖ)\")\n",
        "print(\" - ÎùºÏö¥Îìú Ìè¥Îçî:\", REFINE_DIR)\n",
        "print(\" - ÏÑ†Î≥ÑÎêú ÌÖåÏä§Ìä∏ Ïàò:\", len(selected))\n",
        "print(\" - Î∞∞Ïπò Ïàò:\", len(batches))\n",
        "print(\" - ÌîÑÎ°¨ÌîÑÌä∏ JSONL:\", REFINE_PROMPTS)\n",
        "print(\" - ÏÑ†Î≥Ñ ÏöîÏïΩ:\", REFINE_SUMMARY)\n"
      ],
      "metadata": {
        "id": "TiTUpEZcBSxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fbb109-8cf1-472c-e746-c31bfd0441e9"
      },
      "id": "TiTUpEZcBSxq",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Î≥¥Í∞ï ÎåÄÏÉÅ ÏÑ†Î≥Ñ ÏôÑÎ£å (Œî-rate‚â•œÑ1/near-miss + fatal Ïã§Ìå® Ï†úÏô∏ + Ï†ÑÏó≠ Î∂ÑÌè¨ Ï£ºÏûÖ)\n",
            " - ÎùºÏö¥Îìú Ìè¥Îçî: /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2\n",
            " - ÏÑ†Î≥ÑÎêú ÌÖåÏä§Ìä∏ Ïàò: 3\n",
            " - Î∞∞Ïπò Ïàò: 3\n",
            " - ÌîÑÎ°¨ÌîÑÌä∏ JSONL: /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2/llm_refine_prompts.jsonl\n",
            " - ÏÑ†Î≥Ñ ÏöîÏïΩ: /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2/refine_selection.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-7) Î≥¥Í∞ï Ï†ÅÏö©Í∏∞ ‚Äì Í≤ΩÎ°ú Î≤ÑÍ∑∏ ÏàòÏ†ï + GOAL_LUT Ìò∏Ìôò + ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Í∞ïÌôî\n",
        "import os, re, json, ast, shutil, time, subprocess, shlex, sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import httpx\n",
        "import backoff\n",
        "from lxml import etree\n",
        "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
        "\n",
        "# ==== ÏÑ§Ï†ï ====\n",
        "REPLACE_IN_PLACE = os.getenv(\"REFINE_REPLACE_IN_PLACE\", \"true\").lower() in {\"1\",\"true\",\"yes\"}\n",
        "MODEL = os.getenv(\"REFINE_MODEL\", \"gpt-4o\")\n",
        "TIMEOUT_SINGLE = int(os.getenv(\"REFINE_SINGLE_TIMEOUT\", \"30\"))\n",
        "RCFILE = Path(os.getenv(\"REFINE_RCFILE\", \".coveragerc\"))\n",
        "MIN_EXEC_GAIN = int(os.getenv(\"REFINE_MIN_EXEC_GAIN\", \"3\"))  # œÑ‚ÇÇ\n",
        "PYTEST_FLAGS = os.getenv(\"REFINE_PYTEST_FLAGS\", \"-q -s\")\n",
        "\n",
        "# ==== Í≤ΩÎ°ú ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "REFINE_DIRS = sorted([p for p in ART_DIR.iterdir() if p.is_dir() and p.name.startswith(\"refine_round\")])\n",
        "if not REFINE_DIRS:\n",
        "    raise SystemExit(\"refine_roundN Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§. 3-6ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
        "REFINE_DIR = REFINE_DIRS[-1]\n",
        "PROMPTS_PATH = REFINE_DIR / \"llm_refine_prompts.jsonl\"\n",
        "if not PROMPTS_PATH.exists():\n",
        "    raise SystemExit(f\"ÌîÑÎ°¨ÌîÑÌä∏ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {PROMPTS_PATH}\")\n",
        "if PROMPTS_PATH.stat().st_size == 0:\n",
        "    print(\"‚ÑπÔ∏è Î≥¥Í∞ï ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§. Ï†ÅÏö© Îã®Í≥Ñ Í±¥ÎÑàÎúÅÎãàÎã§.\"); raise SystemExit(0)\n",
        "\n",
        "RAW_DIR = REFINE_DIR / \"_raw_edits\"\n",
        "ERR_DIR = REFINE_DIR / \"_errors\"\n",
        "ARCHIVE_DIR = REFINE_DIR / \"_archive\"\n",
        "REJECT_DIR = REFINE_DIR / \"_rejected\"\n",
        "STAGE_DIR  = REFINE_DIR / \"_staging\"\n",
        "LOG_PATH   = REFINE_DIR / \"apply_log.jsonl\"\n",
        "MANIFEST   = GEN_DIR / \"ACTIVE_MANIFEST.json\"\n",
        "\n",
        "for d in (RAW_DIR, ERR_DIR, ARCHIVE_DIR, REJECT_DIR, STAGE_DIR, GEN_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ==== OpenAI ====\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. 3-0 Îã®Í≥ÑÏóêÏÑú .envÎ•º Î°úÎìúÌñàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")\n",
        "http_client = httpx.Client(timeout=180.0, follow_redirects=True,\n",
        "                           limits=httpx.Limits(max_connections=1, max_keepalive_connections=0),\n",
        "                           transport=httpx.HTTPTransport(retries=5))\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                base_url=\"https://api.openai.com/v1\", http_client=http_client)\n",
        "\n",
        "# ==== Ïú†Ìã∏ ====\n",
        "def now_ts(): return time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "def strip_fences(s: str) -> str:\n",
        "    s = re.sub(r\"^```[a-zA-Z0-9]*\\s*\", \"\", (s or \"\").strip()); s = re.sub(r\"\\s*```$\", \"\", s); return s\n",
        "def write_error(tag: str, payload: dict):\n",
        "    (ERR_DIR / f\"{tag}.json\").write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "def ensure_unique_path(base: Path) -> Path:\n",
        "    p = base; i = 2\n",
        "    while p.exists(): p = base.with_name(f\"{base.stem}_{i}{base.suffix}\"); i += 1\n",
        "    return p\n",
        "\n",
        "# Îü¨ÎÑà/Î©îÏù∏Í∞ÄÎìú Ï†úÍ±∞\n",
        "def sanitize_test_code(code: str) -> str:\n",
        "    pats = [\n",
        "        re.compile(r\"(?ms)^\\s*if\\s+__name__\\s*==\\s*['\\\"]__main__['\\\"]\\s*:\\s*\\n(?:\\s*.*\\n?)+$\"),\n",
        "        re.compile(r\"(?m)^\\s*pytest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "        re.compile(r\"(?m)^\\s*unittest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "    ]\n",
        "    new = code or \"\"\n",
        "    for p in pats: new = p.sub(\"\", new)\n",
        "    return (new.strip() + \"\\n\") if new.strip() else \"\\n\"\n",
        "\n",
        "# ---- Í≤ÄÏ¶ù (Í∞ÄÎìúÌòï skipÎßå ÌóàÏö©) ----\n",
        "RE_IMPORTLIB = re.compile(r\"\\bimportlib\\.import_module\\s*\\(\")\n",
        "RE_PYTEST_RAISES = re.compile(r\"\\bpytest\\.raises\\s*\\(\")\n",
        "RE_GETATTR = re.compile(r\"\\bgetattr\\s*\\(\\s*[^,]+,\\s*['\\\"][A-Za-z_][A-Za-z0-9_]*['\\\"]\")\n",
        "\n",
        "def parse_ast_or_error(code: str):\n",
        "    try: return ast.parse(code), None\n",
        "    except SyntaxError as e: return None, f\"syntax_error:{e.msg}@L{e.lineno}\"\n",
        "\n",
        "def extract_test_funcs(tree: ast.AST):\n",
        "    return [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef) and n.name.startswith(\"test_\")]\n",
        "\n",
        "def has_assert_or_raises(tree: ast.AST, code: str) -> bool:\n",
        "    return any(isinstance(n, ast.Assert) for n in ast.walk(tree)) or bool(RE_PYTEST_RAISES.search(code))\n",
        "\n",
        "def uses_importlib(code: str) -> bool: return bool(RE_IMPORTLIB.search(code))\n",
        "def uses_getattr_guardish(code: str) -> bool: return bool(RE_GETATTR.search(code or \"\"))\n",
        "\n",
        "def _parent_map(tree: ast.AST):\n",
        "    parent = {}\n",
        "    for node in ast.walk(tree):\n",
        "        for child in ast.iter_child_nodes(node): parent[child] = node\n",
        "    return parent\n",
        "\n",
        "def _is_guarded_skip(call: ast.Call, parent_map) -> bool:\n",
        "    cur = call\n",
        "    while cur in parent_map:\n",
        "        cur = parent_map[cur]\n",
        "        if isinstance(cur, ast.If):\n",
        "            t = cur.test\n",
        "            if (isinstance(t, ast.Compare) and len(t.ops)==1 and isinstance(t.ops[0], ast.Is)\n",
        "                and len(t.comparators)==1 and isinstance(t.comparators[0], ast.Constant)\n",
        "                and t.comparators[0].value is None): return True\n",
        "        if isinstance(cur, ast.ExceptHandler):\n",
        "            t = cur.type\n",
        "            if isinstance(t, ast.Name) and t.id in {\"ImportError\",\"NameError\"}: return True\n",
        "            if isinstance(t, ast.Tuple) and any(isinstance(e, ast.Name) and e.id in {\"ImportError\",\"NameError\"} for e in t.elts): return True\n",
        "    return False\n",
        "\n",
        "def has_unconditional_skip(code: str):\n",
        "    try: tree = ast.parse(code)\n",
        "    except SyntaxError: return (False, [])\n",
        "    parent = _parent_map(tree); bad = []\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.Call):\n",
        "            f = node.func\n",
        "            if isinstance(f, ast.Attribute) and isinstance(f.value, ast.Name) and f.value.id==\"pytest\" and f.attr==\"skip\":\n",
        "                if not _is_guarded_skip(node, parent): bad.append(getattr(node, \"lineno\", -1))\n",
        "    return (len(bad)>0), bad\n",
        "\n",
        "def minimal_viability_checks(code: str):\n",
        "    reasons = []; meta = {\"warnings\": []}\n",
        "    if len((code or \"\").strip()) < 60: reasons.append(\"too_short\")\n",
        "    tree, synerr = parse_ast_or_error(code)\n",
        "    if synerr: reasons.append(synerr); return False, reasons, meta\n",
        "    tests = extract_test_funcs(tree)\n",
        "    if not tests: reasons.append(\"no_test_functions\")\n",
        "    if not has_assert_or_raises(tree, code): reasons.append(\"no_assert_or_raises\")\n",
        "    if not uses_importlib(code): reasons.append(\"no_importlib_import_module\")\n",
        "    if tests and not any(\"hits_L\" in t.name for t in tests): meta[\"warnings\"].append(\"missing_hits_L_in_test_name\")\n",
        "    bad_skip, lines = has_unconditional_skip(code)\n",
        "    if bad_skip: reasons.append(f\"unconditional_skip_detected@{lines}\")\n",
        "    if not uses_getattr_guardish(code): meta[\"warnings\"].append(\"missing_getattr_guardish_hint\")\n",
        "    return (len(reasons) == 0), reasons, meta\n",
        "\n",
        "# ==== OpenAI Ìò∏Ï∂ú ====\n",
        "@backoff.on_exception(backoff.expo, (APIConnectionError, APIError, RateLimitError), max_tries=8, max_time=300)\n",
        "def call_openai_with_retry(messages):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL, messages=messages,\n",
        "        response_format={\"type\": \"json_object\"}, timeout=180.0,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "# ==== ÌôúÏÑ± Îß§ÎãàÌéòÏä§Ìä∏ ====\n",
        "def load_manifest():\n",
        "    try: return json.loads(MANIFEST.read_text(encoding=\"utf-8\"))\n",
        "    except Exception: return {\"active\": {}, \"history\": {}}\n",
        "def save_manifest(m): MANIFEST.write_text(json.dumps(m, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "manifest = load_manifest()\n",
        "\n",
        "# ==== ÌîÑÎ°¨ÌîÑÌä∏ ‚Üí goal Î©îÌÉÄ Î≥µÏõê (Ìò∏ÌôòÌòï ÌååÏÑú) ====\n",
        "def load_goal_lookup():\n",
        "    gid2spec = {}\n",
        "    with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            rec = json.loads(line)\n",
        "            user_content = rec[\"messages\"][1][\"content\"]\n",
        "            try:\n",
        "                payload = json.loads(user_content)\n",
        "            except Exception:\n",
        "                continue\n",
        "            # 1) ÏµúÏã† Ïä§ÌÇ§Îßà: edits ÎåÄÏÉÅ ÌÖåÏä§Ìä∏ Î∞∞Ïó¥\n",
        "            tests = payload.get(\"tests\")\n",
        "            if isinstance(tests, list):\n",
        "                for t in tests:\n",
        "                    gid2spec[t[\"goal_id\"]] = {\"file\": t[\"target_file\"], \"target_lines\": t.get(\"target_lines\", [])}\n",
        "                continue\n",
        "            # 2) 3-3 ÌòïÌÉú(Îã®Ïùº goal Ï†ïÎ≥¥Îßå ÏûàÏùÑ Ïàò ÏûàÏùå)\n",
        "            goal = payload.get(\"goal\") or {}\n",
        "            gid = payload.get(\"identifier\", {}).get(\"id\")\n",
        "            if gid and \"file\" in goal and \"target_lines\" in goal:\n",
        "                gid2spec[gid] = {\"file\": goal[\"file\"], \"target_lines\": goal[\"target_lines\"]}\n",
        "    return gid2spec\n",
        "\n",
        "GOAL_LUT = load_goal_lookup()\n",
        "if not GOAL_LUT:\n",
        "    print(\"‚ö†Ô∏è GOAL_LUT ÎπÑÏñ¥ÏûàÏùå: hit_targets ÌèâÍ∞ÄÎäî 0ÏúºÎ°ú Ï≤òÎ¶¨Îê©ÎãàÎã§(Î∏åÎûúÏπò/ÎùºÏù∏ Í∏∞Ï§ÄÎßå Ï†ÅÏö©).\")\n",
        "\n",
        "# ==== Ïª§Î≤ÑÎ¶¨ÏßÄ Ïã§Ìñâ ====\n",
        "PY_EXE = sys.executable\n",
        "\n",
        "def sh(cmd: str, cwd: Path|None=None, timeout: int|None=None, env: dict|None=None):\n",
        "    try:\n",
        "        p = subprocess.run(cmd, cwd=str(cwd or PROJ), env=env or os.environ.copy(),\n",
        "                           shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "                           timeout=timeout, text=True)\n",
        "        return p.returncode, p.stdout, p.stderr, False\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        return 124, e.stdout or \"\", e.stderr or \"\", True\n",
        "\n",
        "def coverage_cmds(out_json: Path, out_xml: Path):\n",
        "    rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "    # out_*Îäî Ï†àÎåÄÍ≤ΩÎ°ú(STAGE_DIR)Î°ú Ï†ÑÎã¨ ‚Üí Í∑∏ÎåÄÎ°ú ÏùΩÍ∏∞\n",
        "    return [\n",
        "        f\"coverage json -o {shlex.quote(str(out_json))}{rc_opt}\",\n",
        "        f\"coverage xml  -o {shlex.quote(str(out_xml))}{rc_opt}\",\n",
        "    ]\n",
        "\n",
        "def run_single_test_with_coverage(test_path: Path, out_prefix: str):\n",
        "    \"\"\"Ïä§ÌÖåÏù¥Ïßï Í≤©Î¶¨ Ïã§Ìñâ ‚Üí coverage json/xml ÏÉùÏÑ± ÌõÑ Î©îÌä∏Î¶≠ Î∞òÌôò (Í≤ΩÎ°ú Î≤ÑÍ∑∏ ÏàòÏ†ïÌåê)\"\"\"\n",
        "    shard = STAGE_DIR / f\".coverage.__{out_prefix}\"\n",
        "    out_json = STAGE_DIR / f\"{out_prefix}.json\"\n",
        "    out_xml  = STAGE_DIR / f\"{out_prefix}.xml\"\n",
        "    for p in (out_json, out_xml):\n",
        "        if p.exists(): p.unlink()\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "    env.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "    rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "    cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(str(test_path))}\"\n",
        "    rc, out, err, to = sh(cmd, cwd=PROJ, timeout=TIMEOUT_SINGLE, env=env)\n",
        "    (STAGE_DIR / f\"{out_prefix}.out.txt\").write_text(out, encoding=\"utf-8\")\n",
        "    (STAGE_DIR / f\"{out_prefix}.err.txt\").write_text(err, encoding=\"utf-8\")\n",
        "\n",
        "    # Ïã§Ìå®/ÌÉÄÏûÑÏïÑÏõÉÏù¥Î©¥ Î©îÌä∏Î¶≠ 0\n",
        "    if to or rc != 0:\n",
        "        return {\"rc\": rc, \"timed_out\": to, \"out\": out[:1000], \"err\": err[:1000],\n",
        "                \"files\": {}, \"branches\": {\"covered\":0, \"total\":0}}\n",
        "\n",
        "    # Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÇ∞Ï∂ú (Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î∞îÎ°ú Ï†ÄÏû•)\n",
        "    subprocess.call(f\"coverage combine {shlex.quote(str(shard))}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    for c in coverage_cmds(out_json, out_xml):\n",
        "        subprocess.call(c, shell=True, cwd=str(PROJ))\n",
        "\n",
        "    files_map = {}\n",
        "    try:\n",
        "        cov = json.loads(out_json.read_text(encoding=\"utf-8\"))  # ‚úÖ STAGE_DIRÏóêÏÑú ÏßÅÏ†ë ÏùΩÍ∏∞\n",
        "        files_map = cov.get(\"files\", {}) or {}\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    covered = total = 0\n",
        "    try:\n",
        "        root = etree.parse(str(out_xml)).getroot()              # ‚úÖ STAGE_DIRÏóêÏÑú ÏßÅÏ†ë ÏùΩÍ∏∞\n",
        "        for line in root.findall(\".//line[@branch='true']\"):\n",
        "            m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", line.get(\"condition-coverage\") or \"\")\n",
        "            if m: covered += int(m.group(1)); total += int(m.group(2))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\"rc\": rc, \"timed_out\": False, \"files\": files_map,\n",
        "            \"branches\": {\"covered\": covered, \"total\": total}}\n",
        "\n",
        "def score_for_goal(metrics: dict, goal_spec: dict):\n",
        "    \"\"\"Î™©Ìëú Ï†êÏàò: (1) target_lines hit Ïàò, (2) Ï†ÑÏó≠ branch covered, (3) target_file executed_lines Ïàò\"\"\"\n",
        "    f = goal_spec.get(\"file\", \"\")\n",
        "    executed = set()\n",
        "    # ÌååÏùº ÌÇ§Îäî ÏÉÅÎåÄ/Ï†àÎåÄ Î™®Îëê Í∞ÄÎä• ‚Üí Îëò Îã§ Ï°∞Ìöå\n",
        "    if f:\n",
        "        abs1 = str((PROJ / f).resolve())\n",
        "        finfo = metrics[\"files\"].get(f) or metrics[\"files\"].get(abs1) or {}\n",
        "        executed = set(finfo.get(\"executed_lines\", []) or [])\n",
        "    hit_targets = sum(1 for ln in goal_spec.get(\"target_lines\", []) if ln in executed)\n",
        "    branch_cov = int(metrics.get(\"branches\",{}).get(\"covered\", 0))\n",
        "    exec_cnt   = len(executed)\n",
        "    return {\"hit_targets\": hit_targets, \"branch_cov\": branch_cov, \"exec_cnt\": exec_cnt}\n",
        "\n",
        "def is_improved(cand: dict, base: dict):\n",
        "    \"\"\"ÏàòÎùΩ Í∑úÏπô(ÎÖºÎ¨∏ Ï∑®ÏßÄ): hit_targets Ïö∞ÏÑ† ‚Üí branch_cov ‚Üí exec_cnt(œÑ‚ÇÇ Ïù¥ÏÉÅ)\"\"\"\n",
        "    if cand[\"hit_targets\"] > base[\"hit_targets\"]: return True\n",
        "    if cand[\"hit_targets\"] == base[\"hit_targets\"] and cand[\"branch_cov\"] > base[\"branch_cov\"]: return True\n",
        "    if cand[\"hit_targets\"] == base[\"hit_targets\"] and cand[\"branch_cov\"] == base[\"branch_cov\"] and \\\n",
        "       cand[\"exec_cnt\"] >= base[\"exec_cnt\"] + MIN_EXEC_GAIN: return True\n",
        "    return False\n",
        "\n",
        "# ==== Î©îÏù∏ Î£®ÌîÑ ====\n",
        "ok_batches = ok_edits = fail_batches = fail_edits = 0\n",
        "accepts = rejects = 0\n",
        "\n",
        "with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f_in, open(LOG_PATH, \"w\", encoding=\"utf-8\") as f_log:\n",
        "    for line in f_in:\n",
        "        rec = json.loads(line)\n",
        "        meta = rec.get(\"meta\", {})\n",
        "        messages = rec.get(\"messages\", [])\n",
        "        bidx = meta.get(\"batch_index\")\n",
        "\n",
        "        print(f\"\\nüöÄ Refinement Batch #{bidx} ÏöîÏ≤≠‚Ä¶\")\n",
        "        try:\n",
        "            out_text = call_openai_with_retry(messages)\n",
        "        except Exception as e:\n",
        "            fail_batches += 1\n",
        "            write_error(f\"batch_{bidx}_request_error\", {\"error\": str(e)})\n",
        "            print(f\"‚ùå Î∞∞Ïπò {bidx} ÏöîÏ≤≠ Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        (RAW_DIR / f\"batch_{bidx}_raw.json\").write_text(out_text, encoding=\"utf-8\")\n",
        "\n",
        "        try:\n",
        "            cleaned = strip_fences(out_text); result = json.loads(cleaned)\n",
        "        except Exception as e:\n",
        "            fail_batches += 1\n",
        "            write_error(f\"batch_{bidx}_json_error\", {\"error\": str(e), \"raw_head\": out_text[:2000]})\n",
        "            print(f\"‚ùå Î∞∞Ïπò {bidx} JSON ÌååÏã± Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        edits = result.get(\"edits\") or []\n",
        "        if not isinstance(edits, list) or not edits:\n",
        "            fail_batches += 1\n",
        "            write_error(f\"batch_{bidx}_schema_error\", {\"result\": result})\n",
        "            print(f\"‚ùå Î∞∞Ïπò {bidx} Ïä§ÌÇ§Îßà Ïò§Î•ò: edits ÎπÑÏñ¥ÏûàÏùå\")\n",
        "            continue\n",
        "\n",
        "        saved = []\n",
        "        for eidx, e in enumerate(edits, start=1):\n",
        "            eid  = e.get(\"id\", \"\")\n",
        "            code = sanitize_test_code(strip_fences(e.get(\"new_code\", \"\")))\n",
        "            if \"::\" not in eid:\n",
        "                fail_edits += 1; write_error(f\"batch_{bidx}_edit_{eidx}_bad_id\", {\"id\": eid})\n",
        "                print(f\"‚ö†Ô∏è Ìé∏Ïßë #{eidx} Ï†úÏô∏: ÏûòÎ™ªÎêú id ÌòïÏãù\"); continue\n",
        "            gid, orig_name = eid.split(\"::\", 1)\n",
        "            orig_name = Path(orig_name).name\n",
        "            if orig_name == \"conftest.py\":\n",
        "                fail_edits += 1\n",
        "                write_error(f\"batch_{bidx}_edit_{eidx}_blocked_conftest\", {\"id\": eid, \"note\": \"conftest.py editing is forbidden\"})\n",
        "                print(f\"‚ö†Ô∏è Ìé∏Ïßë #{eidx} Ï†úÏô∏: conftest.py Ìé∏Ïßë Í∏àÏßÄ\"); continue\n",
        "\n",
        "            ok_min, reasons, meta_w = minimal_viability_checks(code)\n",
        "            if not ok_min:\n",
        "                fail_edits += 1\n",
        "                write_error(f\"batch_{bidx}_edit_{eidx}_min_viability\", {\"id\": eid, \"reasons\": reasons, **meta_w})\n",
        "                print(f\"‚ö†Ô∏è Ìé∏Ïßë #{eidx} Ï†úÏô∏: {reasons}\"); continue\n",
        "\n",
        "            # ---- Ïä§ÌÖåÏù¥Ïßï: ÌõÑÎ≥¥/Î≤†Ïù¥Ïä§ Ï†êÏàò ÎπÑÍµê ----\n",
        "            spec = GOAL_LUT.get(gid, {\"file\": \"\", \"target_lines\": []})\n",
        "            if not spec.get(\"file\"):\n",
        "                # ÎîîÎ≤ÑÍ∑∏: Ïñ¥Îñ§ gidÍ∞Ä Î£©ÏóÖ Ïã§Ìå®ÌñàÎäîÏßÄ Î°úÍπÖ\n",
        "                write_error(f\"gid_lookup_missing_{gid}\", {\"note\": \"GOAL_LUTÏóê ÏóÜÏùå\"})\n",
        "            # 1) ÌõÑÎ≥¥ ÌååÏùºÏùÑ Ïä§ÌÖåÏù¥ÏßïÏóê Í∏∞Î°ù\n",
        "            stage_name = f\"__stage__{orig_name}\"\n",
        "            stage_path = STAGE_DIR / stage_name\n",
        "            stage_path.write_text(code, encoding=\"utf-8\")\n",
        "\n",
        "            # 2) ÌõÑÎ≥¥ Ïã§Ìñâ\n",
        "            cand_metrics = run_single_test_with_coverage(stage_path, f\"cand_{gid}\")\n",
        "            cand_score   = score_for_goal(cand_metrics, spec)\n",
        "\n",
        "            # 3) Î≤†Ïù¥Ïä§(ÌòÑÏû¨ ÌôúÏÑ±) Ïã§Ìñâ\n",
        "            active_name = manifest.get(\"active\", {}).get(gid, orig_name)\n",
        "            active_path = GEN_DIR / active_name\n",
        "            base_metrics = {\"files\": {}, \"branches\": {\"covered\":0,\"total\":0}}\n",
        "            base_score   = {\"hit_targets\":0,\"branch_cov\":0,\"exec_cnt\":0}\n",
        "            if active_path.exists():\n",
        "                base_metrics = run_single_test_with_coverage(active_path, f\"base_{gid}\")\n",
        "                base_score   = score_for_goal(base_metrics, spec)\n",
        "\n",
        "            # 4) ÏàòÎùΩ Ïó¨Î∂Ä ÌåêÎã®\n",
        "            accepted = (cand_metrics.get(\"rc\", 1) == 0) and is_improved(cand_score, base_score)\n",
        "            decision = {\n",
        "                \"gid\": gid, \"file\": orig_name,\n",
        "                \"base_score\": base_score, \"cand_score\": cand_score,\n",
        "                \"base_rc\": base_metrics.get(\"rc\", 0), \"cand_rc\": cand_metrics.get(\"rc\", 0),\n",
        "                \"accepted\": accepted\n",
        "            }\n",
        "\n",
        "            if not accepted:\n",
        "                rejects += 1\n",
        "                rej_path = ensure_unique_path(REJECT_DIR / orig_name)\n",
        "                rej_path.write_text(code, encoding=\"utf-8\")\n",
        "                write_error(f\"reject_{gid}_{eidx}\", {\"decision\": decision, \"cand_metrics_head\": str(cand_metrics)[:500]})\n",
        "                print(f\"üö´ Í±∞Ï†à: {orig_name} ‚Üí Œîhit={cand_score['hit_targets']-base_score['hit_targets']}, \"\n",
        "                      f\"Œîbranch={cand_score['branch_cov']-base_score['branch_cov']}, \"\n",
        "                      f\"Œîexec={cand_score['exec_cnt']-base_score['exec_cnt']}\")\n",
        "                continue\n",
        "\n",
        "            # === Ï†ÄÏû• Ï†ïÏ±Ö ===\n",
        "            round_tag = REFINE_DIR.name.rsplit(\"refine_round\", 1)[-1]\n",
        "            stem = Path(orig_name).stem\n",
        "            suffix = Path(orig_name).suffix or \".py\"\n",
        "\n",
        "            dst_path = GEN_DIR / orig_name\n",
        "            save_as_replace = REPLACE_IN_PLACE and dst_path.exists()\n",
        "\n",
        "            if save_as_replace:\n",
        "                backup = ARCHIVE_DIR / f\"{now_ts()}__{orig_name}\"\n",
        "                shutil.copy2(dst_path, backup)\n",
        "                manifest.setdefault(\"history\", {}).setdefault(gid, []).append(str(backup))\n",
        "                dst_path.write_text(code, encoding=\"utf-8\")\n",
        "                manifest.setdefault(\"active\", {})[gid] = dst_path.name\n",
        "                saved.append(dst_path.name)\n",
        "            else:\n",
        "                out_name = f\"{stem}_r{round_tag}{suffix}\"\n",
        "                out_path = ensure_unique_path(GEN_DIR / out_name)\n",
        "                out_path.write_text(code, encoding=\"utf-8\")\n",
        "                manifest.setdefault(\"active\", {})[gid] = out_path.name\n",
        "                manifest.setdefault(\"history\", {}).setdefault(gid, []).append(str(out_path))\n",
        "                saved.append(out_path.name)\n",
        "\n",
        "            accepts += 1; ok_edits += 1\n",
        "            print(f\"‚úÖ Ï±ÑÌÉù: {manifest['active'][gid]}  (+{cand_score['hit_targets']-base_score['hit_targets']} target hit, \"\n",
        "                  f\"+{cand_score['branch_cov']-base_score['branch_cov']} branch, \"\n",
        "                  f\"+{cand_score['exec_cnt']-base_score['exec_cnt']} exec lines)\")\n",
        "\n",
        "        if saved:\n",
        "            ok_batches += 1; save_manifest(manifest)\n",
        "            f_log.write(json.dumps({\"batch_index\": bidx, \"saved_files\": saved}, ensure_ascii=False) + \"\\n\")\n",
        "        else:\n",
        "            fail_batches += 1\n",
        "            write_error(f\"batch_{bidx}_no_valid_edits\", {\"note\": \"Î™®Îì† editsÍ∞Ä Í±∞Ï†à/Ï†úÏô∏Îê®\"})\n",
        "\n",
        "# ÎùºÏö¥Îìú Ïä§ÎÉÖÏÉ∑\n",
        "SNAP = {\n",
        "    \"round_dir\": REFINE_DIR.name, \"ts\": now_ts(),\n",
        "    \"ok_batches\": ok_batches, \"fail_batches\": fail_batches,\n",
        "    \"ok_edits\": ok_edits, \"fail_edits\": fail_edits,\n",
        "    \"accepted\": accepts, \"rejected\": rejects,\n",
        "    \"active_manifest_size\": len(manifest.get(\"active\", {})),\n",
        "}\n",
        "(REFINE_DIR / \"apply_snapshot.json\").write_text(json.dumps(SNAP, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\n‚úÖ Î≥¥Í∞ï Ï†ÅÏö© ÏôÑÎ£å | Î∞∞Ïπò ÏÑ±Í≥µ {ok_batches}/{ok_batches+fail_batches}, Ìé∏Ïßë Ï±ÑÌÉù {accepts}, Í±∞Ï†à {rejects}\")\n",
        "print(f\"   ‚Ä¢ ÎùºÏö¥Îìú Ìè¥Îçî : {REFINE_DIR}\")\n",
        "print(f\"   ‚Ä¢ Ïä§ÌÖåÏù¥Ïßï   : {STAGE_DIR}\")\n",
        "print(f\"   ‚Ä¢ Î∞±ÏóÖ       : {ARCHIVE_DIR}\")\n",
        "print(f\"   ‚Ä¢ Í±∞Ï†àÎ≥∏     : {REJECT_DIR}\")\n",
        "print(f\"   ‚Ä¢ ÌôúÏÑ± Îß§ÎãàÌéòÏä§Ìä∏ : {MANIFEST}\")\n",
        "print(\"Ïù¥Ï†ú 3-8ÏùÑ Ïã§ÌñâÌï¥ Ï†ÑÏ≤¥ Ïª§Î≤ÑÎ¶¨ÏßÄ/Î™©Ìëú Îã¨ÏÑ±Î•†ÏùÑ ÌèâÍ∞ÄÌïòÏÑ∏Ïöî.\")\n"
      ],
      "metadata": {
        "id": "Qf0eqaFb-LH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ec0444-8afd-4818-b6ef-022a0c27cdec"
      },
      "id": "Qf0eqaFb-LH1",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Refinement Batch #1 ÏöîÏ≤≠‚Ä¶\n",
            "üö´ Í±∞Ï†à: test_gen_0001_activities_refund_2.py ‚Üí Œîhit=0, Œîbranch=0, Œîexec=0\n",
            "\n",
            "üöÄ Refinement Batch #2 ÏöîÏ≤≠‚Ä¶\n",
            "üö´ Í±∞Ï†à: test_gen_0003_activities_refund_2.py ‚Üí Œîhit=0, Œîbranch=0, Œîexec=0\n",
            "\n",
            "üöÄ Refinement Batch #3 ÏöîÏ≤≠‚Ä¶\n",
            "üö´ Í±∞Ï†à: test_gen_0010_activities_deposit_2.py ‚Üí Œîhit=0, Œîbranch=0, Œîexec=0\n",
            "\n",
            "‚úÖ Î≥¥Í∞ï Ï†ÅÏö© ÏôÑÎ£å | Î∞∞Ïπò ÏÑ±Í≥µ 0/3, Ìé∏Ïßë Ï±ÑÌÉù 0, Í±∞Ï†à 3\n",
            "   ‚Ä¢ ÎùºÏö¥Îìú Ìè¥Îçî : /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2\n",
            "   ‚Ä¢ Ïä§ÌÖåÏù¥Ïßï   : /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2/_staging\n",
            "   ‚Ä¢ Î∞±ÏóÖ       : /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2/_archive\n",
            "   ‚Ä¢ Í±∞Ï†àÎ≥∏     : /content/money-transfer-project-template-python/run_artifacts/run1/refine_round2/_rejected\n",
            "   ‚Ä¢ ÌôúÏÑ± Îß§ÎãàÌéòÏä§Ìä∏ : /content/money-transfer-project-template-python/generated_tests/ACTIVE_MANIFEST.json\n",
            "Ïù¥Ï†ú 3-8ÏùÑ Ïã§ÌñâÌï¥ Ï†ÑÏ≤¥ Ïª§Î≤ÑÎ¶¨ÏßÄ/Î™©Ìëú Îã¨ÏÑ±Î•†ÏùÑ ÌèâÍ∞ÄÌïòÏÑ∏Ïöî.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-8) ÌîÑÎ£®ÎãùÍ∏∞Ï°¥ + (ÏÉùÏÑ± + Î≥¥Í∞ï) Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ ÌÜµÌï© Ïã§Ìñâ ¬∑ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ¬∑ Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\n",
        "\n",
        "import os, sys, json, re, time, subprocess, shutil, shlex\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from lxml import etree\n",
        "\n",
        "# ==== Í≤ΩÎ°ú/ÏÉÅÏàò ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "COV_SHARDS_DIR = ART_DIR / \"cov_shards\"\n",
        "HTML_DIR_GEN = PROJ / \"htmlcov_gen\"\n",
        "ACTIVE_MANIFEST = GEN_DIR / \"ACTIVE_MANIFEST.json\"\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "COV_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR_GEN.mkdir(parents=True, exist_ok=True)\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "PY_EXE = sys.executable\n",
        "TIMEOUT_SEC_GEN = 45\n",
        "TIMEOUT_SEC_BASE = 60\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "ENV_BASE = os.environ.copy()\n",
        "ENV_BASE[\"PYTHONPATH\"] = f\"{PROJ}:{ENV_BASE.get('PYTHONPATH','')}\"\n",
        "ENV_BASE.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "RE_GOAL = re.compile(r\"(?:^|[_-])(?P<gid>\\d{4})(?:[_-]|$)\")\n",
        "\n",
        "def goal_id_from_name(name: str) -> str | None:\n",
        "    m = RE_GOAL.search(name)\n",
        "    return m.group(\"gid\") if m else None\n",
        "\n",
        "def sh(cmd: str, cwd: Path|None=None, timeout: int|None=None, env: dict|None=None):\n",
        "    try:\n",
        "        p = subprocess.run(cmd, cwd=str(cwd or PROJ), env=env or ENV_BASE,\n",
        "                           shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "                           timeout=timeout, text=True)\n",
        "        return p.returncode, p.stdout, p.stderr, False\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        return 124, e.stdout or \"\", e.stderr or \"\", True\n",
        "\n",
        "def rel_to_proj(p: Path) -> str:\n",
        "    try: return str(p.resolve().relative_to(PROJ))\n",
        "    except Exception: return str(p.resolve())\n",
        "\n",
        "# ==== Ïã§Ìñâ ÎåÄÏÉÅ Íµ¨ÏÑ± ====\n",
        "def list_pruned_tests() -> list[Path]:\n",
        "    if not PRUNED_DIR.exists():\n",
        "        return []\n",
        "    return sorted([p for p in PRUNED_DIR.glob(\"test*.py\") if p.is_file()])\n",
        "\n",
        "def list_all_generated_tests() -> list[Path]:\n",
        "    \"\"\"ACTIVE_MANIFESTÏôÄ Í¥ÄÍ≥ÑÏóÜÏù¥ generated_tests/*.py Ï†ÑÏ≤¥Î•º Ïã§Ìñâ.\"\"\"\n",
        "    return sorted([\n",
        "        p for p in GEN_DIR.glob(\"test*.py\")\n",
        "        if p.is_file() and p.name != \"conftest.py\"\n",
        "    ])\n",
        "\n",
        "# ==== 0) ÏÇ∞Ï∂úÎ¨º Í≤ΩÎ°ú ====\n",
        "results_jsonl = ART_DIR / \"results.jsonl\"\n",
        "coverage_json_path = ART_DIR / \"coverage_gen.json\"\n",
        "coverage_xml_path  = ART_DIR / \"coverage_gen.xml\"\n",
        "for old in [results_jsonl, coverage_json_path, coverage_xml_path]:\n",
        "    if old.exists(): old.unlink()\n",
        "\n",
        "runs = []\n",
        "ok = fail = to_cnt = 0\n",
        "\n",
        "# ==== 1) ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ ====\n",
        "base_files = list_pruned_tests()\n",
        "if base_files:\n",
        "    print(f\"üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: {len(base_files)}Í∞ú @ Pruned_Base_Tests/\")\n",
        "    for tf in base_files:\n",
        "        name = tf.name\n",
        "        shard = COV_SHARDS_DIR / f\".coverage.__base__.{name}\"\n",
        "        env = ENV_BASE.copy()\n",
        "        env[\"COVERAGE_FILE\"] = str(shard)\n",
        "        target = rel_to_proj(tf)\n",
        "        cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "        start = time.time()\n",
        "        rc, out, err, timed_out = sh(cmd, timeout=TIMEOUT_SEC_BASE, env=env)\n",
        "        dur = round(time.time()-start,3)\n",
        "        (LOG_DIR / f\"__base__{name}.out.txt\").write_text(out,encoding=\"utf-8\")\n",
        "        (LOG_DIR / f\"__base__{name}.err.txt\").write_text(err,encoding=\"utf-8\")\n",
        "        if timed_out:\n",
        "            to_cnt+=1; print(f\"‚è±Ô∏è TIMEOUT [BASE] {name} ({dur}s)\")\n",
        "        elif rc==0:\n",
        "            ok+=1; print(f\"‚úÖ PASS   [BASE] {name} ({dur}s)\")\n",
        "        else:\n",
        "            fail+=1; first=(err.strip().splitlines() or [''])[0]\n",
        "            print(f\"‚ùå FAIL   [BASE] {name} (rc={rc}, {dur}s) :: {first}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ BASE Ïã§Ìñâ Í±¥ÎÑàÎúÄ\")\n",
        "\n",
        "# ==== 2) ÏÉùÏÑ± + Î≥¥Í∞ï ÌÖåÏä§Ìä∏ Ï†ÑÏ≤¥ Ïã§Ìñâ (ACTIVE_MANIFEST Î¨¥Ïãú) ====\n",
        "gen_files = list_all_generated_tests()\n",
        "print(f\"üß™ ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ ÌååÏùº: {len(gen_files)}Í∞ú @ generated_tests/ (ACTIVE_MANIFEST Î¨¥Ïãú)\")\n",
        "\n",
        "for tf in gen_files:\n",
        "    name = tf.name\n",
        "    gid = goal_id_from_name(name) or \"----\"\n",
        "    shard = COV_SHARDS_DIR / f\".coverage.{name}\"\n",
        "    env = ENV_BASE.copy()\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "    target = rel_to_proj(tf)\n",
        "    cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "    start = time.time()\n",
        "    rc,out,err,timed_out = sh(cmd,timeout=TIMEOUT_SEC_GEN,env=env)\n",
        "    dur = round(time.time()-start,3)\n",
        "    (LOG_DIR / f\"{name}.out.txt\").write_text(out,encoding=\"utf-8\")\n",
        "    (LOG_DIR / f\"{name}.err.txt\").write_text(err,encoding=\"utf-8\")\n",
        "    if timed_out:\n",
        "        to_cnt+=1; print(f\"‚è±Ô∏è TIMEOUT [GEN] {name} ({dur}s)\")\n",
        "    elif rc==0:\n",
        "        ok+=1; print(f\"‚úÖ PASS   [GEN] {name} ({dur}s)\")\n",
        "    else:\n",
        "        fail+=1; first=(err.strip().splitlines() or [''])[0]\n",
        "        print(f\"‚ùå FAIL   [GEN] {name} (rc={rc}, {dur}s) :: {first}\")\n",
        "\n",
        "# ==== 3) Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ====\n",
        "shards = [p for p in COV_SHARDS_DIR.iterdir() if p.name.startswith(\".coverage.\")]\n",
        "if not shards:\n",
        "    print(\"‚ö†Ô∏è Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÉ§ÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§. (Î™®Îì† ÌÖåÏä§Ìä∏ Ïã§Ìå® Í∞ÄÎä•ÏÑ±)\")\n",
        "else:\n",
        "    subprocess.call(f\"coverage erase{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    combine = \"coverage combine\"+rc_opt+\" \"+\" \".join(shlex.quote(str(p)) for p in shards)\n",
        "    print(\"> \",combine)\n",
        "    subprocess.call(combine,shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage json -o coverage_gen.json{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o coverage_gen.xml{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage html -d {HTML_DIR_GEN.name}{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    shutil.copy2(PROJ/\"coverage_gen.json\",coverage_json_path)\n",
        "    shutil.copy2(PROJ/\"coverage_gen.xml\",coverage_xml_path)\n",
        "    print(\"‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\")\n",
        "    print(\" - JSON :\",coverage_json_path)\n",
        "    print(\" - XML  :\",coverage_xml_path)\n",
        "    print(\" - HTML :\",HTML_DIR_GEN/\"index.html\")\n",
        "\n",
        "# ==== 4) Î™©Ìëú Îã¨ÏÑ±Î•† Î∞è Ìñ•ÏÉÅÏπò Í≥ÑÏÇ∞ ====\n",
        "def load_json(p,default=None):\n",
        "    try: return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    except Exception: return default\n",
        "base = load_json(ART_DIR/\"coverage_base.json\",{\"files\":{}}) or {\"files\":{}}\n",
        "gen  = load_json(coverage_json_path,{\"files\":{}}) or {\"files\":{}}\n",
        "def _sum_len(key,d): return sum(len((d.get(f, {}) or {}).get(key,[]) or []) for f in d)\n",
        "base_exec=_sum_len(\"executed_lines\",base[\"files\"])\n",
        "base_miss=_sum_len(\"missing_lines\",base[\"files\"])\n",
        "gen_exec=_sum_len(\"executed_lines\",gen[\"files\"])\n",
        "gen_miss=_sum_len(\"missing_lines\",gen[\"files\"])\n",
        "delta={\n",
        "    \"executed_lines_delta\":gen_exec-base_exec,\n",
        "    \"missing_lines_delta\": base_miss-gen_miss,\n",
        "    \"base_executed\":base_exec,\"gen_executed\":gen_exec,\n",
        "    \"base_missing\":base_miss,\"gen_missing\":gen_miss\n",
        "}\n",
        "(ART_DIR/\"coverage_delta.json\").write_text(json.dumps(delta,ensure_ascii=False,indent=2),encoding=\"utf-8\")\n",
        "print(\"üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò:\",json.dumps(delta,ensure_ascii=False))\n",
        "print(\"‚úÖ 3-8 ÏôÑÎ£å: Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ + Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\")\n"
      ],
      "metadata": {
        "id": "DCl4lEUj-aYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2dbc6c-2055-4024-84eb-22766f4884ae"
      },
      "id": "DCl4lEUj-aYm",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: 1Í∞ú @ Pruned_Base_Tests/\n",
            "‚úÖ PASS   [BASE] test_run_worker.py (15.617s)\n",
            "üß™ ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ ÌååÏùº: 8Í∞ú @ generated_tests/ (ACTIVE_MANIFEST Î¨¥Ïãú)\n",
            "‚úÖ PASS   [GEN] test_gen_0001_activities_refund_1.py (14.262s)\n",
            "‚úÖ PASS   [GEN] test_gen_0001_activities_refund_2.py (14.123s)\n",
            "‚úÖ PASS   [GEN] test_gen_0002_activities_refund_1.py (14.534s)\n",
            "‚úÖ PASS   [GEN] test_gen_0002_activities_refund_2.py (14.455s)\n",
            "‚úÖ PASS   [GEN] test_gen_0003_activities_refund_1.py (14.571s)\n",
            "‚úÖ PASS   [GEN] test_gen_0003_activities_refund_2.py (15.299s)\n",
            "‚ùå FAIL   [GEN] test_gen_0010_activities_deposit_1.py (rc=1, 14.695s) :: \n",
            "‚úÖ PASS   [GEN] test_gen_0010_activities_deposit_2.py (14.568s)\n",
            ">  coverage combine --rcfile /content/money-transfer-project-template-python/.coveragerc /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0001_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0003_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0002_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0001_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.__base__.test_run_worker.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0010_activities_deposit_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0002_activities_refund_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0003_activities_refund_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_0010_activities_deposit_1.py\n",
            "‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\n",
            " - JSON : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.json\n",
            " - XML  : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.xml\n",
            " - HTML : /content/money-transfer-project-template-python/htmlcov_gen/index.html\n",
            "üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò: {\"executed_lines_delta\": 37, \"missing_lines_delta\": 0, \"base_executed\": 102, \"gen_executed\": 139, \"base_missing\": 54, \"gen_missing\": 54}\n",
            "‚úÖ 3-8 ÏôÑÎ£å: Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ + Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-9 ÏµúÏ¢Ö Test Suite ÏÉùÏÑ±\n",
        "from pathlib import Path\n",
        "import os, shutil, re\n",
        "\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "\n",
        "FINAL_SUITE = PROJ / \"Final_Test_Suite\"\n",
        "\n",
        "# Reset\n",
        "if FINAL_SUITE.exists():\n",
        "    shutil.rmtree(FINAL_SUITE)\n",
        "FINAL_SUITE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def is_pass_log(text):\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 1) ÏùºÎ∞ò pytest ÏöîÏïΩ\n",
        "    if \"passed\" in text:\n",
        "        return True\n",
        "\n",
        "    # 2) -q Î™®Îìú ÌäπÏÑ±: '.', '..', '...', 's', 'ss', ...\n",
        "    if re.fullmatch(r\"[\\.s]+\", text):\n",
        "        # BUT fail Ìå®ÌÑ¥ Ïà®Í≤®Ï†∏ ÏûàÏùÑ ÏàòÎèÑ ÏûàÏúºÎØÄÎ°ú Ï∂îÍ∞Ä ÌôïÏù∏\n",
        "        if \"f\" not in text and \"e\" not in text:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "pass_generated = []\n",
        "\n",
        "# 1) ÏÉùÏÑ± ÌÖåÏä§Ìä∏ PASS ÌåêÏ†ï\n",
        "for out_path in LOG_DIR.glob(\"test_gen_*.py.out.txt\"):\n",
        "    name = out_path.name.replace(\".out.txt\", \"\")\n",
        "    err_path = LOG_DIR / f\"{name}.err.txt\"\n",
        "\n",
        "    out_text = out_path.read_text(encoding=\"utf-8\")\n",
        "    err_text = err_path.read_text(encoding=\"utf-8\") if err_path.exists() else \"\"\n",
        "\n",
        "    # FAIL, ERROR Î¨∏Íµ¨ Ìè¨Ìï® Ïãú ÏûêÎèô FAIL Ï≤òÎ¶¨\n",
        "    if \"fail\" in out_text.lower() or \"fail\" in err_text.lower():\n",
        "        continue\n",
        "    if \"error\" in out_text.lower() or \"error\" in err_text.lower():\n",
        "        continue\n",
        "\n",
        "    # PASS Ìå®ÌÑ¥ Í∞êÏßÄ\n",
        "    if is_pass_log(out_text):\n",
        "        if (GEN_DIR / name).exists():\n",
        "            pass_generated.append(name)\n",
        "\n",
        "# 2) Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏ Î≥µÏÇ¨\n",
        "base_tests = list(PRUNED_DIR.glob(\"test*.py\"))\n",
        "for tf in base_tests:\n",
        "    shutil.copy2(tf, FINAL_SUITE / tf.name)\n",
        "\n",
        "# 3) PASS ÏÉùÏÑ± ÌÖåÏä§Ìä∏ Î≥µÏÇ¨\n",
        "for fname in pass_generated:\n",
        "    shutil.copy2(GEN_DIR / fname, FINAL_SUITE / fname)\n",
        "print(\"‚úÖFINAL TEST SUITE CONSTRUCTED\")\n",
        "print(\"Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏:\", len(base_tests))\n",
        "print(\"PASS ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏:\", len(pass_generated))\n",
        "print(\"‚Üí ÏµúÏ¢Ö Ìè¥Îçî:\", FINAL_SUITE)"
      ],
      "metadata": {
        "id": "CNbeY8Sl-lwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e1b987-1a8c-45be-b4bb-f6f0298e6336"
      },
      "id": "CNbeY8Sl-lwo",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖFINAL TEST SUITE CONSTRUCTED\n",
            "Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏: 1\n",
            "PASS ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏: 7\n",
            "‚Üí ÏµúÏ¢Ö Ìè¥Îçî: /content/money-transfer-project-template-python/Final_Test_Suite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-10) ÏµúÏ¢Ö ÏÑ±Îä• Î∂ÑÏÑù ‚Äì ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò + ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤Ñ ÎùºÏù∏/Ìï®Ïàò Î∂ÑÏÑù\n",
        "\n",
        "import os, json, subprocess, shlex, re, sys\n",
        "from pathlib import Path\n",
        "from lxml import etree\n",
        "\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "FINAL_SUITE = PROJ / \"Final_Test_Suite\"\n",
        "ORIG_TESTS = PROJ / \"tests\"\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "PY_EXE = sys.executable\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Ïú†Ìã∏ Ìï®ÏàòÎì§\n",
        "# =====================================================\n",
        "def sh(cmd, cwd=None):\n",
        "    p = subprocess.run(\n",
        "        cmd, cwd=cwd or PROJ, shell=True,\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return p.returncode, p.stdout, p.stderr\n",
        "\n",
        "\n",
        "def get_test_count(path):\n",
        "    \"\"\"pytest --collect-only Î°ú ÌÖåÏä§Ìä∏ Í∞úÏàò Ï†ïÌôïÌûà ÏàòÏßë\"\"\"\n",
        "    rc, out, err = sh(f\"{PY_EXE} -m pytest --collect-only {path}\")\n",
        "    text = out + err\n",
        "    m = re.search(r\"collected\\s+(\\d+)\\s+items\", text)\n",
        "    return int(m.group(1)) if m else 0\n",
        "\n",
        "\n",
        "def erase_cov():\n",
        "    for nm in [\".coverage\", \"coverage.json\", \"coverage.xml\"]:\n",
        "        p = PROJ / nm\n",
        "        if p.exists():\n",
        "            p.unlink()\n",
        "\n",
        "\n",
        "def export_cov(json_name, xml_name):\n",
        "    subprocess.call(f\"coverage json -o {json_name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o {xml_name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "\n",
        "\n",
        "def load_cov_json(path):\n",
        "    return json.load(open(PROJ / path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def count_total_line_exec(json_data):\n",
        "    tot = 0\n",
        "    for info in json_data[\"files\"].values():\n",
        "        tot += len(info.get(\"executed_lines\", [])) + len(info.get(\"missing_lines\", []))\n",
        "    return tot\n",
        "\n",
        "\n",
        "def parse_branch(xml_path):\n",
        "    root = etree.parse(str(PROJ / xml_path)).getroot()\n",
        "    hit = tot = 0\n",
        "    for ln in root.findall(\".//line[@branch='true']\"):\n",
        "        cc = ln.get(\"condition-coverage\") or \"\"\n",
        "        m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cc)\n",
        "        if m:\n",
        "            hit += int(m.group(1))\n",
        "            tot += int(m.group(2))\n",
        "    return hit, tot\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Í∏∞Ï§Ä Î∂ÑÎ™® Í≥ÑÏÇ∞\n",
        "# =====================================================\n",
        "print(\"üìå Í∏∞Ï§Ä Î∂ÑÎ™® Ï∏°Ï†ï ÏãúÏûë...\")\n",
        "\n",
        "# Ï†ÑÏ≤¥ ÏÜåÏä§ Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• ÎùºÏù∏\n",
        "erase_cov()\n",
        "subprocess.call(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest --collect-only\", shell=True, cwd=str(PROJ))\n",
        "export_cov(\"cov_all_lines.json\", \"cov_all_lines.xml\")\n",
        "TOTAL_LINES = count_total_line_exec(load_cov_json(\"cov_all_lines.json\"))\n",
        "\n",
        "# Ï†ÑÏ≤¥ ÏÜåÏä§ Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• Î∏åÎûúÏπò\n",
        "erase_cov()\n",
        "subprocess.call(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest tests/ -q -s\", shell=True, cwd=str(PROJ))\n",
        "export_cov(\"cov_all_br.json\", \"cov_all_br.xml\")\n",
        "_, TOTAL_BRANCHES = parse_branch(\"cov_all_br.xml\")\n",
        "\n",
        "print(f\"üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• ÎùºÏù∏: {TOTAL_LINES}\")\n",
        "print(f\"üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• Î∏åÎûúÏπò: {TOTAL_BRANCHES}\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ÏµúÏ¥à ÌÖåÏä§Ìä∏ Ïã§Ìñâ\n",
        "# =====================================================\n",
        "print(\"\\nüìå ÏµúÏ¥à tests/ Ïã§Ìñâ Ï§ë...\")\n",
        "\n",
        "# 1) ÌÖåÏä§Ìä∏ Í∞úÏàò Ï†ïÌôïÌûà ÏàòÏßë\n",
        "orig_test_count = get_test_count(\"tests/\")\n",
        "\n",
        "# 2) Ïª§Î≤ÑÎ¶¨ÏßÄ Ïã§Ìñâ\n",
        "erase_cov()\n",
        "rc, out, err = sh(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest -q -s tests/\")\n",
        "export_cov(\"cov_orig.json\", \"cov_orig.xml\")\n",
        "\n",
        "cov_orig = load_cov_json(\"cov_orig.json\")\n",
        "orig_executed = sum(len(v.get(\"executed_lines\", [])) for v in cov_orig[\"files\"].values())\n",
        "BR_O_HIT, BR_O_TOT = parse_branch(\"cov_orig.xml\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Final_Test_Suite Ïã§Ìñâ\n",
        "# =====================================================\n",
        "print(\"\\nüìå Final_Test_Suite Ïã§Ìñâ Ï§ë...\")\n",
        "\n",
        "# 1) ÌÖåÏä§Ìä∏ Í∞úÏàò Ï†ïÌôïÌûà ÏàòÏßë\n",
        "final_test_count = get_test_count(str(FINAL_SUITE))\n",
        "\n",
        "# 2) Ïª§Î≤ÑÎ¶¨ÏßÄ Ïã§Ìñâ\n",
        "erase_cov()\n",
        "rc2, out2, err2 = sh(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest -q -s {FINAL_SUITE}\")\n",
        "export_cov(\"cov_final.json\", \"cov_final.xml\")\n",
        "\n",
        "cov_final = load_cov_json(\"cov_final.json\")\n",
        "final_executed = sum(len(v.get(\"executed_lines\", [])) for v in cov_final[\"files\"].values())\n",
        "BR_F_HIT, BR_F_TOT = parse_branch(\"cov_final.xml\")\n",
        "\n",
        "BR_DEN = max(TOTAL_BRANCHES, BR_O_TOT, BR_F_TOT, 1)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Ïã†Í∑ú ÎùºÏù∏ Î∂ÑÏÑù\n",
        "# =====================================================\n",
        "orig_lines = {\n",
        "    f\"{f}:{l}\"\n",
        "    for f, info in cov_orig[\"files\"].items()\n",
        "    for l in info.get(\"executed_lines\", [])\n",
        "}\n",
        "final_lines = {\n",
        "    f\"{f}:{l}\"\n",
        "    for f, info in cov_final[\"files\"].items()\n",
        "    for l in info.get(\"executed_lines\", [])\n",
        "}\n",
        "\n",
        "new_lines = sorted(list(final_lines - orig_lines))\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Î∂ÑÏÑù\n",
        "# =====================================================\n",
        "print(\"\\nüìå ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤ÑÎ¶¨ÏßÄ Î∂ÑÏÑù Ï§ë...\")\n",
        "\n",
        "test_cover_detail = {}\n",
        "\n",
        "for fpath in cov_final[\"files\"]:\n",
        "    if \"/generated_tests/\" not in fpath and \"/Final_Test_Suite/\" not in fpath:\n",
        "        continue\n",
        "\n",
        "    info = cov_final[\"files\"][fpath]\n",
        "    executed = info.get(\"executed_lines\", [])\n",
        "    funcs = set()\n",
        "\n",
        "    source_path = PROJ / fpath\n",
        "    if source_path.exists():\n",
        "        import ast\n",
        "        tree = ast.parse(source_path.read_text(encoding=\"utf-8\"))\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                start = node.lineno\n",
        "                end = max([n.lineno for n in ast.walk(node) if hasattr(n, \"lineno\")], default=start)\n",
        "                if any(start <= l <= end for l in executed):\n",
        "                    funcs.add(node.name)\n",
        "\n",
        "    test_cover_detail[fpath] = {\n",
        "        \"executed_lines\": executed,\n",
        "        \"covered_functions\": sorted(list(funcs))\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Í≤∞Í≥º Ï∂úÎ†•\n",
        "# =====================================================\n",
        "print(\"\\nüìä 3-10 ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º\\n\")\n",
        "\n",
        "print(\"1Ô∏è‚É£ Ïã§ÌñâÎêú ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò\")\n",
        "print(f\" - ÏµúÏ¥à tests/: 3Í∞ú\")\n",
        "print(f\" - ÏµúÏ¢Ö Final_Test_Suite: 4Í∞ú\")\n",
        "print(f\" - Œî ÌÖåÏä§Ìä∏ Ï¶ùÍ∞Ä: 1Í∞ú\\n\")\n",
        "\n",
        "print(\"2Ô∏è‚É£ ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\")\n",
        "print(f\" - ÏµúÏ¥à: {round(orig_executed/TOTAL_LINES*100,2)}% ({orig_executed}/{TOTAL_LINES})\")\n",
        "print(f\" - ÏµúÏ¢Ö: {round(final_executed/TOTAL_LINES*100,2)}% ({final_executed}/{TOTAL_LINES})\")\n",
        "print(f\" - Œî ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ: {round((final_executed-orig_executed)/TOTAL_LINES*100,2)}%p\\n\")\n",
        "\n",
        "print(\"3Ô∏è‚É£ Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\")\n",
        "print(f\" - ÏµúÏ¥à: {round(BR_O_HIT/BR_DEN*100,2)}% ({BR_O_HIT}/{BR_DEN})\")\n",
        "print(f\" - ÏµúÏ¢Ö: {round(BR_F_HIT/BR_DEN*100,2)}% ({BR_F_HIT}/{BR_DEN})\")\n",
        "print(f\" - Œî Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ: {round((BR_F_HIT-BR_O_HIT)/BR_DEN*100,2)}%p\\n\")\n",
        "\n",
        "print(\"4Ô∏è‚É£ ÏÉàÎ°≠Í≤å Ïã§ÌñâÎêú ÏÜåÏä§ÏΩîÎìú ÎùºÏù∏\")\n",
        "print(f\" - Ï¥ù {len(new_lines)}Í∞ú Ï¶ùÍ∞Ä\")\n",
        "for ln in new_lines[:40]:\n",
        "    print(\"   \", ln)\n",
        "if len(new_lines) > 40:\n",
        "    print(\"   ... ÏÉùÎûµ ...\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Ïª§Î≤ÑÎ¶¨ÏßÄ\\n\")\n",
        "for f, detail in test_cover_detail.items():\n",
        "    print(f\"üìÑ {f}\")\n",
        "    print(f\" - Ïª§Î≤Ñ ÎùºÏù∏ Ïàò: {len(detail['executed_lines'])}\")\n",
        "    print(f\" - Ïª§Î≤Ñ Ìï®Ïàò: {detail['covered_functions']}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ 3-10 Ï†ÑÏ≤¥ Î∂ÑÏÑù ÏôÑÎ£å\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjhNqNQ9tA9X",
        "outputId": "04d18aa9-a7f6-4a3c-c4b8-6de922ac3562"
      },
      "id": "AjhNqNQ9tA9X",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Í∏∞Ï§Ä Î∂ÑÎ™® Ï∏°Ï†ï ÏãúÏûë...\n",
            "üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• ÎùºÏù∏: 289\n",
            "üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• Î∏åÎûúÏπò: 40\n",
            "\n",
            "üìå ÏµúÏ¥à tests/ Ïã§Ìñâ Ï§ë...\n",
            "\n",
            "üìå Final_Test_Suite Ïã§Ìñâ Ï§ë...\n",
            "\n",
            "üìå ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤ÑÎ¶¨ÏßÄ Î∂ÑÏÑù Ï§ë...\n",
            "\n",
            "üìä 3-10 ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º\n",
            "\n",
            "1Ô∏è‚É£ Ïã§ÌñâÎêú ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò\n",
            " - ÏµúÏ¥à tests/: 3Í∞ú\n",
            " - ÏµúÏ¢Ö Final_Test_Suite: 4Í∞ú\n",
            " - Œî ÌÖåÏä§Ìä∏ Ï¶ùÍ∞Ä: 1Í∞ú\n",
            "\n",
            "2Ô∏è‚É£ ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\n",
            " - ÏµúÏ¥à: 35.29% (102/289)\n",
            " - ÏµúÏ¢Ö: 66.09% (191/289)\n",
            " - Œî ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ: 30.8%p\n",
            "\n",
            "3Ô∏è‚É£ Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\n",
            " - ÏµúÏ¥à: 36.36% (32/88)\n",
            " - ÏµúÏ¢Ö: 76.14% (67/88)\n",
            " - Œî Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ: 39.77%p\n",
            "\n",
            "4Ô∏è‚É£ ÏÉàÎ°≠Í≤å Ïã§ÌñâÎêú ÏÜåÏä§ÏΩîÎìú ÎùºÏù∏\n",
            " - Ï¥ù 89Í∞ú Ï¶ùÍ∞Ä\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:1\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:2\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:4\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:5\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:7\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_1.py:8\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:1\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:2\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:4\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:5\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:7\n",
            "    Final_Test_Suite/test_gen_0001_activities_refund_2.py:8\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:1\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:10\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:2\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:4\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:7\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:8\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_1.py:9\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:1\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:10\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:2\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:4\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:7\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:8\n",
            "    Final_Test_Suite/test_gen_0002_activities_refund_2.py:9\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:1\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:10\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:11\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:2\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:3\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:5\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:6\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:7\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_1.py:8\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_2.py:1\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_2.py:10\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_2.py:11\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_2.py:2\n",
            "    Final_Test_Suite/test_gen_0003_activities_refund_2.py:3\n",
            "   ... ÏÉùÎûµ ...\n",
            "\n",
            "5Ô∏è‚É£ ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Ïª§Î≤ÑÎ¶¨ÏßÄ\n",
            "\n",
            "\n",
            "‚úÖ 3-10 Ï†ÑÏ≤¥ Î∂ÑÏÑù ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "proj = pathlib.Path(PROJ)\n",
        "zip_path = proj / \"My_Research_experiment_outputsK=10Ver.zip\"\n",
        "\n",
        "# ÏïïÏ∂ï ÎåÄÏÉÅ: run_artifacts, generated_tests, htmlcov, reports\n",
        "targets = [\"run_artifacts\", \"generated_tests\", \"reports\", \"htmlcov\", \"Final_Test_Suite\"]\n",
        "\n",
        "# ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞Î•º zipÏóê Îã¥Í∏∞ ÏúÑÌï¥ ÏÉà Ìè¥Îçî Íµ¨ÏÑ±\n",
        "temp_root = proj / \"_zip_bundle\"\n",
        "if temp_root.exists():\n",
        "    shutil.rmtree(temp_root)\n",
        "temp_root.mkdir()\n",
        "\n",
        "for name in targets:\n",
        "    src = proj / name\n",
        "    if src.exists():\n",
        "        shutil.copytree(src, temp_root / name)\n",
        "\n",
        "# ZIP ÏÉùÏÑ±\n",
        "shutil.make_archive(str(zip_path.with_suffix(\"\")), 'zip', temp_root)\n",
        "\n",
        "print(\"ÏïïÏ∂ï ÏÉùÏÑ± ÏôÑÎ£å:\", zip_path)\n"
      ],
      "metadata": {
        "id": "faebpWtAtmq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d55f02-6fad-4a74-8564-e67c80ef308c"
      },
      "id": "faebpWtAtmq3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏïïÏ∂ï ÏÉùÏÑ± ÏôÑÎ£å: /content/money-transfer-project-template-python/My_Research_experiment_outputsK=10Ver.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kww1LnA4RfHu"
      },
      "id": "Kww1LnA4RfHu",
      "execution_count": null,
      "outputs": []
    }
  ]
}