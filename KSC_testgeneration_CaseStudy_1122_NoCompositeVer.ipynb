{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mjpe7STyxkS"
      },
      "source": [
        "# KSC: LLM-Driven **no Composite ver** Test Generation (GPT‚Äë4o, Python)\n",
        "**Repo:** `temporalio/money-transfer-project-template-python`  \n",
        "**LLM:** OpenAI GPT‚Äë4o  \n",
        "**Notebook Î™©Ï†Å:** *Î∂ÑÍ∏∞ ÏùòÎ¨¥ + Def‚ÄëUse Ï≤¥Ïù∏ + ÏòàÏô∏ Í≤ΩÎ°ú*Î•º ÌÜµÌï©Ìïú ÌÖåÏä§Ìä∏Î•º ÏûêÎèô ÏÉùÏÑ±¬∑Ïã§Ìñâ¬∑Ï¶ùÎ∂ÑÌï©ÎãàÎã§."
      ],
      "id": "0mjpe7STyxkS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-0) Îü∞ÌÉÄÏûÑ & ÏùòÏ°¥ÏÑ± Ï§ÄÎπÑ\n",
        "import os, sys, subprocess, pathlib, getpass, shutil\n",
        "\n",
        "# 1) Í∏∞Î≥∏ ÏÉÅÏàò/Í≤ΩÎ°ú\n",
        "REPO_URL = \"https://github.com/temporalio/money-transfer-project-template-python.git\"\n",
        "PROJECT_NAME = REPO_URL.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "ROOT = pathlib.Path(\".\").resolve()\n",
        "PROJ = ROOT / PROJECT_NAME\n",
        "\n",
        "def sh(cmd: str, check: bool = True, cwd: pathlib.Path | None = None) -> None:\n",
        "    print(\"> \", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True, cwd=str(cwd) if cwd else None)\n",
        "    if check and rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "\n",
        "print(f\"Python {sys.version}\")\n",
        "print(\"ROOT:\", ROOT)\n",
        "\n",
        "# 2) Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏\n",
        "if PROJ.exists():\n",
        "    print(f\"Repo exists at {PROJ}. Pulling latest‚Ä¶\")\n",
        "    sh(f\"git -C {PROJ} fetch --all --prune\")\n",
        "    sh(f\"git -C {PROJ} reset --hard origin/main\")\n",
        "else:\n",
        "    sh(f\"git clone --depth=1 {REPO_URL}\")\n",
        "print(\"‚úÖ Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å\")\n",
        "\n",
        "# 3) pip ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\n",
        "def pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs\n",
        "    print(\"> \", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "pkgs_core = [\n",
        "    \"pytest==7.4.4\", \"pytest-asyncio==0.23.7\", \"pytest-mock==3.11.1\", \"pytest-cov==4.1.0\",\n",
        "    \"coverage[toml]==7.6.1\",\n",
        "    \"temporalio==1.7.0\",\n",
        "    \"lxml==5.2.2\",\n",
        "    \"asttokens==2.4.1\", \"libcst==1.4.0\", \"networkx==3.3\",\n",
        "    \"rich==13.7.1\", \"pyyaml==6.0.2\",\n",
        "]\n",
        "if sys.version_info < (3,11):\n",
        "    pkgs_core.append(\"tomli\")\n",
        "\n",
        "pkgs_llm = [\n",
        "    \"openai==1.43.0\",\n",
        "    \"httpx==0.27.2\",\n",
        "    \"backoff==2.2.1\",\n",
        "    \"tiktoken==0.7.0\",\n",
        "    \"aiolimiter==1.1.0\",\n",
        "    \"anyio==4.4.0\",\n",
        "    \"nest_asyncio==1.6.0\",\n",
        "    \"python-dotenv==1.0.1\",\n",
        "    \"tqdm==4.66.5\",\n",
        "]\n",
        "\n",
        "pip_install([\"pip\"])\n",
        "pip_install(pkgs_core)\n",
        "pip_install(pkgs_llm)\n",
        "print(\"‚úÖ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÏôÑÎ£å\")\n",
        "\n",
        "# 4) .env ÏÉùÏÑ±\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except:\n",
        "    pip_install([\"python-dotenv>=1.0.1\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "openai_api_key = getpass.getpass(\"Enter OPENAI_API_KEY (ÌïÑÏàò): \").strip()\n",
        "env_path = ROOT / \".env\"\n",
        "env_path.write_text(f\"OPENAI_API_KEY={openai_api_key}\\n\", encoding=\"utf-8\")\n",
        "load_dotenv(env_path)\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "print(\"‚úÖ .env Ï†ÄÏû• Î∞è ÌôòÍ≤Ω Ï£ºÏûÖ ÏôÑÎ£å\")\n",
        "\n",
        "# 5) PYTHONPATH ÏÑ§Ï†ï\n",
        "os.environ[\"PROJECT_PATH\"] = str(PROJ)\n",
        "py_paths = [str(PROJ)]\n",
        "if (PROJ / \"src\").exists():\n",
        "    py_paths.insert(0, str(PROJ / \"src\"))\n",
        "prev_pp = os.environ.get(\"PYTHONPATH\",\"\")\n",
        "os.environ[\"PYTHONPATH\"] = \":\".join(py_paths + ([prev_pp] if prev_pp else []))\n",
        "\n",
        "print(\"‚úÖ PYTHONPATH =\", os.environ[\"PYTHONPATH\"])\n",
        "\n",
        "# 6) AÏïà: coverage source Ï†ÑÏ≤¥(.) Í≥†Ï†ïÌïòÏó¨ .coveragerc ÏÉùÏÑ±\n",
        "coveragerc_text = \"\"\"\n",
        "[run]\n",
        "branch = True\n",
        "source =\n",
        "    .\n",
        "omit =\n",
        "    tests/*\n",
        "    generated_tests/*\n",
        "    run_artifacts/*\n",
        "    htmlcov/*\n",
        "    .venv/*\n",
        "    */site-packages/*\n",
        "\n",
        "[report]\n",
        "show_missing = True\n",
        "skip_covered = True\n",
        "\"\"\"\n",
        "(PROJ / \".coveragerc\").write_text(coveragerc_text.strip() + \"\\n\", encoding=\"utf-8\")\n",
        "os.environ[\"COVERAGE_RCFILE\"] = str(PROJ / \".coveragerc\")\n",
        "\n",
        "print(\"‚úÖ .coveragerc ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(\"  - source = . (ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥)\")\n",
        "print(\"  - omit   = tests/, generated_tests/, run_artifacts/, htmlcov/, .venv, site-packages\")\n",
        "\n",
        "# 7) Í≤∞Í≥º ÎîîÎ†âÌÑ∞Î¶¨ Ï§ÄÎπÑ\n",
        "for d in [\"run_artifacts\", \"htmlcov\", \"reports\"]:\n",
        "    (PROJ / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ ÏôÑÎ£å\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yJGvcId9bzF6",
        "outputId": "0ddf52ac-ae5b-4749-c7b9-dcee5610f511"
      },
      "id": "yJGvcId9bzF6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "ROOT: /content\n",
            ">  git clone --depth=1 https://github.com/temporalio/money-transfer-project-template-python.git\n",
            "‚úÖ Î†àÌè¨ ÌÅ¥Î°†/ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å\n",
            ">  /usr/bin/python3 -m pip install -U pip\n",
            ">  /usr/bin/python3 -m pip install -U pytest==7.4.4 pytest-asyncio==0.23.7 pytest-mock==3.11.1 pytest-cov==4.1.0 coverage[toml]==7.6.1 temporalio==1.7.0 lxml==5.2.2 asttokens==2.4.1 libcst==1.4.0 networkx==3.3 rich==13.7.1 pyyaml==6.0.2\n",
            ">  /usr/bin/python3 -m pip install -U openai==1.43.0 httpx==0.27.2 backoff==2.2.1 tiktoken==0.7.0 aiolimiter==1.1.0 anyio==4.4.0 nest_asyncio==1.6.0 python-dotenv==1.0.1 tqdm==4.66.5\n",
            "‚úÖ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÏôÑÎ£å\n",
            "Enter OPENAI_API_KEY (ÌïÑÏàò): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ .env Ï†ÄÏû• Î∞è ÌôòÍ≤Ω Ï£ºÏûÖ ÏôÑÎ£å\n",
            "‚úÖ PYTHONPATH = /content/money-transfer-project-template-python:/env/python\n",
            "‚úÖ .coveragerc ÏÉùÏÑ± ÏôÑÎ£å\n",
            "  - source = . (ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥)\n",
            "  - omit   = tests/, generated_tests/, run_artifacts/, htmlcov/, .venv, site-packages\n",
            "‚úÖ Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-1) Í∏∞Ï§ÄÏÑ† Ï∏°Ï†ï (Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Î∞ò)\n",
        "\n",
        "import os, sys, json, subprocess, shutil, re\n",
        "from pathlib import Path\n",
        "from lxml import etree\n",
        "\n",
        "# ====== 0) Í≤ΩÎ°ú/ÌôòÍ≤Ω ======\n",
        "assert \"PROJ\" in globals(), \"3-0 Îã®Í≥ÑÍ∞Ä Î®ºÏ†Ä Ïã§ÌñâÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "HTML_DIR = PROJ / \"htmlcov\"\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "\n",
        "def sh(cmd, check=False):\n",
        "    print(\"> \", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True, cwd=str(PROJ))\n",
        "    if check and rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "    return rc\n",
        "\n",
        "def norm(fp):\n",
        "    p = Path(fp)\n",
        "    if not p.is_absolute():\n",
        "        p = (PROJ / p).resolve()\n",
        "    return str(p)\n",
        "\n",
        "def in_proj(abs_path):\n",
        "    return str(PROJ) in str(Path(abs_path).resolve())\n",
        "\n",
        "# ====== 1) baseline ÌÖåÏä§Ìä∏ Ïã§Ìñâ ======\n",
        "print(\"üß™ BASELINE ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë‚Ä¶\")\n",
        "sh(\"coverage erase\" + rc_opt)\n",
        "sh(f\"{sys.executable} -m coverage run{rc_opt} -m pytest -q\")\n",
        "sh(f\"coverage json -o coverage_base.json{rc_opt}\")\n",
        "sh(f\"coverage xml  -o coverage_base.xml{rc_opt}\")\n",
        "sh(\"coverage html\" + rc_opt)\n",
        "\n",
        "# Î≥µÏÇ¨\n",
        "json_path = PROJ / \"coverage_base.json\"\n",
        "xml_path  = PROJ / \"coverage_base.xml\"\n",
        "shutil.copy2(json_path, ART_DIR / \"coverage_base.json\")\n",
        "shutil.copy2(xml_path, ART_DIR / \"coverage_base.xml\")\n",
        "\n",
        "# ====== 2) baseline JSON ÌååÏã± ======\n",
        "cov = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "files = cov.get(\"files\", {}) or {}\n",
        "\n",
        "# Ïã§Ìñâ Í∞ÄÎä• ÎùºÏù∏ Í≥ÑÏÇ∞ = executed + missing\n",
        "def sum_len(key):\n",
        "    return sum(len((files.get(f, {}) or {}).get(key, []) or []) for f in files)\n",
        "\n",
        "base_exec = sum_len(\"executed_lines\")\n",
        "base_miss = sum_len(\"missing_lines\")\n",
        "total_executable_lines = base_exec + base_miss\n",
        "\n",
        "# ====== 3) branch coverage (coverage XML ‚Üí arc Í∏∞Î∞ò) ======\n",
        "full = half = zero = 0\n",
        "observed_outcomes = {}\n",
        "\n",
        "xml_root = etree.parse(str(xml_path)).getroot()\n",
        "for cls in xml_root.findall(\".//class\"):\n",
        "    filename = cls.get(\"filename\")\n",
        "    abs_f = norm(filename)\n",
        "    if not in_proj(abs_f):\n",
        "        continue\n",
        "\n",
        "    for line in cls.findall(\"./lines/line\"):\n",
        "        if line.get(\"branch\") != \"true\":\n",
        "            continue\n",
        "\n",
        "        num = int(line.get(\"number\"))\n",
        "        cond = line.get(\"condition-coverage\")\n",
        "        m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cond) if cond else None\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        covered, total = int(m.group(1)), int(m.group(2))\n",
        "        observed_outcomes.setdefault(abs_f, {})[num] = {\n",
        "            \"covered\": covered,\n",
        "            \"total\": total,\n",
        "            \"ratio\": round(covered / total, 3)\n",
        "        }\n",
        "\n",
        "        if covered == 0:\n",
        "            zero += 1\n",
        "        elif covered == total:\n",
        "            full += 1\n",
        "        else:\n",
        "            half += 1\n",
        "\n",
        "# Ïã§Ìñâ Í∞ÄÎä•Ìïú branch Ïàò\n",
        "branch_points = full + half + zero\n",
        "\n",
        "# ====== 4) uncovered_map ÏÉùÏÑ± ======\n",
        "uncovered_map = {}\n",
        "for f, finfo in files.items():\n",
        "    abs_f = norm(f)\n",
        "    if not in_proj(abs_f):\n",
        "        continue\n",
        "    if any(seg in abs_f for seg in [\"tests/\", \"generated_tests/\", \"run_artifacts/\", \"htmlcov/\"]):\n",
        "        continue\n",
        "\n",
        "    miss = finfo.get(\"missing_lines\", []) or []\n",
        "    if miss:\n",
        "        uncovered_map[abs_f] = sorted(set(miss))\n",
        "\n",
        "(ART_DIR / \"uncovered_map_base.json\").write_text(\n",
        "    json.dumps(uncovered_map, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "(ART_DIR / \"observed_outcomes_base.json\").write_text(\n",
        "    json.dumps(observed_outcomes, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ====== 5) baseline_info Ï†ÄÏû• (Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Ï§Ä) ======\n",
        "baseline_info = {\n",
        "    \"total_executable_lines\": total_executable_lines,\n",
        "    \"total_executable_branches\": branch_points,\n",
        "    \"baseline_executed_lines\": base_exec,\n",
        "    \"baseline_missing_lines\": base_miss,\n",
        "    \"baseline_branch_full\": full,\n",
        "    \"baseline_branch_half\": half,\n",
        "    \"baseline_branch_zero\": zero,\n",
        "}\n",
        "\n",
        "(ART_DIR / \"baseline_info.json\").write_text(\n",
        "    json.dumps(baseline_info, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ====== 6) Ï∂úÎ†• ======\n",
        "print(\"‚úÖ Í∏∞Ï§ÄÏÑ†(3-1) ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(\" - uncovered_map_base.json ÏÉùÏÑ±Îê®\")\n",
        "print(\" - observed_outcomes_base.json ÏÉùÏÑ±Îê®\")\n",
        "print(\" - baseline_info.json ÏÉùÏÑ±Îê®\")\n",
        "\n",
        "print(\"\\nüìò [Baseline Summary]\")\n",
        "print(f\" ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏Ïàò      : {total_executable_lines}\")\n",
        "print(f\" ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú Î∂ÑÍ∏∞ Ïàò     : {branch_points}\")\n",
        "\n",
        "print(\"\\nüìä [Baseline Coverage]\")\n",
        "print(f\" ‚Ä¢ executed={base_exec}, missing={base_miss}\")\n",
        "pct = round((base_exec / (base_exec + base_miss)) * 100, 2) if (base_exec + base_miss) else 0\n",
        "print(f\"   ‚Üí ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : {pct}%\")\n",
        "\n",
        "print(\"\\nüîÄ [Baseline Branch Coverage]\")\n",
        "print(f\" ‚Ä¢ full-hit  : {full}\")\n",
        "print(f\" ‚Ä¢ half-hit  : {half}\")\n",
        "print(f\" ‚Ä¢ zero-hit  : {zero}\")\n",
        "pct_b = round((full / branch_points) * 100, 2) if branch_points else 0\n",
        "print(f\"   ‚Üí Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : {pct_b}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AHp7XI6xd1DG",
        "outputId": "e5bce876-875e-4522-a98d-a8cb731c52f9"
      },
      "id": "AHp7XI6xd1DG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ BASELINE ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë‚Ä¶\n",
            ">  coverage erase --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  /usr/bin/python3 -m coverage run --rcfile /content/money-transfer-project-template-python/.coveragerc -m pytest -q\n",
            ">  coverage json -o coverage_base.json --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  coverage xml  -o coverage_base.xml --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            ">  coverage html --rcfile /content/money-transfer-project-template-python/.coveragerc\n",
            "‚úÖ Í∏∞Ï§ÄÏÑ†(3-1) ÏÉùÏÑ± ÏôÑÎ£å\n",
            " - uncovered_map_base.json ÏÉùÏÑ±Îê®\n",
            " - observed_outcomes_base.json ÏÉùÏÑ±Îê®\n",
            " - baseline_info.json ÏÉùÏÑ±Îê®\n",
            "\n",
            "üìò [Baseline Summary]\n",
            " ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÎùºÏù∏Ïàò      : 156\n",
            " ‚Ä¢ Ï†ÑÏ≤¥ Ïã§Ìñâ Í∞ÄÎä•Ìïú Î∂ÑÍ∏∞ Ïàò     : 20\n",
            "\n",
            "üìä [Baseline Coverage]\n",
            " ‚Ä¢ executed=102, missing=54\n",
            "   ‚Üí ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : 65.38%\n",
            "\n",
            "üîÄ [Baseline Branch Coverage]\n",
            " ‚Ä¢ full-hit  : 16\n",
            " ‚Ä¢ half-hit  : 0\n",
            " ‚Ä¢ zero-hit  : 4\n",
            "   ‚Üí Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ ÎπÑÏú® : 80.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2a) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌîÑÎ£®Îãù ‚Äì ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ (Í∏∞Ï§ÄÏÑ† ÎåÄÎπÑ Í≥†Ïú† Œîcoverage + ÌÉêÏöïÏ†Å ÏÑ†ÌÉù; ÏàòÏßëÌïÑÌÑ∞/Ìè¥Î∞± Í∞ïÌôî)\n",
        "import subprocess, json, os, shutil, re, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- 0. Í≤ΩÎ°ú/Ï†ÑÏ†ú ----------\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌï¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "SRC_TESTS = PROJ / \"tests\"\n",
        "DST_PRUNED = PROJ / \"Pruned_Base_Tests\"\n",
        "DST_PRUNED.mkdir(exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "assert RCFILE.exists(), \".coveragercÍ∞Ä ÏóÜÏäµÎãàÎã§. 3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "rc_opt = f\"--rcfile={RCFILE}\"\n",
        "\n",
        "BASE_JSON = ART_DIR / \"coverage_base.json\"\n",
        "assert BASE_JSON.exists(), \"Í∏∞Ï§ÄÏÑ† coverage_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "# ---------- 1. Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ ÏßëÌï© ----------\n",
        "def load_executed_lines_from_json(json_path: Path) -> set[str]:\n",
        "    if not json_path.exists():\n",
        "        return set()\n",
        "    data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "    lines = set()\n",
        "    for f, finfo in (data.get(\"files\", {}) or {}).items():\n",
        "        for ln in (finfo.get(\"executed_lines\", []) or []):\n",
        "            lines.add(f\"{f}:{ln}\")\n",
        "    return lines\n",
        "\n",
        "baseline_lines = load_executed_lines_from_json(BASE_JSON)\n",
        "print(f\"üìä Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ Ïàò: {len(baseline_lines)}\")\n",
        "\n",
        "# ---------- 2. ÌÖåÏä§Ìä∏ Î™©Î°ù ÏàòÏßë (nodeid) ----------\n",
        "print(\"üìã pytest ÌÖåÏä§Ìä∏ ÏàòÏßë Ï§ë (--collect-only)...\")\n",
        "collect_out = subprocess.check_output(\n",
        "    [sys.executable, \"-m\", \"pytest\", \"--collect-only\", \"-q\"],\n",
        "    cwd=PROJ,\n",
        "    stderr=subprocess.STDOUT,\n",
        ").decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# nodeidÎ°ú Í∞ÑÏ£º: \"path::...\" Ìå®ÌÑ¥Îßå ÌóàÏö© (ÏïàÎÇ¥Î¨∏/Ï¥ùÍ≥Ñ ÎùºÏù∏ ÌïÑÌÑ∞)\n",
        "nodeid_pat = re.compile(r\".+::.+\")\n",
        "test_items = [ln.strip() for ln in collect_out.splitlines()\n",
        "              if nodeid_pat.match(ln.strip())]\n",
        "print(f\"Ï¥ù ÏàòÏßëÎêú ÌÖåÏä§Ìä∏ Ïàò: {len(test_items)}\")\n",
        "\n",
        "if not test_items:\n",
        "    print(\"‚ÑπÔ∏è ÏàòÏßëÎêú ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§. ÌîÑÎ£®ÎãùÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
        "    (ART_DIR / \"pruning_summary.json\").write_text(\n",
        "        json.dumps({\n",
        "            \"total_tests\": 0, \"retained\": 0, \"removed\": 0,\n",
        "            \"retained_tests\": [], \"removed_tests\": [],\n",
        "        }, indent=2, ensure_ascii=False),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "else:\n",
        "    # ---------- 3. Í∞úÎ≥Ñ ÌÖåÏä§Ìä∏ Ïã§Ìñâ ‚Üí Ïã§Ìñâ ÎùºÏù∏ ÏßëÌï© ----------\n",
        "    def get_executed_lines_tmp() -> set[str]:\n",
        "        tmp_json = PROJ / \"coverage_tmp.json\"\n",
        "        return load_executed_lines_from_json(tmp_json)\n",
        "\n",
        "    def run_test_item(item: str) -> tuple[bool, set[str]]:\n",
        "        subprocess.run(f\"coverage erase {rc_opt}\", shell=True, cwd=PROJ)\n",
        "        rc = subprocess.call(\n",
        "            f\"{sys.executable} -m coverage run {rc_opt} -m pytest -q {item}\",\n",
        "            shell=True, cwd=PROJ,\n",
        "        )\n",
        "        subprocess.run(f\"coverage json -o coverage_tmp.json {rc_opt}\", shell=True, cwd=PROJ)\n",
        "        return (rc == 0), get_executed_lines_tmp()\n",
        "\n",
        "    per_test_exec: list[tuple[str, bool, set[str]]] = []\n",
        "    for i, item in enumerate(test_items, 1):\n",
        "        print(f\"[{i}/{len(test_items)}] ‚ñ∂ Ïã§Ìñâ Ï§ë: {item}\")\n",
        "        ok, exec_set = run_test_item(item)\n",
        "        per_test_exec.append((item, ok, exec_set))\n",
        "        print(f\"   ‚Üí {'‚úÖPASS' if ok else '‚ùåFAIL'}, executed_lines={len(exec_set)}\")\n",
        "\n",
        "    # ---------- 4. ÌÉêÏöïÏ†Å ÏÑ†ÌÉù (set-cover Ïú†ÏÇ¨) ----------\n",
        "    covered_so_far = set(baseline_lines)\n",
        "    candidates = [(it, s) for (it, ok, s) in per_test_exec if ok]\n",
        "\n",
        "    THRESHOLD_MIN_GAIN = 1  # Í≥†Ïú† Í∏∞Ïó¨ ÎùºÏù∏ ‚â• 1\n",
        "    selected: list[str] = []\n",
        "\n",
        "    while True:\n",
        "        best = None\n",
        "        best_gain_val = 0\n",
        "        for it, s in candidates:\n",
        "            gain_val = len(s - covered_so_far)\n",
        "            if gain_val > best_gain_val:\n",
        "                best_gain_val = gain_val\n",
        "                best = (it, s)\n",
        "        if not best or best_gain_val < THRESHOLD_MIN_GAIN:\n",
        "            break\n",
        "        it, s = best\n",
        "        selected.append(it)\n",
        "        covered_so_far |= s\n",
        "        candidates = [(iit, ss) for (iit, ss) in candidates if iit != it]\n",
        "\n",
        "    # ---- Ìè¥Î∞±: Î™®Îëê 0Ïù¥Î©¥ PASS Ï§ëÏóêÏÑú ÏµúÎåÄ executed_lines 1Í∞ú Ïú†ÏßÄ\n",
        "    if not selected:\n",
        "        passables = [(it, s) for (it, ok, s) in per_test_exec if ok]\n",
        "        if passables:\n",
        "            it, s = max(passables, key=lambda x: len(x[1]))\n",
        "            selected = [it]\n",
        "            covered_so_far |= s\n",
        "            print(f\"‚ÑπÔ∏è Ìè¥Î∞± Ï†ÅÏö©: '{it}' 1Í∞ú Ïú†ÏßÄ (Ïã§Ìñâ ÎùºÏù∏ {len(s)}).\")\n",
        "\n",
        "    # ---------- 5. Í≤∞Í≥º ÏöîÏïΩ ----------\n",
        "    selected_set = set(selected)\n",
        "    retained = [{\"name\": it, \"success\": True} for it in selected]\n",
        "    removed = [{\"name\": it, \"success\": bool(ok)}\n",
        "               for (it, ok, _s) in per_test_exec if it not in selected_set]\n",
        "\n",
        "    summary = {\n",
        "        \"total_tests\": len(test_items),\n",
        "        \"retained\": len(retained),\n",
        "        \"removed\": len(removed),\n",
        "        \"retained_tests\": retained,\n",
        "        \"removed_tests\": removed,\n",
        "        \"baseline_lines\": len(baseline_lines),\n",
        "        \"final_covered_lines\": len(covered_so_far),\n",
        "        \"incremental_gain_lines\": len(covered_so_far - baseline_lines),\n",
        "        \"threshold_min_gain\": THRESHOLD_MIN_GAIN,\n",
        "    }\n",
        "    (ART_DIR / \"pruning_summary.json\").write_text(\n",
        "        json.dumps(summary, indent=2, ensure_ascii=False),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    # ---------- 6. Ïú†ÏßÄÎêú ÌÖåÏä§Ìä∏Îßå Î≥µÏÇ¨ ----------\n",
        "    retained_files = set()\n",
        "    for nodeid in selected:\n",
        "        file_part = nodeid.split(\"::\", 1)[0]\n",
        "        path_obj = (PROJ / file_part).resolve()\n",
        "        if path_obj.exists():\n",
        "            retained_files.add(path_obj)\n",
        "\n",
        "    if SRC_TESTS.exists():\n",
        "        copied = 0\n",
        "        for test_file in sorted(SRC_TESTS.rglob(\"test_*.py\")):\n",
        "            if test_file.resolve() in retained_files:\n",
        "                rel = test_file.relative_to(SRC_TESTS)\n",
        "                dst = DST_PRUNED / rel\n",
        "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(test_file, dst)\n",
        "                copied += 1\n",
        "        print(f\"üì¶ Ïú†ÏßÄ ÌååÏùº Î≥µÏÇ¨ ÏôÑÎ£å: {copied}Í∞ú ÌååÏùº ‚Üí {DST_PRUNED}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è tests/ Ìè¥ÎçîÍ∞Ä ÏóÜÏñ¥ ÌååÏùº Î≥µÏÇ¨Îäî Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
        "\n",
        "    (ART_DIR / \"retained_nodeids.txt\").write_text(\"\\n\".join(selected), encoding=\"utf-8\")\n",
        "\n",
        "    print(\"‚úÖ ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ ÌîÑÎ£®Îãù ÏôÑÎ£å\")\n",
        "    print(\" - pruning_summary.json  :\", ART_DIR / \"pruning_summary.json\")\n",
        "    print(\" - retained_nodeids.txt  :\", ART_DIR / \"retained_nodeids.txt\")\n",
        "    print(\" - Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨(ÏÑ†ÌÉù ÌååÏùº):\", DST_PRUNED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qCyuGBQWExjy",
        "outputId": "f3db4847-d967-45ad-86aa-5a0bb6785f67"
      },
      "id": "qCyuGBQWExjy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Í∏∞Ï§ÄÏÑ† ÎùºÏù∏ Ïàò: 102\n",
            "üìã pytest ÌÖåÏä§Ìä∏ ÏàòÏßë Ï§ë (--collect-only)...\n",
            "Ï¥ù ÏàòÏßëÎêú ÌÖåÏä§Ìä∏ Ïàò: 3\n",
            "[1/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer\n",
            "   ‚Üí ‚úÖPASS, executed_lines=91\n",
            "[2/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer_withdraw_insufficient_funds\n",
            "   ‚Üí ‚úÖPASS, executed_lines=83\n",
            "[3/3] ‚ñ∂ Ïã§Ìñâ Ï§ë: tests/test_run_worker.py::test_money_transfer_withdraw_invalid_account\n",
            "   ‚Üí ‚úÖPASS, executed_lines=79\n",
            "‚ÑπÔ∏è Ìè¥Î∞± Ï†ÅÏö©: 'tests/test_run_worker.py::test_money_transfer' 1Í∞ú Ïú†ÏßÄ (Ïã§Ìñâ ÎùºÏù∏ 91).\n",
            "üì¶ Ïú†ÏßÄ ÌååÏùº Î≥µÏÇ¨ ÏôÑÎ£å: 1Í∞ú ÌååÏùº ‚Üí /content/money-transfer-project-template-python/Pruned_Base_Tests\n",
            "‚úÖ ÌÖåÏä§Ìä∏ Ìï®Ïàò Îã®ÏúÑ ÌîÑÎ£®Îãù ÏôÑÎ£å\n",
            " - pruning_summary.json  : /content/money-transfer-project-template-python/run_artifacts/run1/pruning_summary.json\n",
            " - retained_nodeids.txt  : /content/money-transfer-project-template-python/run_artifacts/run1/retained_nodeids.txt\n",
            " - Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨(ÏÑ†ÌÉù ÌååÏùº): /content/money-transfer-project-template-python/Pruned_Base_Tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2b) Î≥µÌï© Î™©Ìëú ÏÉùÏÑ±\n",
        "import ast\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set, Optional\n",
        "\n",
        "# ---------- Í≤ΩÎ°ú ----------\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR    = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "UNCOVERED_JSON = ART_DIR / \"uncovered_map_base.json\"\n",
        "OBSERVED_JSON  = ART_DIR / \"observed_outcomes_base.json\"\n",
        "\n",
        "assert UNCOVERED_JSON.exists(),  \"uncovered_map_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "assert OBSERVED_JSON.exists(),   \"observed_outcomes_base.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-1ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "# ===================== Ïú†Ìã∏ =====================\n",
        "def norm_abs(p: str) -> str:\n",
        "    q = Path(p)\n",
        "    if not q.is_absolute():\n",
        "        q = (PROJ_PATH / q).resolve()\n",
        "    return str(q.resolve())\n",
        "\n",
        "def rel_from_proj(abs_path: str) -> str:\n",
        "    try:\n",
        "        return str(Path(abs_path).resolve().relative_to(PROJ_PATH))\n",
        "    except Exception:\n",
        "        return abs_path\n",
        "\n",
        "def is_source(abs_path: str) -> bool:\n",
        "    try:\n",
        "        rel = Path(abs_path).resolve().relative_to(PROJ_PATH)\n",
        "    except Exception:\n",
        "        return False\n",
        "    s = str(rel).replace(\"\\\\\", \"/\")\n",
        "    if not s.endswith(\".py\"):\n",
        "        return False\n",
        "    if s.startswith((\"generated_tests/\", \"tests/\", \".venv/\", \"venv/\", \"run_artifacts/\", \"htmlcov/\")):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def try_unparse(node: ast.AST) -> Optional[str]:\n",
        "    try:\n",
        "        if hasattr(ast, \"unparse\"):\n",
        "            return ast.unparse(node)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ===================== baseline ÏûÖÎ†• Î°úÎìú =====================\n",
        "raw_uncovered = json.loads(UNCOVERED_JSON.read_text(encoding=\"utf-8\"))\n",
        "uncovered_map: Dict[str, List[int]] = {\n",
        "    norm_abs(k): sorted(set(v))\n",
        "    for k, v in raw_uncovered.items()\n",
        "    if is_source(norm_abs(k))\n",
        "}\n",
        "\n",
        "raw_observed = json.loads(OBSERVED_JSON.read_text(encoding=\"utf-8\"))\n",
        "observed_outcomes: Dict[str, Dict[int, Dict[str, float]]] = {}\n",
        "\n",
        "for k, mp in raw_observed.items():\n",
        "    abs_k = norm_abs(k)\n",
        "    if not is_source(abs_k):\n",
        "        continue\n",
        "    fixed = {}\n",
        "    for ln_str, meta in mp.items():\n",
        "        try:\n",
        "            fixed[int(ln_str)] = meta\n",
        "        except:\n",
        "            continue\n",
        "    observed_outcomes[abs_k] = fixed\n",
        "\n",
        "# half-hit\n",
        "half_hit_map: Dict[str, Set[int]] = {}\n",
        "for fp, mapping in observed_outcomes.items():\n",
        "    half = {ln for ln, meta in mapping.items()\n",
        "            if 0 < int(meta.get(\"covered\", 0)) < int(meta.get(\"total\", 0))}\n",
        "    if half:\n",
        "        half_hit_map[fp] = half\n",
        "\n",
        "\n",
        "# ===================== AST Ïä§Ï∫î =====================\n",
        "MOD_REQS = {\"requests\"}\n",
        "MOD_HTTPX = {\"httpx\"}\n",
        "MOD_TEMPORAL = {\"temporalio\"}\n",
        "MOD_SUBPROCESS = {\"subprocess\"}\n",
        "TEMPORAL_PREFIXES = (\"temporalio.client.\", \"temporalio.worker.\", \"temporalio.workflow.\")\n",
        "\n",
        "class FunctionInfo:\n",
        "    def __init__(self, name, lineno):\n",
        "        self.name = name or \"<module>\"\n",
        "        self.lineno = lineno\n",
        "        self.branches: List[int] = []\n",
        "        self.defs: Dict[str, List[int]] = {}\n",
        "        self.uses: Dict[str, List[int]] = {}\n",
        "        self.exceptions: List[Tuple[str, int, dict]] = []\n",
        "        self.side_effect_calls: List[Tuple[str, int]] = []\n",
        "\n",
        "    def add_def(self, v, ln): self.defs.setdefault(v, []).append(ln)\n",
        "    def add_use(self, v, ln): self.uses.setdefault(v, []).append(ln)\n",
        "\n",
        "\n",
        "class ASTVisitor(ast.NodeVisitor):\n",
        "    def __init__(self, file_rel):\n",
        "        self.file_rel = file_rel\n",
        "        self.stack: List[FunctionInfo] = []\n",
        "        self.funcs: List[FunctionInfo] = []\n",
        "        self.alias_to_module = {}\n",
        "        self.symbol_to_module = {}\n",
        "\n",
        "    def current(self):\n",
        "        if not self.stack:\n",
        "            if not self.funcs or self.funcs[0].name != \"<module>\":\n",
        "                fi = FunctionInfo(\"<module>\", 1)\n",
        "                self.funcs.insert(0, fi)\n",
        "            return self.funcs[0]\n",
        "        return self.stack[-1]\n",
        "\n",
        "    # imports\n",
        "    def visit_Import(self, node):\n",
        "        for alias in node.names:\n",
        "            mod = alias.name\n",
        "            asname = alias.asname or mod.split(\".\")[0]\n",
        "            self.alias_to_module[asname] = mod\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_ImportFrom(self, node):\n",
        "        mod = node.module or \"\"\n",
        "        for alias in node.names:\n",
        "            self.symbol_to_module[alias.asname or alias.name] = mod\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    # function defs\n",
        "    def visit_FunctionDef(self, node):\n",
        "        fi = FunctionInfo(node.name, node.lineno)\n",
        "        self.stack.append(fi); self.generic_visit(node); self.stack.pop()\n",
        "        self.funcs.append(fi)\n",
        "\n",
        "    def visit_AsyncFunctionDef(self, node):\n",
        "        self.visit_FunctionDef(node)\n",
        "\n",
        "    # branches\n",
        "    def visit_If(self, node):        self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_While(self, node):     self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_For(self, node):       self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_AsyncFor(self, node):  self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "    def visit_With(self, node):      self.current().branches.append(node.lineno); self.generic_visit(node)\n",
        "\n",
        "    def visit_Try(self, node):\n",
        "        self.current().branches.append(node.lineno)\n",
        "        self.current().exceptions.append((\"try\", node.lineno, {}))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_ExceptHandler(self, node):\n",
        "        hint = {}\n",
        "        if node.type is not None:\n",
        "            typ = try_unparse(node.type)\n",
        "            if not typ and hasattr(node.type, \"id\"):\n",
        "                typ = node.type.id\n",
        "            if typ: hint[\"exception_type\"] = typ\n",
        "        self.current().branches.append(node.lineno)\n",
        "        self.current().exceptions.append((\"except\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Raise(self, node):\n",
        "        hint = {}\n",
        "        if node.exc is not None:\n",
        "            text = try_unparse(node.exc)\n",
        "            if text: hint[\"expr\"] = text\n",
        "            if isinstance(node.exc, ast.Call):\n",
        "                fn = node.exc.func\n",
        "                if isinstance(fn, ast.Name):\n",
        "                    hint[\"exception_type\"] = fn.id\n",
        "                elif isinstance(fn, ast.Attribute):\n",
        "                    hint[\"exception_type\"] = fn.attr\n",
        "                if node.exc.args:\n",
        "                    a0 = node.exc.args[0]\n",
        "                    if isinstance(a0, ast.Constant) and isinstance(a0.value, str):\n",
        "                        hint[\"message_contains\"] = a0.value[:80]\n",
        "        self.current().exceptions.append((\"raise\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Assert(self, node):\n",
        "        hint = {}\n",
        "        if node.msg and isinstance(node.msg, ast.Constant) and isinstance(node.msg.value, str):\n",
        "            hint[\"message_contains\"] = node.msg.value[:80]\n",
        "        self.current().exceptions.append((\"assert\", node.lineno, hint))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Name(self, node):\n",
        "        if isinstance(node.ctx, ast.Store):\n",
        "            self.current().add_def(node.id, node.lineno)\n",
        "        elif isinstance(node.ctx, ast.Load):\n",
        "            self.current().add_use(node.id, node.lineno)\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    # dotted name\n",
        "    def _dotted_name(self, node):\n",
        "        if isinstance(node, ast.Name):\n",
        "            nm = node.id\n",
        "            if nm in self.alias_to_module:\n",
        "                return self.alias_to_module[nm]\n",
        "            if nm in self.symbol_to_module:\n",
        "                return f\"{self.symbol_to_module[nm]}.{nm}\"\n",
        "            return nm\n",
        "        if isinstance(node, ast.Attribute):\n",
        "            base = self._dotted_name(node.value)\n",
        "            if base: return f\"{base}.{node.attr}\"\n",
        "        return None\n",
        "\n",
        "    # calls\n",
        "    def visit_Call(self, node):\n",
        "        qn = self._dotted_name(node.func) or \"\"\n",
        "        if qn in (\"open\", \"builtins.open\"):\n",
        "            self.current().side_effect_calls.append((\"io_open\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_REQS):\n",
        "            self.current().side_effect_calls.append((\"net_requests\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_HTTPX):\n",
        "            self.current().side_effect_calls.append((\"net_httpx\", node.lineno))\n",
        "        if any(qn.startswith(pref) for pref in TEMPORAL_PREFIXES) \\\n",
        "           or any(qn.startswith(m + \".\") for m in MOD_TEMPORAL):\n",
        "            self.current().side_effect_calls.append((\"temporal\", node.lineno))\n",
        "        if any(qn.startswith(m + \".\") for m in MOD_SUBPROCESS):\n",
        "            self.current().side_effect_calls.append((\"subprocess\", node.lineno))\n",
        "        self.generic_visit(node)\n",
        "\n",
        "\n",
        "# ===================== ÎåÄÏÉÅ ÌååÏùº =====================\n",
        "candidate_files = sorted(set([*uncovered_map.keys(), *observed_outcomes.keys()]))\n",
        "candidate_files = [fp for fp in candidate_files if is_source(fp)]\n",
        "\n",
        "\n",
        "# ---------------- Í∞ÄÏ§ëÏπò/ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ----------------\n",
        "ALPHA_BRANCH = 0.45\n",
        "BETA_DU      = 0.25\n",
        "GAMMA_EXC    = 0.30\n",
        "LAMBDA_1_CONST   = 0.20\n",
        "LAMBDA_2_CONTEXT = 0.60\n",
        "LAMBDA_3_TARGETS = 0.20\n",
        "TOP_K = 8\n",
        "OVERLAP_THETA = 0.50\n",
        "\n",
        "assert abs((ALPHA_BRANCH + BETA_DU + GAMMA_EXC) - 1.0) < 1e-6\n",
        "assert abs((LAMBDA_1_CONST + LAMBDA_2_CONTEXT + LAMBDA_3_TARGETS) - 1.0) < 1e-6\n",
        "\n",
        "\n",
        "def coverage_gain_structural(b_cnt, du_cnt, exc_cnt):\n",
        "    return ALPHA_BRANCH*b_cnt + BETA_DU*du_cnt + GAMMA_EXC*exc_cnt\n",
        "\n",
        "def coverage_gain_total(target_lines, b_cnt, du_cnt, exc_cnt):\n",
        "    return len(target_lines) + coverage_gain_structural(b_cnt, du_cnt, exc_cnt)\n",
        "\n",
        "def generation_cost(context_size, target_count):\n",
        "    return (LAMBDA_1_CONST*1.0) + (LAMBDA_2_CONTEXT*context_size) + (LAMBDA_3_TARGETS*target_count)\n",
        "\n",
        "\n",
        "# ===================== AST ‚Üí file_infos =====================\n",
        "file_infos = {}\n",
        "MAX_NEAR_GAP = 8\n",
        "\n",
        "for file_abs in candidate_files:\n",
        "    try:\n",
        "        source = Path(file_abs).read_text(encoding=\"utf-8\")\n",
        "        tree = ast.parse(source)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è AST parse Ïã§Ìå®: {file_abs}, {e}\")\n",
        "        continue\n",
        "\n",
        "    rel = rel_from_proj(file_abs)\n",
        "    visitor = ASTVisitor(rel)\n",
        "    visitor.visit(tree)\n",
        "\n",
        "    miss = set(uncovered_map.get(file_abs, []))\n",
        "    half = set(half_hit_map.get(file_abs, []))\n",
        "\n",
        "    for fi in visitor.funcs:\n",
        "        tb = [ln for ln in fi.branches if (ln in miss or ln in half)]\n",
        "        tu_pairs = []\n",
        "        for var, defs in fi.defs.items():\n",
        "            uses = sorted(fi.uses.get(var, []))\n",
        "            defs_sorted = sorted(defs)\n",
        "            for u in uses:\n",
        "                if u not in miss:\n",
        "                    continue\n",
        "                d_le = [d for d in defs_sorted if d <= u]\n",
        "                if not d_le:\n",
        "                    continue\n",
        "                d = max(d_le)\n",
        "                tu_pairs.append((var, d, u))\n",
        "\n",
        "        te = [(kind, line, hint)\n",
        "              for (kind, line, hint) in fi.exceptions if line in miss]\n",
        "\n",
        "        ctx = len({ln for ls in fi.defs.values() for ln in ls} |\n",
        "                  {ln for ls in fi.uses.values() for ln in ls})\n",
        "\n",
        "        if tb or tu_pairs or te:\n",
        "            file_infos[(rel, fi.name)] = {\n",
        "                \"branches\": sorted(tb),\n",
        "                \"def_uses\": sorted(tu_pairs, key=lambda x: (x[2], x[1], x[0])),\n",
        "                \"exceptions\": sorted(te, key=lambda x: (x[1], x[0])),\n",
        "                \"func_lineno\": fi.lineno,\n",
        "                \"context_size\": ctx,\n",
        "            }\n",
        "\n",
        "\n",
        "# ===================== ÌõÑÎ≥¥ goal ÏÉùÏÑ± =====================\n",
        "def make_goal(file_rel, func, func_lineno, branches, def_uses, exceptions, context_size):\n",
        "    target_lines = set(branches)\n",
        "    for _, d, u in def_uses: target_lines.update([d, u])\n",
        "    for _, line, _ in exceptions: target_lines.add(line)\n",
        "\n",
        "    b_cnt = len(branches)\n",
        "    du_cnt = len(def_uses)\n",
        "    exc_cnt = len(exceptions)\n",
        "\n",
        "    gain = coverage_gain_total(target_lines, b_cnt, du_cnt, exc_cnt)\n",
        "    cost = generation_cost(context_size, len(target_lines))\n",
        "\n",
        "    return {\n",
        "        \"file\": file_rel,\n",
        "        \"function\": {\"name\": func, \"lineno\": func_lineno},\n",
        "        \"components\": {\n",
        "            \"branches\": [{\"line\": b} for b in branches],\n",
        "            \"def_uses\": [{\"var\": v, \"def_line\": d, \"use_line\": u} for (v, d, u) in def_uses],\n",
        "            \"exceptions\": [{\"kind\": k, \"line\": ln, **hint} for (k, ln, hint) in exceptions],\n",
        "        },\n",
        "        \"target_lines\": sorted(target_lines),\n",
        "        \"coverage_gain\": float(gain),\n",
        "        \"generation_cost\": float(cost),\n",
        "        \"context_size\": context_size,\n",
        "    }\n",
        "\n",
        "\n",
        "candidates = []\n",
        "\n",
        "for (file_rel, func), info in file_infos.items():\n",
        "    tb = info[\"branches\"]\n",
        "    tu = info[\"def_uses\"]\n",
        "    te = info[\"exceptions\"]\n",
        "    ctx = info[\"context_size\"]\n",
        "    func_lineno = info[\"func_lineno\"]\n",
        "\n",
        "    # Îã®Ïùº ÏöîÏÜå\n",
        "    for b in tb:              candidates.append(make_goal(file_rel, func, func_lineno, [b], [], [], ctx))\n",
        "    for (v, d, u) in tu:      candidates.append(make_goal(file_rel, func, func_lineno, [], [(v, d, u)], [], ctx))\n",
        "    for (k, ln, h) in te:     candidates.append(make_goal(file_rel, func, func_lineno, [], [], [(k, ln, h)], ctx))\n",
        "\n",
        "    # Í∑ºÏ†ë Ï°∞Ìï©\n",
        "    for b in tb:\n",
        "        for (v, d, u) in tu:\n",
        "            if max(b, u) - min(b, d) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [b], [(v, d, u)], [], ctx))\n",
        "\n",
        "    for b in tb:\n",
        "        for (k, ln, h) in te:\n",
        "            if abs(b - ln) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [b], [], [(k, ln, h)], ctx))\n",
        "\n",
        "    for (v, d, u) in tu:\n",
        "        for (k, ln, h) in te:\n",
        "            if max(u, ln) - min(d, ln) <= MAX_NEAR_GAP:\n",
        "                candidates.append(make_goal(file_rel, func, func_lineno, [], [(v, d, u)], [(k, ln, h)], ctx))\n",
        "\n",
        "    for b in tb:\n",
        "        for (v, d, u) in tu:\n",
        "            for (k, ln, h) in te:\n",
        "                lines = [b, d, u, ln]\n",
        "                if max(lines) - min(lines) <= MAX_NEAR_GAP:\n",
        "                    candidates.append(make_goal(file_rel, func, func_lineno,\n",
        "                                                [b], [(v, d, u)], [(k, ln, h)], ctx))\n",
        "\n",
        "\n",
        "# ===================== Algorithm 1 (19Ï§Ñ) Íµ¨ÌòÑ =====================\n",
        "\n",
        "def overlap_basic(g, h):\n",
        "    tg = set(g[\"target_lines\"])\n",
        "    th = set(h[\"target_lines\"])\n",
        "    if not tg:\n",
        "        return 0.0\n",
        "    return len(tg & th) / len(tg)\n",
        "\n",
        "# goc Í≥ÑÏÇ∞\n",
        "for g in candidates:\n",
        "    gain = g[\"coverage_gain\"]\n",
        "    cost = g[\"generation_cost\"]\n",
        "    g[\"goc\"] = gain / cost if cost > 0 else 0.0\n",
        "\n",
        "# ---- filtering (line 12~16) ----\n",
        "filtered = []\n",
        "for g in candidates:\n",
        "    if g[\"goc\"] <= 0:\n",
        "        continue\n",
        "    bad = False\n",
        "    for h in filtered:\n",
        "        if overlap_basic(g, h) > OVERLAP_THETA and h[\"goc\"] >= g[\"goc\"]:\n",
        "            bad = True\n",
        "            break\n",
        "    if not bad:\n",
        "        filtered.append(g)\n",
        "\n",
        "# ---- Ï†ïÎ†¨ (line 17) ----\n",
        "filtered_sorted = sorted(filtered, key=lambda x: x[\"goc\"], reverse=True)\n",
        "\n",
        "# ---- top-k ÏÑ†ÌÉù (line 18) ----\n",
        "selected = filtered_sorted[:TOP_K]\n",
        "\n",
        "# ---- Ï†ïÏ†ú Î∞è Ï†ÄÏû• ----\n",
        "for i, g in enumerate(selected, 1):\n",
        "    g[\"id\"] = f\"{i:04d}\"\n",
        "    g[\"coverage_gain\"] = round(g[\"coverage_gain\"], 6)\n",
        "    g[\"generation_cost\"] = round(g[\"generation_cost\"], 6)\n",
        "    g[\"score\"] = round(g[\"goc\"], 6)\n",
        "\n",
        "(ART_DIR / \"goals_raw.json\").write_text(json.dumps(selected, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "ranked = [\n",
        "    {\n",
        "        \"id\": g[\"id\"],\n",
        "        \"file\": g[\"file\"],\n",
        "        \"function\": g[\"function\"],\n",
        "        \"components\": g[\"components\"],\n",
        "        \"target_lines\": g[\"target_lines\"],\n",
        "        \"coverage_gain\": g[\"coverage_gain\"],\n",
        "        \"generation_cost\": g[\"generation_cost\"],\n",
        "        \"score\": g[\"score\"],\n",
        "        \"context_size\": g[\"context_size\"],\n",
        "    }\n",
        "    for g in selected\n",
        "]\n",
        "\n",
        "(ART_DIR / \"goals_ranked.json\").write_text(json.dumps(ranked, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"‚úÖ Î≥µÌï© Î™©Ìëú ÏÉùÏÑ± ÏôÑÎ£å (Algorithm1, top-k={TOP_K})\")\n",
        "print(f\" - ÏõêÎ≥∏ ÌõÑÎ≥¥ Ïàò: {len(candidates)}\")\n",
        "print(f\" - filtered ÌõÑÎ≥¥ Ïàò: {len(filtered)}\")\n",
        "print(f\" - ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú Î™©Ìëú Ïàò: {len(selected)}\")\n",
        "print(\" - Ï†ÄÏû•: goals_raw.json, goals_ranked.json\")\n"
      ],
      "metadata": {
        "id": "q3TIL3otgCzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd750636-c110-4c64-866b-ac222ed0c6cf"
      },
      "id": "q3TIL3otgCzm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Î≥µÌï© Î™©Ìëú ÏÉùÏÑ± ÏôÑÎ£å (Algorithm1, top-k=8)\n",
            " - ÏõêÎ≥∏ ÌõÑÎ≥¥ Ïàò: 94\n",
            " - filtered ÌõÑÎ≥¥ Ïàò: 50\n",
            " - ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú Î™©Ìëú Ïàò: 8\n",
            " - Ï†ÄÏû•: goals_raw.json, goals_ranked.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2c) ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Î∞ò Í≤ΩÎüâ ÏûÖÎ†• Ï†úÏïΩ Ï∂îÏ∂úÍ∏∞ (Î≥µÌï©Î™©Ìëú Ï†úÍ±∞ Î≤ÑÏ†Ñ)\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "\n",
        "UNCOVERED = ART_DIR / \"uncovered_map_base.json\"\n",
        "BRANCHES = ART_DIR / \"observed_outcomes_base.json\"\n",
        "\n",
        "assert UNCOVERED.exists(), \"uncovered_map_base.json ÏóÜÏùå. 3-1 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "assert BRANCHES.exists(), \"observed_outcomes_base.json ÏóÜÏùå. 3-1 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "out_file = ART_DIR / \"goals_minimal.json\"\n",
        "\n",
        "# ------------------------------\n",
        "# ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏ + ÎØ∏Ïª§Î≤Ñ Î∏åÎûúÏπò Í∏∞Î∞ò Î™©Ìëú ÏÉùÏÑ±\n",
        "# ------------------------------\n",
        "\n",
        "uncovered = json.loads(UNCOVERED.read_text(encoding=\"utf-8\"))\n",
        "branch_info = json.loads(BRANCHES.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "goals_out = []\n",
        "\n",
        "for file_path, miss_lines in uncovered.items():\n",
        "\n",
        "    file_item = {\n",
        "        \"file\": file_path,\n",
        "        \"target_lines\": miss_lines,\n",
        "        \"branch_targets\": [],\n",
        "        \"input_constraints\": []   # ÏµúÏÜå Ï†úÏïΩ Íµ¨ÏÑ±\n",
        "    }\n",
        "\n",
        "    # ----------- ÎØ∏Ïª§Î≤Ñ Î∏åÎûúÏπò Ï†ïÎ≥¥ Ï∂îÍ∞Ä -----------\n",
        "    if file_path in branch_info:\n",
        "        for ln, info in branch_info[file_path].items():\n",
        "            if info[\"covered\"] == 0:  # Î™ÖÌôïÌïú ÎØ∏Ïª§Î≤Ñ Î∏åÎûúÏπò\n",
        "                file_item[\"branch_targets\"].append({\n",
        "                    \"line\": ln,\n",
        "                    \"total_outcomes\": info[\"total\"],\n",
        "                    \"covered\": info[\"covered\"]\n",
        "                })\n",
        "\n",
        "    # ----------- Îã®Ïàú Ï†úÏïΩ Íµ¨ÏÑ± -----------\n",
        "    # LLMÏóêÍ≤å \"Ïù¥ ÎùºÏù∏ÏùÑ Ïª§Î≤ÑÌïòÎäî ÌÖåÏä§Ìä∏ ÎßåÎì§Ïñ¥Îùº\" ÏãùÏùò ÏµúÏÜå constraint\n",
        "    constraints = []\n",
        "\n",
        "    # ÎùºÏù∏ Í∏∞Î∞ò Ï†úÏïΩ\n",
        "    for ln in miss_lines:\n",
        "        constraints.append(f\"hit line {ln}\")\n",
        "\n",
        "    # Î∏åÎûúÏπò Í∏∞Î∞ò Ï†úÏïΩ\n",
        "    for b in file_item[\"branch_targets\"]:\n",
        "        constraints.append(f\"cover branch at line {b['line']} with {b['total_outcomes']} outcomes\")\n",
        "\n",
        "    # Ï§ëÎ≥µ Ï†úÍ±∞\n",
        "    constraints = list(dict.fromkeys(constraints))\n",
        "\n",
        "    file_item[\"input_constraints\"] = constraints\n",
        "\n",
        "    goals_out.append(file_item)\n",
        "\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "out_file.write_text(\n",
        "    json.dumps(goals_out, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò ÏµúÏÜå Î™©Ìëú(goal_minimal) ÏÉùÏÑ± ÏôÑÎ£å ‚Üí\", out_file)\n",
        "print(\"Ï¥ù ÌååÏùº Ïàò:\", len(goals_out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "no-RwijrFe7_",
        "outputId": "39ae77cb-84c9-4109-eead-24348c4a125d"
      },
      "id": "no-RwijrFe7_",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò ÏµúÏÜå Î™©Ìëú(goal_minimal) ÏÉùÏÑ± ÏôÑÎ£å ‚Üí /content/money-transfer-project-template-python/run_artifacts/run1/goals_minimal.json\n",
            "Ï¥ù ÌååÏùº Ïàò: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-3) LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± (ÎØ∏Ïª§Î≤ÑÎùºÏù∏/Î∏åÎûúÏπò Í∏∞Î∞ò Î≤ÑÏ†Ñ)\n",
        "\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "PROJ_PATH = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ_PATH / \"run_artifacts\" / \"run1\"\n",
        "\n",
        "GOALS_MINIMAL = ART_DIR / \"goals_minimal.json\"\n",
        "LLM_OUT = ART_DIR / \"llm_prompts.jsonl\"\n",
        "\n",
        "assert GOALS_MINIMAL.exists(), \"goals_minimal.json ÏóÜÏùå. 3-2c(ÎØ∏ÎãàÎ©Ä) Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "\n",
        "goals = json.loads(GOALS_MINIMAL.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# -------------------------\n",
        "# Ïú†Ìã∏ Ìï®Ïàò\n",
        "# -------------------------\n",
        "\n",
        "def to_mod(file):\n",
        "    s = file.replace(\"\\\\\", \"/\")\n",
        "    if s.startswith(\"src/\"): s = s[4:]\n",
        "    if s.endswith(\".py\"): s = s[:-3]\n",
        "    return s.replace(\"/\", \".\")\n",
        "\n",
        "def suggest_filename(goal_idx, file):\n",
        "    base = Path(file).stem\n",
        "    safe = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", base)\n",
        "    return f\"test_gen_goal_{goal_idx}_{safe}.py\"\n",
        "\n",
        "# -------------------------\n",
        "# ÏãúÏä§ÌÖú Î©îÏãúÏßÄ\n",
        "# -------------------------\n",
        "\n",
        "SYSTEM = (\n",
        "\"Ïó≠Ìï†: ÎãπÏã†ÏùÄ Ï£ºÏñ¥ÏßÑ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏/Î∏åÎûúÏπò Î™©ÌëúÎ•º ÌÉÄÍ≤©ÌïòÎäî pytest ÌÖåÏä§Ìä∏ ÏΩîÎìúÎ•º ÏÉùÏÑ±ÌïòÎäî ÏóîÏßÑÏûÖÎãàÎã§.\\n\"\n",
        "\"Ï∂úÎ†•: Ïò§ÏßÅ ÌïòÎÇòÏùò JSON Í∞ùÏ≤¥Îßå Î∞òÌôòÌïòÍ≥† Ï£ºÏÑù/markdown Í∏àÏßÄ.\\n\"\n",
        "\"{\\n\"\n",
        "'  \"filename\": \"test_*.py\",\\n'\n",
        "'  \"tests\": [ {\"name\": \"test_*\", \"code\": \"<pytest code>\"} ]\\n'\n",
        "\"}\\n\"\n",
        "\"\\n\"\n",
        "\"ÏóÑÍ≤© Í∑úÏπô:\\n\"\n",
        "\"‚Ä¢ importlib.import_module + getattr Î°úÎßå Ïã¨Î≥º Ï†ëÍ∑º (ÏóÜÏúºÎ©¥ skip Í∞ÄÎä•)\\n\"\n",
        "\"‚Ä¢ ÏµúÏÜå 1Í∞ú assert ÎòêÎäî pytest.raises ÌïÑÏàò\\n\"\n",
        "\"‚Ä¢ target_linesÎ•º Ï†ÅÏñ¥ÎèÑ 1Í∞ú Ïù¥ÏÉÅ Î∞òÎìúÏãú Ïª§Î≤ÑÌï† Í≤É\\n\"\n",
        "\"‚Ä¢ branch targetÏù¥ ÏûàÏúºÎ©¥ Ïñë Í≤ΩÎ°ú ÌÖåÏä§Ìä∏ ÏÉùÏÑ± Í∂åÏû•\\n\"\n",
        "\"‚Ä¢ IO/ÏãúÍ∞Ñ/env/net/Temporal Îì± Ïô∏Î∂Ä Ìò∏Ï∂úÏùÄ Î™®Îëê monkeypatch\\n\"\n",
        "\"‚Ä¢ async ÎåÄÏÉÅÏùÄ Î∞òÎìúÏãú async ÌÖåÏä§Ìä∏ + await\\n\"\n",
        "\"‚Ä¢ asyncio.run Í∏àÏßÄ\\n\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±\n",
        "# -------------------------\n",
        "\n",
        "with LLM_OUT.open(\"w\", encoding=\"utf-8\") as outf:\n",
        "    for idx, g in enumerate(goals, start=1):\n",
        "\n",
        "        file = g[\"file\"]\n",
        "        target_lines = g[\"target_lines\"]\n",
        "        branch_targets = g.get(\"branch_targets\", [])\n",
        "        constraints = g.get(\"input_constraints\", [])\n",
        "\n",
        "        mod = to_mod(file)\n",
        "        filename = suggest_filename(idx, file)\n",
        "\n",
        "        USER = {\n",
        "            \"schema_version\": \"v1_minimal\",\n",
        "            \"identifier\": {\n",
        "                \"goal_index\": idx,\n",
        "                \"file\": file,\n",
        "                \"suggested_filename\": filename\n",
        "            },\n",
        "            \"project\": {\n",
        "                \"root\": str(PROJ_PATH),\n",
        "                \"module\": mod\n",
        "            },\n",
        "            \"goal\": {\n",
        "                \"target_lines\": target_lines,\n",
        "                \"branch_targets\": branch_targets,\n",
        "                \"input_constraints\": constraints\n",
        "            },\n",
        "            \"instructions\": [\n",
        "                \"Ï£ºÏñ¥ÏßÑ target_linesÎ•º Î∞òÎìúÏãú ÌÉÄÍ≤©ÌïòÎäî pytest ÌÖåÏä§Ìä∏ ÏÉùÏÑ±\",\n",
        "                \"branch_targetsÍ∞Ä ÏûàÏúºÎ©¥ Ìï¥Îãπ Î∂ÑÍ∏∞Î•º Î™®Îëê Ïª§Î≤Ñ\",\n",
        "                \"importlib + getattr Í∏∞Î∞ò ÎèôÏ†Å ÏûÑÌè¨Ìä∏ ÏÇ¨Ïö©\",\n",
        "                \"ÌÖåÏä§Ìä∏Îäî ÏµúÏÜå 1Í∞ú assert ÎòêÎäî pytest.raises Ìè¨Ìï®\",\n",
        "                \"async/Temporal Í∑úÏπô Ï§ÄÏàò\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        rec = {\n",
        "            \"meta\": {\n",
        "                \"goal_index\": idx,\n",
        "                \"file\": file,\n",
        "                \"module\": mod,\n",
        "                \"suggested_filename\": filename,\n",
        "                \"num_target_lines\": len(target_lines),\n",
        "                \"num_branch_targets\": len(branch_targets),\n",
        "                \"num_constraints\": len(constraints)\n",
        "            },\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(USER, ensure_ascii=False, indent=2)}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        outf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å ‚Üí\", LLM_OUT)\n",
        "print(\"Ï¥ù Î™©Ìëú:\", len(goals))\n",
        "print(\"ÏòàÏãú ÌååÏùºÎ™Ö:\", suggest_filename(1, goals[0]['file']))\n"
      ],
      "metadata": {
        "id": "dI-iQnUDhBwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fb12476e-1eee-4324-caeb-1a56ac59d7f9"
      },
      "id": "dI-iQnUDhBwy",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å ‚Üí /content/money-transfer-project-template-python/run_artifacts/run1/llm_prompts.jsonl\n",
            "Ï¥ù Î™©Ìëú: 5\n",
            "ÏòàÏãú ÌååÏùºÎ™Ö: test_gen_goal_1_activities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-4) ÌÖåÏä§Ìä∏ ÏΩîÎìú ÏÉùÏÑ± (ÎØ∏Ïª§Î≤Ñ Î™©Ìëú Í∏∞Î∞ò Î≤ÑÏ†Ñ)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import ast\n",
        "from pathlib import Path\n",
        "import httpx\n",
        "import backoff\n",
        "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
        "\n",
        "# ---------- Í≤ΩÎ°ú ÏÑ§Ï†ï ----------\n",
        "ART_DIR = Path(PROJ) / \"run_artifacts\" / \"run1\"\n",
        "LLM_PROMPTS_PATH = ART_DIR / \"llm_prompts.jsonl\"\n",
        "GEN_DIR = Path(PROJ) / \"generated_tests\"\n",
        "RAW_DIR = ART_DIR / \"_raw\"\n",
        "ERR_DIR = ART_DIR / \"_errors\"\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ERR_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- OpenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ----------\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEYÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "\n",
        "http_client = httpx.Client(\n",
        "    timeout=180.0,\n",
        "    follow_redirects=True,\n",
        "    limits=httpx.Limits(max_connections=1, max_keepalive_connections=0),\n",
        "    transport=httpx.HTTPTransport(retries=5),\n",
        ")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), http_client=http_client)\n",
        "\n",
        "# ---------- Ïú†Ìã∏ ----------\n",
        "_slug_re = re.compile(r\"[^a-z0-9_]+\")\n",
        "\n",
        "def slugify(s: str, maxlen: int = 40) -> str:\n",
        "    s = s.lower().strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
        "    s = _slug_re.sub(\"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return s[:maxlen] or \"t\"\n",
        "\n",
        "def strip_fences(s: str) -> str:\n",
        "    s = re.sub(r\"^```[a-zA-Z0-9]*\\s*\", \"\", s.strip())\n",
        "    s = re.sub(r\"\\s*```$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def ensure_unique_path(base: Path) -> Path:\n",
        "    p = base\n",
        "    i = 2\n",
        "    while p.exists():\n",
        "        p = base.with_name(f\"{base.stem}_{i}{base.suffix}\")\n",
        "        i += 1\n",
        "    return p\n",
        "\n",
        "def write_error(goal_idx: int, kind: str, payload: dict, idx: int | None = None):\n",
        "    tag = f\"goal_{goal_idx}_{kind}\" if idx is None else f\"goal_{goal_idx}_t{idx}_{kind}\"\n",
        "    (ERR_DIR / f\"{tag}.json\").write_text(\n",
        "        json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "# ---------- import Î≥¥Ï†ï ----------\n",
        "def auto_fix_imports(code: str) -> str:\n",
        "    lines = code.splitlines()\n",
        "    text = \"\\n\".join(lines)\n",
        "\n",
        "    need_pytest = (\"pytest.\" in text) and not re.search(r\"^\\s*import\\s+pytest\\b\", text, re.M)\n",
        "    need_importlib = (\"importlib.\" in text) and not re.search(r\"^\\s*import\\s+importlib\\b\", text, re.M)\n",
        "\n",
        "    prepend = []\n",
        "    if need_pytest: prepend.append(\"import pytest\")\n",
        "    if need_importlib: prepend.append(\"import importlib\")\n",
        "\n",
        "    if not prepend:\n",
        "        return code\n",
        "\n",
        "    return \"\\n\".join(prepend + lines) + \"\\n\"\n",
        "\n",
        "# ---------- ÏΩîÎìú Ï†ïÎ¶¨ ----------\n",
        "def sanitize_test_code(code: str) -> str:\n",
        "    patterns = [\n",
        "        re.compile(r\"(?ms)^\\s*if\\s+__name__\\s*==\\s*['\\\"]__main__['\\\"]\\s*:\\s*\\n(?:\\s+.*\\n?)+$\"),\n",
        "        re.compile(r\"(?m)^\\s*pytest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "        re.compile(r\"(?m)^\\s*unittest\\.main\\s*\\(.*?\\)\\s*$\"),\n",
        "    ]\n",
        "    new = code\n",
        "    for pat in patterns:\n",
        "        new = pat.sub(\"\", new)\n",
        "    return new.strip() + \"\\n\"\n",
        "\n",
        "# ---------- AST Í≤ÄÏ¶ù ----------\n",
        "def parse_ast_or_error(code: str):\n",
        "    try:\n",
        "        return ast.parse(code), None\n",
        "    except SyntaxError as e:\n",
        "        return None, f\"syntax_error:{e.msg}@L{e.lineno}\"\n",
        "\n",
        "def extract_test_funcs(tree: ast.AST) -> list:\n",
        "    return [\n",
        "        n for n in ast.walk(tree)\n",
        "        if isinstance(n, ast.FunctionDef) and n.name.startswith(\"test_\")\n",
        "    ]\n",
        "\n",
        "def has_assert_or_raises(tree: ast.AST, code: str) -> bool:\n",
        "    has_assert_stmt = any(isinstance(n, ast.Assert) for n in ast.walk(tree))\n",
        "    has_pytest_raises = \"pytest.raises(\" in code\n",
        "    return has_assert_stmt or has_pytest_raises\n",
        "\n",
        "# ---------- ÏµúÏÜå Í≤ÄÏ¶ù ----------\n",
        "def minimal_viability_checks(code: str):\n",
        "    reasons = []\n",
        "\n",
        "    tree, synerr = parse_ast_or_error(code)\n",
        "    if synerr:\n",
        "        reasons.append(synerr)\n",
        "        return False, reasons\n",
        "\n",
        "    tests = extract_test_funcs(tree)\n",
        "    if not tests:\n",
        "        reasons.append(\"no_test_functions\")\n",
        "\n",
        "    if not has_assert_or_raises(tree, code):\n",
        "        reasons.append(\"no_assert_or_raises\")\n",
        "\n",
        "    return (len(reasons) == 0), reasons\n",
        "\n",
        "# ---------- OpenAI Ìò∏Ï∂ú ----------\n",
        "@backoff.on_exception(\n",
        "    backoff.expo,\n",
        "    (APIConnectionError, APIError, RateLimitError),\n",
        "    max_tries=8,\n",
        "    max_time=300\n",
        ")\n",
        "def call_openai_with_retry(messages):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        timeout=200.0,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "# ---------- filename Í≤∞Ï†ï ----------\n",
        "def choose_filename(goal_idx: int, idx: int):\n",
        "    return f\"test_gen_goal_{goal_idx}_{idx}.py\"\n",
        "\n",
        "# ---------- Î©îÏù∏ ----------\n",
        "gen_log_path = ART_DIR / \"gen_log.jsonl\"\n",
        "ok_count = 0\n",
        "fail_count = 0\n",
        "\n",
        "with LLM_PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f_in, gen_log_path.open(\"w\", encoding=\"utf-8\") as f_log:\n",
        "    for line in f_in:\n",
        "        rec = json.loads(line)\n",
        "        goal_idx = rec[\"meta\"][\"goal_index\"]\n",
        "\n",
        "        messages = rec[\"messages\"]\n",
        "        print(f\"\\nüöÄ Goal {goal_idx} ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\")\n",
        "\n",
        "        try:\n",
        "            out_text = call_openai_with_retry(messages)\n",
        "        except Exception as e:\n",
        "            fail_count += 1\n",
        "            write_error(goal_idx, \"request_error\", {\"error\": str(e)})\n",
        "            print(f\"‚ùå Goal {goal_idx} ÏöîÏ≤≠ Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        (RAW_DIR / f\"goal_{goal_idx}_raw.json\").write_text(out_text, encoding=\"utf-8\")\n",
        "\n",
        "        try:\n",
        "            cleaned = strip_fences(out_text)\n",
        "            result = json.loads(cleaned)\n",
        "        except Exception as e:\n",
        "            fail_count += 1\n",
        "            write_error(goal_idx, \"json_parse_error\", {\"error\": str(e), \"raw\": out_text[:1000]})\n",
        "            print(f\"‚ùå Goal {goal_idx} JSON ÌååÏã± Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        tests = result.get(\"tests\", [])\n",
        "        saved_files = []\n",
        "        excluded = []\n",
        "\n",
        "        for idx, t in enumerate(tests, start=1):\n",
        "            code = sanitize_test_code(strip_fences(t.get(\"code\", \"\")))\n",
        "            code = auto_fix_imports(code)\n",
        "\n",
        "            ok_min, reasons = minimal_viability_checks(code)\n",
        "            if not ok_min:\n",
        "                excluded.append({\"index\": idx, \"reasons\": reasons})\n",
        "                write_error(goal_idx, \"min_viability\", {\"index\": idx, \"reasons\": reasons})\n",
        "                print(f\"‚ö†Ô∏è Goal {goal_idx} ÌÖåÏä§Ìä∏ #{idx} Ï†úÏô∏: {reasons}\")\n",
        "                continue\n",
        "\n",
        "            out_filename = choose_filename(goal_idx, idx)\n",
        "            out_path = ensure_unique_path(GEN_DIR / out_filename)\n",
        "            out_path.write_text(code, encoding=\"utf-8\")\n",
        "            saved_files.append(out_path.name)\n",
        "            print(f\"‚úÖ Ï†ÄÏû•: {out_path.name}\")\n",
        "\n",
        "        if saved_files:\n",
        "            ok_count += 1\n",
        "            f_log.write(json.dumps({\"goal_idx\": goal_idx, \"saved_files\": saved_files, \"excluded\": excluded}, ensure_ascii=False) + \"\\n\")\n",
        "        else:\n",
        "            fail_count += 1\n",
        "            write_error(goal_idx, \"no_valid_tests\", {\"excluded\": excluded})\n",
        "            print(f\"‚ùå Goal {goal_idx} Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå\")\n",
        "\n",
        "print(f\"\\n‚úÖ ÏÉùÏÑ± ÏôÑÎ£å: ÏÑ±Í≥µ {ok_count} / Ïã§Ìå® {fail_count}\")\n",
        "print(f\"üìÇ Ï†ÄÏû• ÏúÑÏπò: {GEN_DIR}\")\n",
        "print(f\"üìÅ ÏõêÎ≥∏: {RAW_DIR}\")\n",
        "print(f\"üìÅ ÏóêÎü¨: {ERR_DIR}\")\n",
        "print(f\"üìÑ Î°úÍ∑∏: {gen_log_path}\")\n"
      ],
      "metadata": {
        "id": "pYNqW94ih1mr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "101f49fc-e221-4be5-e362-e5c8df6c6410"
      },
      "id": "pYNqW94ih1mr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Goal 1 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_goal_1_1.py\n",
            "\n",
            "üöÄ Goal 2 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 2 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ùå Goal 2 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå\n",
            "\n",
            "üöÄ Goal 3 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 3 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_assert_or_raises']\n",
            "‚ùå Goal 3 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå\n",
            "\n",
            "üöÄ Goal 4 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚úÖ Ï†ÄÏû•: test_gen_goal_4_1.py\n",
            "‚úÖ Ï†ÄÏû•: test_gen_goal_4_2.py\n",
            "\n",
            "üöÄ Goal 5 ÌÖåÏä§Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠‚Ä¶\n",
            "‚ö†Ô∏è Goal 5 ÌÖåÏä§Ìä∏ #1 Ï†úÏô∏: ['no_test_functions']\n",
            "‚ùå Goal 5 Ïú†Ìö® ÌÖåÏä§Ìä∏ ÏóÜÏùå\n",
            "\n",
            "‚úÖ ÏÉùÏÑ± ÏôÑÎ£å: ÏÑ±Í≥µ 2 / Ïã§Ìå® 3\n",
            "üìÇ Ï†ÄÏû• ÏúÑÏπò: /content/money-transfer-project-template-python/generated_tests\n",
            "üìÅ ÏõêÎ≥∏: /content/money-transfer-project-template-python/run_artifacts/run1/_raw\n",
            "üìÅ ÏóêÎü¨: /content/money-transfer-project-template-python/run_artifacts/run1/_errors\n",
            "üìÑ Î°úÍ∑∏: /content/money-transfer-project-template-python/run_artifacts/run1/gen_log.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-5) Í∏∞Ï°¥ tests(ÌîÑÎ£®Îãù) + ÏÉùÏÑ± tests Í≤©Î¶¨ Ïã§Ìñâ ¬∑ Î°úÍ∑∏ ÏàòÏßë ¬∑ ÏÉ§Îìú Í≤∞Ìï© ¬∑ Ìñ•ÏÉÅÏπò Í≥ÑÏÇ∞ (autosetup Ìè¨Ìï®)\n",
        "import os, sys, json, re, time, subprocess, shutil, shlex\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from lxml import etree\n",
        "\n",
        "# ==== Í≤ΩÎ°ú/ÏÉÅÏàò ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "TESTS_DIR = PROJ / \"tests\"                   # ÏõêÎ≥∏(Î∞±ÏóÖÏö©)\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"      # ÌîÑÎ£®Îãù ÏßëÌï©(Ïö∞ÏÑ†)\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "COV_SHARDS_DIR = ART_DIR / \"cov_shards\"\n",
        "HTML_DIR_GEN = PROJ / \"htmlcov_gen\"\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "COV_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR_GEN.mkdir(parents=True, exist_ok=True)\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "\n",
        "# Ïã§Ìñâ ÌååÎùºÎØ∏ÌÑ∞\n",
        "PY_EXE = sys.executable\n",
        "TIMEOUT_SEC_GEN = 45            # ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº 1Í∞úÎãπ ÌÉÄÏûÑÏïÑÏõÉ (ÎäòÎ¶º)\n",
        "TIMEOUT_SEC_BASE = 45           # (ÌîÑÎ£®ÎãùÎêú) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº 1Í∞úÎãπ ÌÉÄÏûÑÏïÑÏõÉ\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "ENV_BASE = os.environ.copy()\n",
        "\n",
        "# goal_id Ï∂îÏ∂ú\n",
        "RE_GOAL = re.compile(r\"(?:^|[_-])(?P<gid>\\d{4})(?:[_-]|$)\")\n",
        "\n",
        "# ==== Autosetup: pytest.ini + generated_tests/conftest.py ====\n",
        "def ensure_pytest_ini():\n",
        "    ini = PROJ / \"pytest.ini\"\n",
        "    base = []\n",
        "    if ini.exists():\n",
        "        base = ini.read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "    def set_or_append(lines, key, value):\n",
        "        if not any(l.strip().startswith(\"[pytest]\") for l in lines):\n",
        "            lines = [\"[pytest]\"] + lines\n",
        "        found = False\n",
        "        for i, l in enumerate(lines):\n",
        "            if l.strip().startswith(key):\n",
        "                lines[i] = f\"{key} = {value}\"\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            try:\n",
        "                idx = next(i for i,l in enumerate(lines) if l.strip().startswith(\"[pytest]\"))\n",
        "            except StopIteration:\n",
        "                idx = -1\n",
        "            insert_at = idx+1 if idx>=0 else len(lines)\n",
        "            lines.insert(insert_at, f\"{key} = {value}\")\n",
        "        return lines\n",
        "\n",
        "    lines = base[:]\n",
        "    lines = set_or_append(lines, \"asyncio_mode\", \"auto\")  # pytest-asyncio ÏûêÎèô Î™®Îìú\n",
        "    if not any(l.strip().startswith(\"addopts\") for l in lines):\n",
        "        lines.append(\"addopts = -q -s\")\n",
        "    ini.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"‚úÖ pytest.ini ensured ‚Üí\", ini)\n",
        "\n",
        "def ensure_conftest_autouse():\n",
        "    cf = GEN_DIR / \"conftest.py\"\n",
        "    payload = (\n",
        "        \"import os, asyncio, pytest, importlib, inspect\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _no_real_net_env(monkeypatch):\\n\"\n",
        "        \"    monkeypatch.setenv('NO_PROXY', '*')\\n\"\n",
        "        \"    for k in ['HTTP_PROXY','HTTPS_PROXY','http_proxy','https_proxy','ALL_PROXY','all_proxy']:\\n\"\n",
        "        \"        monkeypatch.delenv(k, raising=False)\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _patch_time(monkeypatch):\\n\"\n",
        "        \"    import time\\n\"\n",
        "        \"    monkeypatch.setattr(time, 'sleep', lambda *_: None, raising=False)\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _patch_temporal(monkeypatch):\\n\"\n",
        "        \"    try:\\n\"\n",
        "        \"        import temporalio  # noqa: F401\\n\"\n",
        "        \"    except Exception:\\n\"\n",
        "        \"        return\\n\"\n",
        "        \"    class _Dummy:\\n\"\n",
        "        \"        def __init__(self, *a, **k): pass\\n\"\n",
        "        \"        async def __aenter__(self): return self\\n\"\n",
        "        \"        async def __aexit__(self, *a): pass\\n\"\n",
        "        \"        def __call__(self, *a, **k): return self\\n\"\n",
        "        \"        async def run(self, *a, **k): return None\\n\"\n",
        "        \"        async def start(self, *a, **k): return None\\n\"\n",
        "        \"    for path in ['temporalio.client.Client','temporalio.worker.Worker','temporalio.workflow.Workflow']:\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            mod_name, attr = path.rsplit('.', 1)\\n\"\n",
        "        \"            mod = importlib.import_module(mod_name)\\n\"\n",
        "        \"            setattr(mod, attr, _Dummy)\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            pass\\n\"\n",
        "        \"\\n\"\n",
        "        \"def _wrap_async(func):\\n\"\n",
        "        \"    if inspect.iscoroutinefunction(func):\\n\"\n",
        "        \"        def sync_wrapper(*a, **k):\\n\"\n",
        "        \"            try:\\n\"\n",
        "        \"                loop = asyncio.get_event_loop()\\n\"\n",
        "        \"            except RuntimeError:\\n\"\n",
        "        \"                loop = asyncio.new_event_loop()\\n\"\n",
        "        \"                asyncio.set_event_loop(loop)\\n\"\n",
        "        \"            return loop.run_until_complete(func(*a, **k))\\n\"\n",
        "        \"        return sync_wrapper\\n\"\n",
        "        \"    return func\\n\"\n",
        "        \"\\n\"\n",
        "        \"_TARGET_MODULES = [\\n\"\n",
        "        \"    'run_worker_module',\\n\"\n",
        "        \"    'run_workflow_module',\\n\"\n",
        "        \"    'run_workflow_main',\\n\"\n",
        "        \"]\\n\"\n",
        "        \"\\n\"\n",
        "        \"@pytest.fixture(autouse=True)\\n\"\n",
        "        \"def _wrap_async_entrypoints(monkeypatch):\\n\"\n",
        "        \"    for mod_name in _TARGET_MODULES:\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            mod = importlib.import_module(mod_name)\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            continue\\n\"\n",
        "        \"        for attr in ('main','run','start'):\\n\"\n",
        "        \"            if hasattr(mod, attr):\\n\"\n",
        "        \"                try:\\n\"\n",
        "        \"                    monkeypatch.setattr(mod, attr, _wrap_async(getattr(mod, attr)), raising=False)\\n\"\n",
        "        \"                except Exception:\\n\"\n",
        "        \"                    pass\\n\"\n",
        "        \"    def _safe_run(coro):\\n\"\n",
        "        \"        if inspect.iscoroutine(coro):\\n\"\n",
        "        \"            try:\\n\"\n",
        "        \"                loop = asyncio.get_event_loop()\\n\"\n",
        "        \"            except RuntimeError:\\n\"\n",
        "        \"                loop = asyncio.new_event_loop()\\n\"\n",
        "        \"                asyncio.set_event_loop(loop)\\n\"\n",
        "        \"            return loop.run_until_complete(coro)\\n\"\n",
        "        \"        return coro\\n\"\n",
        "        \"    monkeypatch.setattr(asyncio, 'run', _safe_run, raising=False)\\n\"\n",
        "    )\n",
        "    if cf.exists():\n",
        "        text = cf.read_text(encoding=\"utf-8\")\n",
        "        if \"_wrap_async_entrypoints\" not in text:\n",
        "            text = text.rstrip() + \"\\n\\n\" + payload\n",
        "            cf.write_text(text, encoding=\"utf-8\")\n",
        "            print(\"‚úÖ conftest.py augmented ‚Üí\", cf)\n",
        "        else:\n",
        "            print(\"‚úÖ conftest.py already has autosetup ‚Üí\", cf)\n",
        "    else:\n",
        "        cf.write_text(payload, encoding=\"utf-8\")\n",
        "        print(\"‚úÖ conftest.py created ‚Üí\", cf)\n",
        "\n",
        "ensure_pytest_ini()\n",
        "ensure_conftest_autouse()\n",
        "\n",
        "# ==== Ïú†Ìã∏ ====\n",
        "def goal_id_from_name(name: str) -> str | None:\n",
        "    m = RE_GOAL.search(name)\n",
        "    return m.group(\"gid\") if m else None\n",
        "\n",
        "def sh(cmd: str, cwd: Path | None = None, timeout: int | None = None, env: dict | None = None):\n",
        "    try:\n",
        "        p = subprocess.run(\n",
        "            cmd, cwd=str(cwd or PROJ), env=env or ENV_BASE,\n",
        "            shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "            timeout=timeout, text=True\n",
        "        )\n",
        "        return p.returncode, p.stdout, p.stderr, False\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        return 124, e.stdout or \"\", e.stderr or \"\", True\n",
        "\n",
        "def list_test_files(dir_path: Path) -> list[Path]:\n",
        "    if not dir_path.exists():\n",
        "        return []\n",
        "    files = []\n",
        "    files += list(dir_path.glob(\"test*.py\"))\n",
        "    files += list(dir_path.glob(\"*_test.py\"))\n",
        "    return sorted(set(p for p in files if p.is_file()))\n",
        "\n",
        "def list_generated_test_files() -> list[Path]:\n",
        "    if not GEN_DIR.exists():\n",
        "        return []\n",
        "    # conftest.pyÎäî ÏàòÏßë ÎåÄÏÉÅÏóêÏÑú Ï†úÏô∏\n",
        "    return sorted([p for p in GEN_DIR.glob(\"*.py\") if p.is_file() and p.name != \"conftest.py\"])\n",
        "\n",
        "def rel_to_proj(p: Path) -> str:\n",
        "    try:\n",
        "        return str(p.resolve().relative_to(PROJ))\n",
        "    except Exception:\n",
        "        return str(p.resolve())\n",
        "\n",
        "# ==== 0) ÏÇ∞Ï∂úÎ¨º ÌååÏùº Í≤ΩÎ°ú ====\n",
        "results_jsonl = ART_DIR / \"results.jsonl\"\n",
        "manifest_path = ART_DIR / \"manifest.json\"\n",
        "coverage_json_path = ART_DIR / \"coverage_gen.json\"     # Ïù¥Î≤à ÎùºÏö¥Îìú ÌÜµÌï©(ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±)\n",
        "coverage_xml_path  = ART_DIR / \"coverage_gen.xml\"\n",
        "coverage_base_json = ART_DIR / \"coverage_base.json\"    # 3-1 Í∏∞Ï§ÄÏÑ†(ÏõêÎ≥∏ Í∏∞Ï°¥ ÌÖåÏä§Ìä∏)\n",
        "\n",
        "for old in [results_jsonl, coverage_json_path, coverage_xml_path]:\n",
        "    if old.exists():\n",
        "        old.unlink()\n",
        "\n",
        "runs = []\n",
        "ok = fail = to_cnt = 0\n",
        "\n",
        "# ==== 1) (ÌîÑÎ£®ÎãùÎêú) Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ Í∞úÎ≥Ñ Í≤©Î¶¨ Ïã§Ìñâ ====\n",
        "base_root = PRUNED_DIR if PRUNED_DIR.exists() else TESTS_DIR\n",
        "base_files = list_test_files(base_root)\n",
        "\n",
        "if base_files:\n",
        "    print(f\"üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: {len(base_files)}Í∞ú @ {rel_to_proj(base_root)}\")\n",
        "    for tf in base_files:\n",
        "        name = tf.name\n",
        "        shard = COV_SHARDS_DIR / f\".coverage.__base__.{name}\"\n",
        "\n",
        "        env = ENV_BASE.copy()\n",
        "        env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "        env[\"COVERAGE_FILE\"] = str(shard)\n",
        "        env.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "        target = rel_to_proj(tf)\n",
        "        cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "\n",
        "        start = time.time()\n",
        "        ts_start = datetime.now(timezone.utc).isoformat()\n",
        "        rc, out, err, timed_out = sh(cmd, cwd=PROJ, timeout=TIMEOUT_SEC_BASE, env=env)\n",
        "        dur = round(time.time() - start, 3)\n",
        "        ts_end = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "        (LOG_DIR / f\"__base__{name}.out.txt\").write_text(out, encoding=\"utf-8\")\n",
        "        (LOG_DIR / f\"__base__{name}.err.txt\").write_text(err, encoding=\"utf-8\")\n",
        "\n",
        "        runs.append({\n",
        "            \"suite\": \"BASE\",\n",
        "            \"test_file\": name,\n",
        "            \"goal_id\": None,\n",
        "            \"start_utc\": ts_start,\n",
        "            \"end_utc\": ts_end,\n",
        "            \"duration_sec\": dur,\n",
        "            \"returncode\": rc,\n",
        "            \"timed_out\": timed_out,\n",
        "            \"stdout_len\": len(out),\n",
        "            \"stderr_len\": len(err),\n",
        "            \"shard_path\": str(shard),\n",
        "            \"invoked_path\": target,\n",
        "        })\n",
        "\n",
        "        if timed_out:\n",
        "            to_cnt += 1\n",
        "            print(f\"‚è±Ô∏è TIMEOUT [BASE] {name} ({dur}s)\")\n",
        "        elif rc == 0:\n",
        "            ok += 1\n",
        "            print(f\"‚úÖ PASS   [BASE] {name} ({dur}s)\")\n",
        "        else:\n",
        "            fail += 1\n",
        "            first_err = (err.strip().splitlines() or [''])[0]\n",
        "            print(f\"‚ùå FAIL   [BASE] {name} (rc={rc}, {dur}s) :: {first_err}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ BASE Ïã§ÌñâÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§. (Pruned_Base_Tests/ ÎòêÎäî tests/ ÎπÑÏñ¥ÏûàÏùå)\")\n",
        "\n",
        "# ==== 2) ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº Í∞úÎ≥Ñ Í≤©Î¶¨ Ïã§Ìñâ ====\n",
        "test_files = list_generated_test_files()\n",
        "print(f\"üß™ ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº: {len(test_files)}Í∞ú @ {rel_to_proj(GEN_DIR)}\")\n",
        "for tf in test_files:\n",
        "    name = tf.name\n",
        "    gid = goal_id_from_name(name) or \"----\"\n",
        "    shard = COV_SHARDS_DIR / f\".coverage.{name}\"\n",
        "\n",
        "    env = ENV_BASE.copy()\n",
        "    env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "    env.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "    target = rel_to_proj(tf)   # e.g. \"generated_tests/test_0001_...py\"\n",
        "    cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "\n",
        "    start = time.time()\n",
        "    ts_start = datetime.now(timezone.utc).isoformat()\n",
        "    rc, out, err, timed_out = sh(cmd, cwd=PROJ, timeout=TIMEOUT_SEC_GEN, env=env)\n",
        "    dur = round(time.time() - start, 3)\n",
        "    ts_end = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "    (LOG_DIR / f\"{name}.out.txt\").write_text(out, encoding=\"utf-8\")\n",
        "    (LOG_DIR / f\"{name}.err.txt\").write_text(err, encoding=\"utf-8\")\n",
        "\n",
        "    runs.append({\n",
        "        \"suite\": \"GEN\",\n",
        "        \"test_file\": name,\n",
        "        \"goal_id\": gid,\n",
        "        \"start_utc\": ts_start,\n",
        "        \"end_utc\": ts_end,\n",
        "        \"duration_sec\": dur,\n",
        "        \"returncode\": rc,\n",
        "        \"timed_out\": timed_out,\n",
        "        \"stdout_len\": len(out),\n",
        "        \"stderr_len\": len(err),\n",
        "        \"shard_path\": str(shard),\n",
        "        \"invoked_path\": target,\n",
        "    })\n",
        "\n",
        "    if timed_out:\n",
        "        to_cnt += 1\n",
        "        print(f\"‚è±Ô∏è TIMEOUT [GEN] {name} ({dur}s)\")\n",
        "    elif rc == 0:\n",
        "        ok += 1\n",
        "        print(f\"‚úÖ PASS   [GEN] {name} ({dur}s)\")\n",
        "    else:\n",
        "        fail += 1\n",
        "        first_err = (err.strip().splitlines() or [''])[0]\n",
        "        print(f\"‚ùå FAIL   [GEN] {name} (rc={rc}, {dur}s) :: {first_err}\")\n",
        "\n",
        "# Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•\n",
        "results_jsonl.write_text(\n",
        "    \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in runs) + (\"\\n\" if runs else \"\"),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "manifest = {\n",
        "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"project\": str(PROJ),\n",
        "    \"run_dir\": str(ART_DIR),\n",
        "    \"tests_total\": len([r for r in runs if r['suite'] in {'BASE','GEN'}]),\n",
        "    \"pass\": ok,\n",
        "    \"fail\": fail,\n",
        "    \"timeout\": to_cnt,\n",
        "    \"logs_dir\": str(LOG_DIR),\n",
        "    \"cov_shards_dir\": str(COV_SHARDS_DIR),\n",
        "    \"base_root\": str(rel_to_proj(base_root)) if base_files else None,\n",
        "    \"gen_root\": str(rel_to_proj(GEN_DIR)),\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"\\nüì¶ Ïã§Ìñâ ÏöîÏïΩ:\", json.dumps({\"pass\": ok, \"fail\": fail, \"timeout\": to_cnt, \"total\": manifest['tests_total']}, ensure_ascii=False))\n",
        "\n",
        "# ==== 3) Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï©(JSON/XML/HTML) ====\n",
        "shards = sorted([p for p in COV_SHARDS_DIR.iterdir() if p.name.startswith(\".coverage.\")])\n",
        "if not shards:\n",
        "    print(\"‚ö†Ô∏è Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÉ§ÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§. ÌÖåÏä§Ìä∏Í∞Ä Ï¶âÏãú Ïã§Ìå®ÌñàÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\")\n",
        "else:\n",
        "    subprocess.call(f\"coverage erase{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    combine_cmd = \"coverage combine\" + rc_opt + \" \" + \" \".join(shlex.quote(str(p)) for p in shards)\n",
        "    print(\"> \", combine_cmd)\n",
        "    subprocess.call(combine_cmd, shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage json -o {coverage_json_path.name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o {coverage_xml_path.name}{rc_opt}\",  shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage html -d {HTML_DIR_GEN.name}{rc_opt}\",       shell=True, cwd=str(PROJ))\n",
        "\n",
        "    # Í≤∞Í≥º ÌååÏùºÏùÑ run_artifactsÏóê Î≥µÏÇ¨ Î≥¥Í¥Ä\n",
        "    src_json = PROJ / coverage_json_path.name\n",
        "    src_xml  = PROJ / coverage_xml_path.name\n",
        "    if src_json.exists(): shutil.copy2(src_json, coverage_json_path)\n",
        "    if src_xml.exists():  shutil.copy2(src_xml,  coverage_xml_path)\n",
        "\n",
        "    print(\"‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\")\n",
        "    print(\" - JSON :\", coverage_json_path)\n",
        "    print(\" - XML  :\", coverage_xml_path)\n",
        "    print(\" - HTML :\", HTML_DIR_GEN / \"index.html\")\n",
        "\n",
        "# ==== 4) Î∂ÑÍ∏∞ Í¥ÄÏ∏°/Î™©Ìëú Îã¨ÏÑ±Î•† Í≥ÑÏÇ∞ ====\n",
        "observed_outcomes_gen = {}\n",
        "branch_points = full_hit = half_hit = zero_hit = 0\n",
        "\n",
        "if coverage_xml_path.exists():\n",
        "    try:\n",
        "        xml_root = etree.parse(str(coverage_xml_path)).getroot()\n",
        "        for cls in xml_root.findall(\".//class\"):\n",
        "            filename = cls.get(\"filename\") or \"\"\n",
        "            if not filename:\n",
        "                continue\n",
        "            abs_path = (PROJ / filename).resolve() if not Path(filename).is_absolute() else Path(filename)\n",
        "            for line in cls.findall(\"./lines/line\"):\n",
        "                if line.get(\"branch\") != \"true\":\n",
        "                    continue\n",
        "                try:\n",
        "                    num = int(line.get(\"number\"))\n",
        "                except Exception:\n",
        "                    continue\n",
        "                cond = line.get(\"condition-coverage\")  # \"50% (1/2)\"\n",
        "                covered = total = 0\n",
        "                if cond:\n",
        "                    m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cond)\n",
        "                    if m:\n",
        "                        covered, total = int(m.group(1)), int(m.group(2))\n",
        "                if total == 0:\n",
        "                    continue\n",
        "                observed_outcomes_gen.setdefault(str(abs_path), {})[num] = {\n",
        "                    \"covered\": covered, \"total\": total, \"ratio\": round(covered/total, 3)\n",
        "                }\n",
        "                branch_points += 1\n",
        "                if covered == 0: zero_hit += 1\n",
        "                elif covered == total: full_hit += 1\n",
        "                else: half_hit += 1\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è coverage_xml ÌååÏã± Ïã§Ìå®:\", e)\n",
        "\n",
        "(ART_DIR / \"observed_outcomes_gen.json\").write_text(\n",
        "    json.dumps(observed_outcomes_gen, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"üßÆ Î∂ÑÍ∏∞ Í¥ÄÏ∏° ÏöîÏïΩ ‚Üí total:{branch_points}, full:{full_hit}, half:{half_hit}, zero:{zero_hit}\")\n",
        "\n",
        "# ==== 5) Î™©Ìëú Îã¨ÏÑ±Î•†(Í∏∞Î≥∏) ====\n",
        "GOALS_FILE = ART_DIR / \"goals_ranked.json\"\n",
        "if GOALS_FILE.exists() and coverage_json_path.exists():\n",
        "    cov_json = json.loads((coverage_json_path).read_text(encoding=\"utf-8\"))\n",
        "    files_map = cov_json.get(\"files\", {}) or {}\n",
        "\n",
        "    def line_hit(fpath: str, ln: int) -> bool:\n",
        "        finfo = files_map.get(fpath) or files_map.get(str(Path(fpath).resolve()))\n",
        "        if not finfo:\n",
        "            return False\n",
        "        executed = set(finfo.get(\"executed_lines\", []) or [])\n",
        "        return ln in executed\n",
        "\n",
        "    goals = json.loads(GOALS_FILE.read_text(encoding=\"utf-8\"))\n",
        "    goal_stats = []\n",
        "    for g in goals:\n",
        "        f = g[\"file\"]\n",
        "        abs1 = str((PROJ / f).resolve())\n",
        "        abs2 = f\n",
        "        hit = sum(1 for ln in g.get(\"target_lines\", []) if line_hit(abs1, ln) or line_hit(abs2, ln))\n",
        "        total = len(g.get(\"target_lines\", [])) or 1\n",
        "        goal_stats.append({\"id\": g[\"id\"], \"hit\": hit, \"total\": total, \"rate\": round(hit/total, 3)})\n",
        "\n",
        "    (ART_DIR / \"goal_achievements.json\").write_text(\n",
        "        json.dumps(goal_stats, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    hit_goals = sum(1 for s in goal_stats if s[\"hit\"] > 0)\n",
        "    print(f\"üéØ Î™©Ìëú Îã¨ÏÑ±Î•†: {hit_goals}/{len(goal_stats)} Î™©ÌëúÍ∞Ä ‚â•1 ÎùºÏù∏ ÎèÑÎã¨\")\n",
        "\n",
        "# ==== 6) Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò(delta) Í≥ÑÏÇ∞ ====\n",
        "def load_json(p: Path, default=None):\n",
        "    try:\n",
        "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "base = load_json(coverage_base_json, {\"files\": {}}) or {\"files\": {}}\n",
        "gen  = load_json(coverage_json_path, {\"files\": {}}) or {\"files\": {}}\n",
        "base_files = base.get(\"files\", {}) or {}\n",
        "gen_files  = gen.get(\"files\", {})  or {}\n",
        "\n",
        "def _sum_len(key, d):\n",
        "    return sum(len((d.get(f, {}) or {}).get(key, []) or []) for f in d.keys())\n",
        "\n",
        "base_exec = _sum_len(\"executed_lines\", base_files)\n",
        "base_miss = _sum_len(\"missing_lines\",  base_files)\n",
        "gen_exec  = _sum_len(\"executed_lines\", gen_files)\n",
        "gen_miss  = _sum_len(\"missing_lines\",  gen_files)\n",
        "\n",
        "delta = {\n",
        "    \"executed_lines_delta\": gen_exec - base_exec,\n",
        "    \"missing_lines_delta\":  base_miss - gen_miss,   # +Î©¥ ÎØ∏Ïã± Í∞êÏÜå\n",
        "    \"base_executed\": base_exec,\n",
        "    \"gen_executed\":  gen_exec,\n",
        "    \"base_missing\":  base_miss,\n",
        "    \"gen_missing\":   gen_miss,\n",
        "}\n",
        "(ART_DIR / \"coverage_delta.json\").write_text(json.dumps(delta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò:\", json.dumps(delta, ensure_ascii=False))\n",
        "\n",
        "print(\"‚úÖ 3-5 ÏôÑÎ£å: (ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±) autosetup + Í≤©Î¶¨ Ïã§Ìñâ/ÏÉ§Îìú Í≤∞Ìï©/Î∂ÑÍ∏∞¬∑Î™©Ìëú¬∑Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú\")\n"
      ],
      "metadata": {
        "id": "cxb2ea7Wi2Zk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53e89efd-fbd2-4d7d-c42a-e5ccbeb15267"
      },
      "id": "cxb2ea7Wi2Zk",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ pytest.ini ensured ‚Üí /content/money-transfer-project-template-python/pytest.ini\n",
            "‚úÖ conftest.py created ‚Üí /content/money-transfer-project-template-python/generated_tests/conftest.py\n",
            "üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: 1Í∞ú @ Pruned_Base_Tests\n",
            "‚úÖ PASS   [BASE] test_run_worker.py (7.151s)\n",
            "üß™ ÏÉùÏÑ± ÌÖåÏä§Ìä∏ ÌååÏùº: 3Í∞ú @ generated_tests\n",
            "‚ùå FAIL   [GEN] test_gen_goal_1_1.py (rc=2, 6.723s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            "‚ùå FAIL   [GEN] test_gen_goal_4_1.py (rc=1, 6.718s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            "‚ùå FAIL   [GEN] test_gen_goal_4_2.py (rc=1, 6.76s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            "\n",
            "üì¶ Ïã§Ìñâ ÏöîÏïΩ: {\"pass\": 1, \"fail\": 3, \"timeout\": 0, \"total\": 4}\n",
            ">  coverage combine --rcfile /content/money-transfer-project-template-python/.coveragerc /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.__base__.test_run_worker.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_1_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_4_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_4_2.py\n",
            "‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\n",
            " - JSON : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.json\n",
            " - XML  : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.xml\n",
            " - HTML : /content/money-transfer-project-template-python/htmlcov_gen/index.html\n",
            "üßÆ Î∂ÑÍ∏∞ Í¥ÄÏ∏° ÏöîÏïΩ ‚Üí total:31, full:27, half:0, zero:4\n",
            "üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò: {\"executed_lines_delta\": 37, \"missing_lines_delta\": 0, \"base_executed\": 102, \"gen_executed\": 139, \"base_missing\": 54, \"gen_missing\": 54}\n",
            "‚úÖ 3-5 ÏôÑÎ£å: (ÌîÑÎ£®ÎãùÍ∏∞Ï°¥+ÏÉùÏÑ±) autosetup + Í≤©Î¶¨ Ïã§Ìñâ/ÏÉ§Îìú Í≤∞Ìï©/Î∂ÑÍ∏∞¬∑Î™©Ìëú¬∑Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-6) ÎØ∏Ïª§Î≤ÑÎùºÏù∏ Í∏∞Î∞ò Adaptive Refinement Round (NO goals version)\n",
        "\n",
        "import os, sys, json, re\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# ===== Paths =====\n",
        "assert 'PROJ' in globals(), \"3-0 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "COV_GEN = ART_DIR / \"coverage_gen.json\"\n",
        "\n",
        "REFINE_DIR = ART_DIR / f\"refine_round1\"\n",
        "REFINE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REFINE_PROMPTS = REFINE_DIR / \"llm_refine_prompts.jsonl\"\n",
        "REFINE_SUMMARY = REFINE_DIR / \"refine_summary.json\"\n",
        "\n",
        "def load_json(path):\n",
        "    try: return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    except: return None\n",
        "\n",
        "cov = load_json(COV_GEN)\n",
        "if not cov:\n",
        "    raise SystemExit(\"coverage_gen.jsonÏù¥ ÏóÜÏäµÎãàÎã§. 3-5 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
        "\n",
        "files_map = cov.get(\"files\", {}) or {}\n",
        "\n",
        "# ====== ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏ Í∏∞Î∞ò target mapping ======\n",
        "file_targets = {}\n",
        "for fpath, finfo in files_map.items():\n",
        "    miss = finfo.get(\"missing_lines\", []) or []\n",
        "    if miss:\n",
        "        file_targets[fpath] = sorted(miss)\n",
        "\n",
        "# ====== ÏÉùÏÑ±Îêú ÌÖåÏä§Ìä∏ Î™©Î°ù ======\n",
        "gen_tests = [p for p in GEN_DIR.glob(\"*.py\") if p.name != \"conftest.py\"]\n",
        "\n",
        "if not gen_tests:\n",
        "    print(\"ÏÉùÏÑ±Îêú ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ Î≥¥Í∞ï ÎùºÏö¥ÎìúÎ•º Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "# ====== Ïã§Ìå® ÌÖåÏä§Ìä∏ Í∞êÏßÄ ======\n",
        "def load_log_head(name):\n",
        "    out = (LOG_DIR / f\"{name}.out.txt\")\n",
        "    err = (LOG_DIR / f\"{name}.err.txt\")\n",
        "    o = out.read_text(encoding=\"utf-8\") if out.exists() else \"\"\n",
        "    e = err.read_text(encoding=\"utf-8\") if err.exists() else \"\"\n",
        "    return (o[:1500], e[:1500])\n",
        "\n",
        "def classify_failure(std, err):\n",
        "    blob = std + \"\\n\" + err\n",
        "    if \"AssertionError\" in blob: return \"assert\"\n",
        "    if \"ImportError\" in blob or \"ModuleNotFoundError\" in blob: return \"import\"\n",
        "    if \"TimeoutExpired\" in blob or \"timed out\" in blob: return \"timeout\"\n",
        "    if \"SyntaxError\" in blob: return \"syntax\"\n",
        "    if \"TypeError\" in blob: return \"type\"\n",
        "    return None\n",
        "\n",
        "selected = []\n",
        "\n",
        "for tfile in gen_tests:\n",
        "    name = tfile.name\n",
        "    code = tfile.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    # Ïã§Ìå® Ïó¨Î∂Ä\n",
        "    out_h, err_h = load_log_head(name)\n",
        "    ftype = classify_failure(out_h, err_h)\n",
        "\n",
        "    # Ïñ¥Îñ§ ÌååÏùºÏùÑ importlibÎ°ú Î°úÎìúÌïòÎäîÏßÄ Ï∂îÏ∂ú\n",
        "    imods = re.findall(r\"importlib\\.import_module\\(['\\\"]([^'\\\"]+)['\\\"]\\)\", code)\n",
        "\n",
        "    hit_any = False\n",
        "    target_info = []\n",
        "\n",
        "    for mod in imods:\n",
        "        for fpath, miss_lines in file_targets.items():\n",
        "            # Ìï¥Îãπ ÌÖåÏä§Ìä∏Í∞Ä Îã§Î£®Îäî ÌååÏùºÏù¥ÎùºÍ≥† Í∞ÄÏ†ï\n",
        "            target_info.append({\n",
        "                \"file\": fpath,\n",
        "                \"miss_lines\": miss_lines\n",
        "            })\n",
        "            if miss_lines:\n",
        "                hit_any = False\n",
        "\n",
        "    # Ïã§Ìå®Ìïú ÌÖåÏä§Ìä∏Îäî Ìï≠ÏÉÅ Î≥¥Í∞ï ÌõÑÎ≥¥Î°ú Ìè¨Ìï®\n",
        "    if ftype or target_info:\n",
        "        selected.append({\n",
        "            \"id\": name,\n",
        "            \"filename\": name,\n",
        "            \"failure\": ftype,\n",
        "            \"stdout\": out_h,\n",
        "            \"stderr\": err_h,\n",
        "            \"original_code\": code,\n",
        "            \"targets\": target_info,\n",
        "        })\n",
        "\n",
        "# ===== ÏÑ†ÌÉù ÏóÜÏúºÎ©¥ Ï¢ÖÎ£å =====\n",
        "if not selected:\n",
        "    REFINE_PROMPTS.write_text(\"\", encoding=\"utf-8\")\n",
        "    REFINE_SUMMARY.write_text(json.dumps({\n",
        "        \"round_dir\": REFINE_DIR.name,\n",
        "        \"selected\": 0,\n",
        "        \"reason\": \"ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞òÏúºÎ°ú Î≥¥Í∞ïÌï† ÌÖåÏä§Ìä∏ ÏóÜÏùå\"\n",
        "    }, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print(\"Î≥¥Í∞ïÌï† ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "# ====== LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ======\n",
        "SYSTEM = (\n",
        "\"ÎãπÏã†ÏùÄ pytest ÌÖåÏä§Ìä∏ ÌååÏùºÏùÑ Î≥¥Í∞ïÌï¥ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏ÏùÑ Ïã§ÌñâÌïòÍ≤å ÎßåÎìúÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.\\n\"\n",
        "\"Ï∂úÎ†•ÏùÄ ÎßàÌÅ¨Îã§Ïö¥ ÏóÜÏù¥ JSON Í∞ùÏ≤¥ ÌïòÎÇòÏûÖÎãàÎã§.\\n\"\n",
        "\"{\\n\"\n",
        "'  \"edits\": [ {\"id\": \"ÌååÏùºÎ™Ö\", \"new_code\": \"Î≥¥Í∞ïÎêú Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ ÏΩîÎìú\"} ]\\n'\n",
        "\"}\\n\"\n",
        "\"Í∑úÏπô:\\n\"\n",
        "\"‚Ä¢ test ÌååÏùº Ï†ÑÏ≤¥Î•º new_codeÏóê ÎÑ£Îêò ÏÑ§Î™Ö/Ï£ºÏÑù Í∏àÏßÄ.\\n\"\n",
        "\"‚Ä¢ importlib + getattr Î∞©Ïãù Ïú†ÏßÄ.\\n\"\n",
        "\"‚Ä¢ pytest.raises ÎòêÎäî assert Î∞òÎìúÏãú Ìè¨Ìï®.\\n\"\n",
        "\"‚Ä¢ ÎØ∏Ïª§Î≤Ñ ÎùºÏù∏(target_lines)ÏùÑ Ïã§Ï†úÎ°ú Ïã§ÌñâÌïòÍ≤å ÏûÖÎ†•Í∞í/ÌùêÎ¶ÑÏùÑ Ï°∞Ï†ï.\\n\"\n",
        "\"‚Ä¢ Ïô∏Î∂Ä I/O/net/time/Temporal ÏÇ¨Ïö© Í∏àÏßÄ, monkeypatch ÌóàÏö©.\\n\"\n",
        ")\n",
        "\n",
        "with REFINE_PROMPTS.open(\"w\", encoding=\"utf-8\") as fo:\n",
        "    for rec in selected:\n",
        "        user = {\n",
        "            \"schema_version\": \"refine-simple-v1\",\n",
        "            \"test_id\": rec[\"id\"],\n",
        "            \"filename\": rec[\"filename\"],\n",
        "            \"failure\": rec[\"failure\"],\n",
        "            \"targets\": rec[\"targets\"],\n",
        "            \"stdout_head\": rec[\"stdout\"],\n",
        "            \"stderr_head\": rec[\"stderr\"],\n",
        "            \"original_code\": rec[\"original_code\"],\n",
        "            \"instructions\": [\n",
        "                \"target_linesÎ•º Î∞òÎìúÏãú Ïã§ÌñâÌï† Í≤É.\",\n",
        "                \"Î∂àÌïÑÏöîÌïú skip Ï†úÍ±∞.\",\n",
        "                \"pytest Í≤ΩÍ≥ÑÍ∞í ÌÖåÏä§Ìä∏Î•º ÏÇ¨Ïö©Ìï¥ Ï°∞Í±¥ÏùÑ ÎßåÏ°±ÏãúÌÇ¨ Í≤É.\",\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        fo.write(json.dumps({\n",
        "            \"meta\": {\"filename\": rec[\"filename\"]},\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(user, ensure_ascii=False, indent=2)}\n",
        "            ]\n",
        "        }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "REFINE_SUMMARY.write_text(json.dumps({\n",
        "    \"round_dir\": REFINE_DIR.name,\n",
        "    \"selected\": len(selected),\n",
        "    \"tests\": [s[\"filename\"] for s in selected],\n",
        "}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò Î≥¥Í∞ï ÎùºÏö¥Îìú Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
        "print(\" - ÏÑ†ÌÉùÎêú ÌÖåÏä§Ìä∏:\", len(selected))\n",
        "print(\" - ÌîÑÎ°¨ÌîÑÌä∏:\", REFINE_PROMPTS)\n"
      ],
      "metadata": {
        "id": "TiTUpEZcBSxq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cb086ac3-5816-44c8-ea1a-84de2bfc4829"
      },
      "id": "TiTUpEZcBSxq",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÎØ∏Ïª§Î≤Ñ Í∏∞Î∞ò Î≥¥Í∞ï ÎùºÏö¥Îìú Ï§ÄÎπÑ ÏôÑÎ£å\n",
            " - ÏÑ†ÌÉùÎêú ÌÖåÏä§Ìä∏: 3\n",
            " - ÌîÑÎ°¨ÌîÑÌä∏: /content/money-transfer-project-template-python/run_artifacts/run1/refine_round1/llm_refine_prompts.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-7) ÎØ∏Ïª§Î≤ÑÎùºÏù∏ Í∏∞Î∞ò Î≥¥Í∞ï Ï†ÅÏö©Í∏∞ (NO goals, coverage-only version)\n",
        "\n",
        "import os, re, json, ast, shutil, time, subprocess, shlex, sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import httpx\n",
        "import backoff\n",
        "from lxml import etree\n",
        "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
        "\n",
        "# ===== Í∏∞Î≥∏ ÏÑ§Ï†ï =====\n",
        "MODEL = \"gpt-4o\"\n",
        "TIMEOUT_SINGLE = 30\n",
        "MIN_EXEC_GAIN = 3          # Ïã§Ìñâ ÎùºÏù∏ Ï¶ùÍ∞Ä ÏµúÏÜå Ï°∞Í±¥\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "\n",
        "# ===== Í≤ΩÎ°ú =====\n",
        "assert 'PROJ' in globals(), \"3-0 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "\n",
        "REFINE_DIRS = sorted([p for p in ART_DIR.iterdir() if p.name.startswith(\"refine_round\")])\n",
        "if not REFINE_DIRS:\n",
        "    raise SystemExit(\"refine_roundN Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§. 3-6 Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
        "REFINE_DIR = REFINE_DIRS[-1]\n",
        "\n",
        "PROMPTS = REFINE_DIR / \"llm_refine_prompts.jsonl\"\n",
        "if not PROMPTS.exists() or PROMPTS.stat().st_size == 0:\n",
        "    print(\"Î≥¥Í∞ï ÌîÑÎ°¨ÌîÑÌä∏ ÏóÜÏùå. Ï¢ÖÎ£å.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "RAW_DIR = REFINE_DIR / \"_raw_edits\"\n",
        "ERR_DIR = REFINE_DIR / \"_errors\"\n",
        "STAGE_DIR = REFINE_DIR / \"_staging\"\n",
        "ARCHIVE_DIR = REFINE_DIR / \"_archive\"\n",
        "REJECT_DIR = REFINE_DIR / \"_rejected\"\n",
        "\n",
        "for d in [RAW_DIR, ERR_DIR, STAGE_DIR, ARCHIVE_DIR, REJECT_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ===== API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ =====\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY missing\")\n",
        "\n",
        "http_client = httpx.Client(\n",
        "    timeout=180, follow_redirects=True,\n",
        "    limits=httpx.Limits(max_connections=1, max_keepalive_connections=0),\n",
        "    transport=httpx.HTTPTransport(retries=5),\n",
        ")\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), http_client=http_client)\n",
        "\n",
        "# ===== Í∏∞Î≥∏ Ïú†Ìã∏ =====\n",
        "def strip_fences(s):\n",
        "    s = re.sub(r\"^```[a-zA-Z0-9]*\", \"\", (s or \"\").strip())\n",
        "    s = re.sub(r\"```$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def sanitize_test_code(code):\n",
        "    patterns = [\n",
        "        re.compile(r\"(?ms)^\\s*if __name__ == ['\\\"]__main__['\\\"]:\\s*\\n.+$\"),\n",
        "        re.compile(r\"^\\s*pytest\\.main\\(.*?\\)$\", re.M),\n",
        "        re.compile(r\"^\\s*unittest\\.main\\(.*?\\)$\", re.M),\n",
        "    ]\n",
        "    out = code\n",
        "    for p in patterns:\n",
        "        out = p.sub(\"\", out)\n",
        "    return out.strip() + \"\\n\"\n",
        "\n",
        "# ===== Coverage Îã®Ïùº Ïã§Ìñâ =====\n",
        "def run_test_with_coverage(path: Path):\n",
        "    shard = STAGE_DIR / f\".coverage_{path.name}\"\n",
        "    out_json = STAGE_DIR / f\"{path.stem}.json\"\n",
        "    out_xml = STAGE_DIR / f\"{path.stem}.xml\"\n",
        "\n",
        "    for f in [shard, out_json, out_xml]:\n",
        "        if f.exists():\n",
        "            f.unlink()\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"PYTHONPATH\"] = f\"{PROJ}:{env.get('PYTHONPATH','')}\"\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "\n",
        "    cmd = f\"{sys.executable} -m coverage run -m pytest {PYTEST_FLAGS} {shlex.quote(str(path))}\"\n",
        "\n",
        "    try:\n",
        "        rc = subprocess.run(\n",
        "            cmd, shell=True, cwd=PROJ,\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "            timeout=TIMEOUT_SINGLE, text=True, env=env\n",
        "        )\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return {\"rc\": 124, \"missing\": {}, \"exec\": 0, \"branch_cov\": (0,0)}\n",
        "\n",
        "    # collect coverage\n",
        "    subprocess.run(f\"coverage json -o {out_json}\", shell=True, cwd=PROJ)\n",
        "    subprocess.run(f\"coverage xml -o {out_xml}\", shell=True, cwd=PROJ)\n",
        "\n",
        "    try:\n",
        "        cov = json.loads(out_json.read_text(encoding=\"utf-8\"))\n",
        "        files = cov.get(\"files\", {})\n",
        "    except:\n",
        "        return {\"rc\": rc.returncode, \"missing\": {}, \"exec\": 0, \"branch_cov\": (0,0)}\n",
        "\n",
        "    # missing lines\n",
        "    missing_map = {f: set(v.get(\"missing_lines\", [])) for f, v in files.items()}\n",
        "    exec_count = sum(len(v.get(\"executed_lines\",[])) for v in files.values())\n",
        "\n",
        "    # branch\n",
        "    covered = total = 0\n",
        "    try:\n",
        "        root = etree.parse(str(out_xml)).getroot()\n",
        "        for line in root.findall(\".//line[@branch='true']\"):\n",
        "            m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", line.get(\"condition-coverage\") or \"\")\n",
        "            if m:\n",
        "                covered += int(m.group(1))\n",
        "                total += int(m.group(2))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\"rc\": rc.returncode, \"missing\": missing_map,\n",
        "            \"exec\": exec_count, \"branch_cov\": (covered, total)}\n",
        "\n",
        "# ====== Í∞úÏÑ† Ïó¨Î∂Ä ÌåêÎã® ======\n",
        "def improved(base, cand, target_lines: set):\n",
        "    if cand[\"rc\"] != 0:\n",
        "        return False\n",
        "\n",
        "    # 1) missing Í∞êÏÜå\n",
        "    miss_base = len(target_lines & base[\"missing\"])\n",
        "    miss_cand = len(target_lines & cand[\"missing\"])\n",
        "    if miss_cand < miss_base:\n",
        "        return True\n",
        "\n",
        "    # 2) Ïã§Ìñâ ÎùºÏù∏ Ï¶ùÍ∞Ä\n",
        "    if cand[\"exec\"] >= base[\"exec\"] + MIN_EXEC_GAIN:\n",
        "        return True\n",
        "\n",
        "    # 3) branch hit Ï¶ùÍ∞Ä\n",
        "    bc_b, bt_b = base[\"branch_cov\"]\n",
        "    bc_c, bt_c = cand[\"branch_cov\"]\n",
        "    if bc_c > bc_b:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# ===== Main =====\n",
        "LOG = []\n",
        "with PROMPTS.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        user_payload = json.loads(rec[\"messages\"][1][\"content\"])\n",
        "\n",
        "        filename = user_payload[\"filename\"]\n",
        "        targets = user_payload[\"targets\"]\n",
        "        test_orig = GEN_DIR / filename\n",
        "\n",
        "        orig_code = test_orig.read_text(encoding=\"utf-8\") if test_orig.exists() else \"\"\n",
        "\n",
        "        # base metrics\n",
        "        base_m = run_test_with_coverage(test_orig) if orig_code else {\n",
        "            \"rc\": 1, \"missing\": {}, \"exec\": 0, \"branch_cov\": (0,0)\n",
        "        }\n",
        "\n",
        "        # call LLM\n",
        "        try:\n",
        "            out_raw = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=rec[\"messages\"],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "            ).choices[0].message.content\n",
        "        except Exception as e:\n",
        "            ERR_DIR.joinpath(f\"{filename}_request.json\").write_text(\n",
        "                json.dumps({\"error\": str(e)}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "            continue\n",
        "\n",
        "        RAW_DIR.joinpath(f\"{filename}_raw.json\").write_text(out_raw, encoding=\"utf-8\")\n",
        "\n",
        "        try:\n",
        "            result = json.loads(strip_fences(out_raw))\n",
        "            edit = result[\"edits\"][0]\n",
        "            new_code = sanitize_test_code(strip_fences(edit[\"new_code\"]))\n",
        "        except Exception as e:\n",
        "            ERR_DIR.joinpath(f\"{filename}_parse.json\").write_text(\n",
        "                json.dumps({\"error\": str(e), \"raw\": out_raw[:2000]}, ensure_ascii=False, indent=2),\n",
        "                encoding=\"utf-8\")\n",
        "            continue\n",
        "\n",
        "        # ÏûÑÏãú Ï†ÄÏû• ‚Üí ÌèâÍ∞Ä\n",
        "        stage_file = STAGE_DIR / filename\n",
        "        stage_file.write_text(new_code, encoding=\"utf-8\")\n",
        "\n",
        "        cand_m = run_test_with_coverage(stage_file)\n",
        "\n",
        "        # target_lines Í≥ÑÏÇ∞\n",
        "        target_lines = set()\n",
        "        for t in targets:\n",
        "            target_lines |= set(t[\"miss_lines\"])\n",
        "\n",
        "        # Í∞úÏÑ† Ïó¨Î∂Ä ÌåêÎã®\n",
        "        if improved(base_m, cand_m, target_lines):\n",
        "            # ÏàòÎùΩ ‚Üí ÏõêÎ≥∏ Î∞±ÏóÖ ÌõÑ ÍµêÏ≤¥\n",
        "            if test_orig.exists():\n",
        "                shutil.copy2(test_orig, ARCHIVE_DIR / f\"{filename}.{int(time.time())}.bak\")\n",
        "            test_orig.write_text(new_code, encoding=\"utf-8\")\n",
        "            LOG.append({\"file\": filename, \"decision\": \"ACCEPT\"})\n",
        "            print(f\"‚úÖ ACCEPT: {filename}\")\n",
        "        else:\n",
        "            REJECT_DIR.joinpath(filename).write_text(new_code, encoding=\"utf-8\")\n",
        "            LOG.append({\"file\": filename, \"decision\": \"REJECT\"})\n",
        "            print(f\"‚ùå REJECT: {filename}\")\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "(REFINE_DIR / \"apply_log.jsonl\").write_text(\n",
        "    \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in LOG),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ ÎØ∏Ïª§Î≤ÑÎùºÏù∏ Í∏∞Î∞ò Î≥¥Í∞ï Ï†ÅÏö© ÏôÑÎ£å\")\n"
      ],
      "metadata": {
        "id": "Qf0eqaFb-LH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3ff6f459-f582-41a6-9c79-6da814418674"
      },
      "id": "Qf0eqaFb-LH1",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå REJECT: test_gen_goal_4_1.py\n",
            "‚ùå REJECT: test_gen_goal_1_1.py\n",
            "‚ùå REJECT: test_gen_goal_4_2.py\n",
            "\n",
            "‚úÖ ÎØ∏Ïª§Î≤ÑÎùºÏù∏ Í∏∞Î∞ò Î≥¥Í∞ï Ï†ÅÏö© ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-8) ÌîÑÎ£®ÎãùÍ∏∞Ï°¥ + (ÏÉùÏÑ± + Î≥¥Í∞ï) Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ ÌÜµÌï© Ïã§Ìñâ ¬∑ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ¬∑ Ìñ•ÏÉÅÏπò ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\n",
        "\n",
        "import os, sys, json, re, time, subprocess, shutil, shlex\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from lxml import etree\n",
        "\n",
        "# ==== Í≤ΩÎ°ú/ÏÉÅÏàò ====\n",
        "assert 'PROJ' in globals(), \"3-0 Îã®Í≥ÑÎ•º Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\"\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "COV_SHARDS_DIR = ART_DIR / \"cov_shards\"\n",
        "HTML_DIR_GEN = PROJ / \"htmlcov_gen\"\n",
        "ACTIVE_MANIFEST = GEN_DIR / \"ACTIVE_MANIFEST.json\"\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "COV_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HTML_DIR_GEN.mkdir(parents=True, exist_ok=True)\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "PY_EXE = sys.executable\n",
        "TIMEOUT_SEC_GEN = 45\n",
        "TIMEOUT_SEC_BASE = 60\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "ENV_BASE = os.environ.copy()\n",
        "ENV_BASE[\"PYTHONPATH\"] = f\"{PROJ}:{ENV_BASE.get('PYTHONPATH','')}\"\n",
        "ENV_BASE.setdefault(\"NO_PROXY\", \"*\")\n",
        "\n",
        "RE_GOAL = re.compile(r\"(?:^|[_-])(?P<gid>\\d{4})(?:[_-]|$)\")\n",
        "\n",
        "def goal_id_from_name(name: str) -> str | None:\n",
        "    m = RE_GOAL.search(name)\n",
        "    return m.group(\"gid\") if m else None\n",
        "\n",
        "def sh(cmd: str, cwd: Path|None=None, timeout: int|None=None, env: dict|None=None):\n",
        "    try:\n",
        "        p = subprocess.run(cmd, cwd=str(cwd or PROJ), env=env or ENV_BASE,\n",
        "                           shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "                           timeout=timeout, text=True)\n",
        "        return p.returncode, p.stdout, p.stderr, False\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        return 124, e.stdout or \"\", e.stderr or \"\", True\n",
        "\n",
        "def rel_to_proj(p: Path) -> str:\n",
        "    try: return str(p.resolve().relative_to(PROJ))\n",
        "    except Exception: return str(p.resolve())\n",
        "\n",
        "# ==== Ïã§Ìñâ ÎåÄÏÉÅ Íµ¨ÏÑ± ====\n",
        "def list_pruned_tests() -> list[Path]:\n",
        "    if not PRUNED_DIR.exists():\n",
        "        return []\n",
        "    return sorted([p for p in PRUNED_DIR.glob(\"test*.py\") if p.is_file()])\n",
        "\n",
        "def list_all_generated_tests() -> list[Path]:\n",
        "    \"\"\"ACTIVE_MANIFESTÏôÄ Í¥ÄÍ≥ÑÏóÜÏù¥ generated_tests/*.py Ï†ÑÏ≤¥Î•º Ïã§Ìñâ.\"\"\"\n",
        "    return sorted([\n",
        "        p for p in GEN_DIR.glob(\"test*.py\")\n",
        "        if p.is_file() and p.name != \"conftest.py\"\n",
        "    ])\n",
        "\n",
        "# ==== 0) ÏÇ∞Ï∂úÎ¨º Í≤ΩÎ°ú ====\n",
        "results_jsonl = ART_DIR / \"results.jsonl\"\n",
        "coverage_json_path = ART_DIR / \"coverage_gen.json\"\n",
        "coverage_xml_path  = ART_DIR / \"coverage_gen.xml\"\n",
        "for old in [results_jsonl, coverage_json_path, coverage_xml_path]:\n",
        "    if old.exists(): old.unlink()\n",
        "\n",
        "runs = []\n",
        "ok = fail = to_cnt = 0\n",
        "\n",
        "# ==== 1) ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ ====\n",
        "base_files = list_pruned_tests()\n",
        "if base_files:\n",
        "    print(f\"üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: {len(base_files)}Í∞ú @ Pruned_Base_Tests/\")\n",
        "    for tf in base_files:\n",
        "        name = tf.name\n",
        "        shard = COV_SHARDS_DIR / f\".coverage.__base__.{name}\"\n",
        "        env = ENV_BASE.copy()\n",
        "        env[\"COVERAGE_FILE\"] = str(shard)\n",
        "        target = rel_to_proj(tf)\n",
        "        cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "        start = time.time()\n",
        "        rc, out, err, timed_out = sh(cmd, timeout=TIMEOUT_SEC_BASE, env=env)\n",
        "        dur = round(time.time()-start,3)\n",
        "        (LOG_DIR / f\"__base__{name}.out.txt\").write_text(out,encoding=\"utf-8\")\n",
        "        (LOG_DIR / f\"__base__{name}.err.txt\").write_text(err,encoding=\"utf-8\")\n",
        "        if timed_out:\n",
        "            to_cnt+=1; print(f\"‚è±Ô∏è TIMEOUT [BASE] {name} ({dur}s)\")\n",
        "        elif rc==0:\n",
        "            ok+=1; print(f\"‚úÖ PASS   [BASE] {name} ({dur}s)\")\n",
        "        else:\n",
        "            fail+=1; first=(err.strip().splitlines() or [''])[0]\n",
        "            print(f\"‚ùå FAIL   [BASE] {name} (rc={rc}, {dur}s) :: {first}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ BASE Ïã§Ìñâ Í±¥ÎÑàÎúÄ\")\n",
        "\n",
        "# ==== 2) ÏÉùÏÑ± + Î≥¥Í∞ï ÌÖåÏä§Ìä∏ Ï†ÑÏ≤¥ Ïã§Ìñâ (ACTIVE_MANIFEST Î¨¥Ïãú) ====\n",
        "gen_files = list_all_generated_tests()\n",
        "print(f\"üß™ ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ ÌååÏùº: {len(gen_files)}Í∞ú @ generated_tests/ (ACTIVE_MANIFEST Î¨¥Ïãú)\")\n",
        "\n",
        "for tf in gen_files:\n",
        "    name = tf.name\n",
        "    gid = goal_id_from_name(name) or \"----\"\n",
        "    shard = COV_SHARDS_DIR / f\".coverage.{name}\"\n",
        "    env = ENV_BASE.copy()\n",
        "    env[\"COVERAGE_FILE\"] = str(shard)\n",
        "    target = rel_to_proj(tf)\n",
        "    cmd = f\"{PY_EXE} -m coverage run{rc_opt} -m pytest {PYTEST_FLAGS} {shlex.quote(target)}\"\n",
        "    start = time.time()\n",
        "    rc,out,err,timed_out = sh(cmd,timeout=TIMEOUT_SEC_GEN,env=env)\n",
        "    dur = round(time.time()-start,3)\n",
        "    (LOG_DIR / f\"{name}.out.txt\").write_text(out,encoding=\"utf-8\")\n",
        "    (LOG_DIR / f\"{name}.err.txt\").write_text(err,encoding=\"utf-8\")\n",
        "    if timed_out:\n",
        "        to_cnt+=1; print(f\"‚è±Ô∏è TIMEOUT [GEN] {name} ({dur}s)\")\n",
        "    elif rc==0:\n",
        "        ok+=1; print(f\"‚úÖ PASS   [GEN] {name} ({dur}s)\")\n",
        "    else:\n",
        "        fail+=1; first=(err.strip().splitlines() or [''])[0]\n",
        "        print(f\"‚ùå FAIL   [GEN] {name} (rc={rc}, {dur}s) :: {first}\")\n",
        "\n",
        "# ==== 3) Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ====\n",
        "shards = [p for p in COV_SHARDS_DIR.iterdir() if p.name.startswith(\".coverage.\")]\n",
        "if not shards:\n",
        "    print(\"‚ö†Ô∏è Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÉ§ÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§. (Î™®Îì† ÌÖåÏä§Ìä∏ Ïã§Ìå® Í∞ÄÎä•ÏÑ±)\")\n",
        "else:\n",
        "    subprocess.call(f\"coverage erase{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    combine = \"coverage combine\"+rc_opt+\" \"+\" \".join(shlex.quote(str(p)) for p in shards)\n",
        "    print(\"> \",combine)\n",
        "    subprocess.call(combine,shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage json -o coverage_gen.json{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o coverage_gen.xml{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage html -d {HTML_DIR_GEN.name}{rc_opt}\",shell=True,cwd=str(PROJ))\n",
        "    shutil.copy2(PROJ/\"coverage_gen.json\",coverage_json_path)\n",
        "    shutil.copy2(PROJ/\"coverage_gen.xml\",coverage_xml_path)\n",
        "    print(\"‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\")\n",
        "    print(\" - JSON :\",coverage_json_path)\n",
        "    print(\" - XML  :\",coverage_xml_path)\n",
        "    print(\" - HTML :\",HTML_DIR_GEN/\"index.html\")\n",
        "\n",
        "# ==== 4) Î™©Ìëú Îã¨ÏÑ±Î•† Î∞è Ìñ•ÏÉÅÏπò Í≥ÑÏÇ∞ ====\n",
        "def load_json(p,default=None):\n",
        "    try: return json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    except Exception: return default\n",
        "base = load_json(ART_DIR/\"coverage_base.json\",{\"files\":{}}) or {\"files\":{}}\n",
        "gen  = load_json(coverage_json_path,{\"files\":{}}) or {\"files\":{}}\n",
        "def _sum_len(key,d): return sum(len((d.get(f, {}) or {}).get(key,[]) or []) for f in d)\n",
        "base_exec=_sum_len(\"executed_lines\",base[\"files\"])\n",
        "base_miss=_sum_len(\"missing_lines\",base[\"files\"])\n",
        "gen_exec=_sum_len(\"executed_lines\",gen[\"files\"])\n",
        "gen_miss=_sum_len(\"missing_lines\",gen[\"files\"])\n",
        "delta={\n",
        "    \"executed_lines_delta\":gen_exec-base_exec,\n",
        "    \"missing_lines_delta\": base_miss-gen_miss,\n",
        "    \"base_executed\":base_exec,\"gen_executed\":gen_exec,\n",
        "    \"base_missing\":base_miss,\"gen_missing\":gen_miss\n",
        "}\n",
        "(ART_DIR/\"coverage_delta.json\").write_text(json.dumps(delta,ensure_ascii=False,indent=2),encoding=\"utf-8\")\n",
        "print(\"üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò:\",json.dumps(delta,ensure_ascii=False))\n",
        "print(\"‚úÖ 3-8 ÏôÑÎ£å: Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ + Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\")\n"
      ],
      "metadata": {
        "id": "DCl4lEUj-aYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f32ed544-967e-4457-dda0-ea2f7d2cb523"
      },
      "id": "DCl4lEUj-aYm",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ ÌîÑÎ£®ÎãùÎêú Í∏∞Ï°¥ ÌÖåÏä§Ìä∏ ÌååÏùº: 1Í∞ú @ Pruned_Base_Tests/\n",
            "‚úÖ PASS   [BASE] test_run_worker.py (7.255s)\n",
            "üß™ ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ ÌååÏùº: 3Í∞ú @ generated_tests/ (ACTIVE_MANIFEST Î¨¥Ïãú)\n",
            "‚ùå FAIL   [GEN] test_gen_goal_1_1.py (rc=2, 6.207s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            "‚ùå FAIL   [GEN] test_gen_goal_4_1.py (rc=1, 7.565s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            "‚ùå FAIL   [GEN] test_gen_goal_4_2.py (rc=1, 6.787s) :: /usr/local/lib/python3.12/dist-packages/coverage/control.py:894: CoverageWarning: No data was collected. (no-data-collected)\n",
            ">  coverage combine --rcfile /content/money-transfer-project-template-python/.coveragerc /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_4_2.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_1_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.test_gen_goal_4_1.py /content/money-transfer-project-template-python/run_artifacts/run1/cov_shards/.coverage.__base__.test_run_worker.py\n",
            "‚úÖ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤∞Ìï© ÏôÑÎ£å\n",
            " - JSON : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.json\n",
            " - XML  : /content/money-transfer-project-template-python/run_artifacts/run1/coverage_gen.xml\n",
            " - HTML : /content/money-transfer-project-template-python/htmlcov_gen/index.html\n",
            "üìà Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ Ìñ•ÏÉÅÏπò: {\"executed_lines_delta\": 37, \"missing_lines_delta\": 0, \"base_executed\": 102, \"gen_executed\": 139, \"base_missing\": 54, \"gen_missing\": 54}\n",
            "‚úÖ 3-8 ÏôÑÎ£å: Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ Ïã§Ìñâ + Ïª§Î≤ÑÎ¶¨ÏßÄ ÏÇ∞Ï∂ú (ACTIVE_MANIFEST Î¨¥Ïãú Î≤ÑÏ†Ñ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-9 ÏµúÏ¢Ö Test Suite ÏÉùÏÑ± (ÎØ∏Ïª§Î≤ÑÎùºÏù∏ Í∏∞Î∞ò Ï†ÑÏö© Î≤ÑÏ†Ñ)\n",
        "\n",
        "from pathlib import Path\n",
        "import os, shutil, re\n",
        "\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "GEN_DIR = PROJ / \"generated_tests\"\n",
        "PRUNED_DIR = PROJ / \"Pruned_Base_Tests\"\n",
        "LOG_DIR = ART_DIR / \"logs\"\n",
        "\n",
        "FINAL_SUITE = PROJ / \"Final_Test_Suite\"\n",
        "\n",
        "# Reset\n",
        "if FINAL_SUITE.exists():\n",
        "    shutil.rmtree(FINAL_SUITE)\n",
        "FINAL_SUITE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def is_pass_log(text):\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    if \"fail\" in text or \"failed\" in text:\n",
        "        return False\n",
        "    if \"error\" in text:\n",
        "        return False\n",
        "\n",
        "    # pytest -q Ìå®ÌÑ¥\n",
        "    if \"passed\" in text:\n",
        "        return True\n",
        "\n",
        "    # .ss.s. Ïä§ÌÉÄÏùº\n",
        "    if re.fullmatch(r\"[\\.s]+\", text):\n",
        "        if \"f\" not in text and \"e\" not in text:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "pass_generated = []\n",
        "\n",
        "# 1) ÏÉùÏÑ±¬∑Î≥¥Í∞ï ÌÖåÏä§Ìä∏ PASS ÌåêÏ†ï\n",
        "for out_path in LOG_DIR.glob(\"*.py.out.txt\"):     # ‚òÖ Ï†ÑÏ≤¥ test ÌååÏùº ÎåÄÏÉÅ\n",
        "    name = out_path.name.replace(\".out.txt\", \"\")\n",
        "    if not name.startswith(\"test\"):               # ‚òÖ ÏïàÏ†Ñ ÌïÑÌÑ∞\n",
        "        continue\n",
        "\n",
        "    err_path = LOG_DIR / f\"{name}.err.txt\"\n",
        "\n",
        "    out_text = out_path.read_text(encoding=\"utf-8\")\n",
        "    err_text = err_path.read_text(encoding=\"utf-8\") if err_path.exists() else \"\"\n",
        "\n",
        "    # FAIL/ERROR anywhere ‚Üí Ï†úÏô∏\n",
        "    if \"fail\" in out_text.lower() or \"fail\" in err_text.lower():\n",
        "        continue\n",
        "    if \"error\" in out_text.lower() or \"error\" in err_text.lower():\n",
        "        continue\n",
        "\n",
        "    if is_pass_log(out_text):\n",
        "        if (GEN_DIR / name).exists():\n",
        "            pass_generated.append(name)\n",
        "\n",
        "\n",
        "# 2) Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏ Î≥µÏÇ¨\n",
        "base_tests = list(PRUNED_DIR.glob(\"test*.py\"))\n",
        "for tf in base_tests:\n",
        "    shutil.copy2(tf, FINAL_SUITE / tf.name)\n",
        "\n",
        "\n",
        "# 3) PASS ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏ Î≥µÏÇ¨\n",
        "for fname in pass_generated:\n",
        "    shutil.copy2(GEN_DIR / fname, FINAL_SUITE / fname)\n",
        "\n",
        "print(\"‚úÖ FINAL TEST SUITE CONSTRUCTED\")\n",
        "print(\"Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏:\", len(base_tests))\n",
        "print(\"PASS ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏:\", len(pass_generated))\n",
        "print(\"‚Üí ÏµúÏ¢Ö Ìè¥Îçî:\", FINAL_SUITE)\n"
      ],
      "metadata": {
        "id": "CNbeY8Sl-lwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a67df967-629c-4011-eb50-2cc0b8f87962"
      },
      "id": "CNbeY8Sl-lwo",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FINAL TEST SUITE CONSTRUCTED\n",
            "Í∏∞Ï°¥ ÌîÑÎ£®Îãù ÌÖåÏä§Ìä∏: 1\n",
            "PASS ÏÉùÏÑ±/Î≥¥Í∞ï ÌÖåÏä§Ìä∏: 0\n",
            "‚Üí ÏµúÏ¢Ö Ìè¥Îçî: /content/money-transfer-project-template-python/Final_Test_Suite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-10) ÏµúÏ¢Ö ÏÑ±Îä• Î∂ÑÏÑù ‚Äì ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò + ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤Ñ ÎùºÏù∏/Ìï®Ïàò Î∂ÑÏÑù\n",
        "\n",
        "import os, json, subprocess, shlex, re, sys\n",
        "from pathlib import Path\n",
        "from lxml import etree\n",
        "\n",
        "PROJ = Path(PROJ).resolve()\n",
        "ART_DIR = PROJ / \"run_artifacts\" / \"run1\"\n",
        "FINAL_SUITE = PROJ / \"Final_Test_Suite\"\n",
        "ORIG_TESTS = PROJ / \"tests\"\n",
        "\n",
        "RCFILE = PROJ / \".coveragerc\"\n",
        "rc_opt = f\" --rcfile {RCFILE}\" if RCFILE.exists() else \"\"\n",
        "PYTEST_FLAGS = \"-q -s\"\n",
        "PY_EXE = sys.executable\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Ïú†Ìã∏ Ìï®ÏàòÎì§\n",
        "# =====================================================\n",
        "def sh(cmd, cwd=None):\n",
        "    p = subprocess.run(\n",
        "        cmd, cwd=cwd or PROJ, shell=True,\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return p.returncode, p.stdout, p.stderr\n",
        "\n",
        "\n",
        "def get_test_count(path):\n",
        "    \"\"\"pytest --collect-only Î°ú ÌÖåÏä§Ìä∏ Í∞úÏàò Ï†ïÌôïÌûà ÏàòÏßë\"\"\"\n",
        "    rc, out, err = sh(f\"{PY_EXE} -m pytest --collect-only {path}\")\n",
        "    text = out + err\n",
        "    m = re.search(r\"collected\\s+(\\d+)\\s+items\", text)\n",
        "    return int(m.group(1)) if m else 0\n",
        "\n",
        "\n",
        "def erase_cov():\n",
        "    for nm in [\".coverage\", \"coverage.json\", \"coverage.xml\"]:\n",
        "        p = PROJ / nm\n",
        "        if p.exists():\n",
        "            p.unlink()\n",
        "\n",
        "\n",
        "def export_cov(json_name, xml_name):\n",
        "    subprocess.call(f\"coverage json -o {json_name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "    subprocess.call(f\"coverage xml  -o {xml_name}{rc_opt}\", shell=True, cwd=str(PROJ))\n",
        "\n",
        "\n",
        "def load_cov_json(path):\n",
        "    return json.load(open(PROJ / path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def parse_branch(xml_path):\n",
        "    root = etree.parse(str(PROJ / xml_path)).getroot()\n",
        "    hit = tot = 0\n",
        "    for ln in root.findall(\".//line[@branch='true']\"):\n",
        "        cc = ln.get(\"condition-coverage\") or \"\"\n",
        "        m = re.search(r\"\\((\\d+)\\s*/\\s*(\\d+)\\)\", cc)\n",
        "        if m:\n",
        "            hit += int(m.group(1))\n",
        "            tot += int(m.group(2))\n",
        "    return hit, tot\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# (ÏàòÎèô Î∂ÑÎ™®) Í∏∞Ï§Ä Î∂ÑÎ™® ÏÑ§Ï†ï\n",
        "# =====================================================\n",
        "print(\"üìå Í∏∞Ï§Ä Î∂ÑÎ™® Ï∏°Ï†ï ÏãúÏûë...\")\n",
        "\n",
        "TOTAL_LINES = 235\n",
        "TOTAL_BRANCHES = 72\n",
        "\n",
        "print(f\"üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• ÎùºÏù∏ : {TOTAL_LINES}\")\n",
        "print(f\"üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• Î∏åÎûúÏπò : {TOTAL_BRANCHES}\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ÏµúÏ¥à ÌÖåÏä§Ìä∏ Ïã§Ìñâ\n",
        "# =====================================================\n",
        "print(\"\\nüìå ÏµúÏ¥à tests/ Ïã§Ìñâ Ï§ë...\")\n",
        "\n",
        "orig_test_count = get_test_count(\"tests/\")\n",
        "\n",
        "erase_cov()\n",
        "rc, out, err = sh(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest -q -s tests/\")\n",
        "export_cov(\"cov_orig.json\", \"cov_orig.xml\")\n",
        "\n",
        "cov_orig = load_cov_json(\"cov_orig.json\")\n",
        "orig_executed = sum(len(v.get(\"executed_lines\", [])) for v in cov_orig[\"files\"].values())\n",
        "BR_O_HIT, BR_O_TOT = parse_branch(\"cov_orig.xml\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Final_Test_Suite Ïã§Ìñâ\n",
        "# =====================================================\n",
        "print(\"\\nüìå Final_Test_Suite Ïã§Ìñâ Ï§ë...\")\n",
        "\n",
        "final_test_count = get_test_count(str(FINAL_SUITE))\n",
        "\n",
        "erase_cov()\n",
        "rc2, out2, err2 = sh(f\"{PY_EXE} -m coverage run{rc_opt} -m pytest -q -s {FINAL_SUITE}\")\n",
        "export_cov(\"cov_final.json\", \"cov_final.xml\")\n",
        "\n",
        "cov_final = load_cov_json(\"cov_final.json\")\n",
        "final_executed = sum(len(v.get(\"executed_lines\", [])) for v in cov_final[\"files\"].values())\n",
        "BR_F_HIT, BR_F_TOT = parse_branch(\"cov_final.xml\")\n",
        "\n",
        "BR_DEN = TOTAL_BRANCHES\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Ïã†Í∑ú ÎùºÏù∏ Î∂ÑÏÑù\n",
        "# =====================================================\n",
        "orig_lines = {\n",
        "    f\"{f}:{l}\"\n",
        "    for f, info in cov_orig[\"files\"].items()\n",
        "    for l in info.get(\"executed_lines\", [])\n",
        "}\n",
        "final_lines = {\n",
        "    f\"{f}:{l}\"\n",
        "    for f, info in cov_final[\"files\"].items()\n",
        "    for l in info.get(\"executed_lines\", [])\n",
        "}\n",
        "\n",
        "new_lines = sorted(list(final_lines - orig_lines))\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Î∂ÑÏÑù\n",
        "# =====================================================\n",
        "print(\"\\nüìå ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤ÑÎ¶¨ÏßÄ Î∂ÑÏÑù Ï§ë...\")\n",
        "\n",
        "test_cover_detail = {}\n",
        "\n",
        "for fpath in cov_final[\"files\"]:\n",
        "    if \"/generated_tests/\" not in fpath and \"/Final_Test_Suite/\" not in fpath:\n",
        "        continue\n",
        "\n",
        "    info = cov_final[\"files\"][fpath]\n",
        "    executed = info.get(\"executed_lines\", [])\n",
        "    funcs = set()\n",
        "\n",
        "    source_path = PROJ / fpath\n",
        "    if source_path.exists():\n",
        "        import ast\n",
        "        tree = ast.parse(source_path.read_text(encoding=\"utf-8\"))\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                start = node.lineno\n",
        "                end = max([n.lineno for n in ast.walk(node) if hasattr(n, \"lineno\")], default=start)\n",
        "                if any(start <= l <= end for l in executed):\n",
        "                    funcs.add(node.name)\n",
        "\n",
        "    test_cover_detail[fpath] = {\n",
        "        \"executed_lines\": executed,\n",
        "        \"covered_functions\": sorted(list(funcs))\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Í≤∞Í≥º Ï∂úÎ†•\n",
        "# =====================================================\n",
        "print(\"\\nüìä 3-10 ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º\\n\")\n",
        "\n",
        "print(\"1Ô∏è‚É£ Ïã§ÌñâÎêú ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò\")\n",
        "print(f\" - ÏµúÏ¥à tests/: {orig_test_count}Í∞ú\")\n",
        "print(f\" - ÏµúÏ¢Ö Final_Test_Suite: {final_test_count}Í∞ú\")\n",
        "print(f\" - Œî ÌÖåÏä§Ìä∏ Ï¶ùÍ∞Ä: {final_test_count - orig_test_count}Í∞ú\\n\")\n",
        "\n",
        "print(\"2Ô∏è‚É£ ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\")\n",
        "print(f\" - ÏµúÏ¥à: {round(orig_executed/TOTAL_LINES*100,2)}% ({orig_executed}/{TOTAL_LINES})\")\n",
        "print(f\" - ÏµúÏ¢Ö: {round(final_executed/TOTAL_LINES*100,2)}% ({final_executed}/{TOTAL_LINES})\")\n",
        "print(f\" - Œî ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ: {round((final_executed-orig_executed)/TOTAL_LINES*100,2)}%p\\n\")\n",
        "\n",
        "print(\"3Ô∏è‚É£ Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\")\n",
        "print(f\" - ÏµúÏ¥à: {round(BR_O_HIT/BR_DEN*100,2)}% ({BR_O_HIT}/{BR_DEN})\")\n",
        "print(f\" - ÏµúÏ¢Ö: {round(BR_F_HIT/BR_DEN*100,2)}% ({BR_F_HIT}/{BR_DEN})\")\n",
        "print(f\" - Œî Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ: {round((BR_F_HIT-BR_O_HIT)/BR_DEN*100,2)}%p\\n\")\n",
        "\n",
        "print(\"4Ô∏è‚É£ ÏÉàÎ°≠Í≤å Ïã§ÌñâÎêú ÏÜåÏä§ÏΩîÎìú ÎùºÏù∏\")\n",
        "print(f\" - Ï¥ù {len(new_lines)}Í∞ú Ï¶ùÍ∞Ä\")\n",
        "for ln in new_lines[:40]:\n",
        "    print(\"   \", ln)\n",
        "if len(new_lines) > 40:\n",
        "    print(\"   ... ÏÉùÎûµ ...\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Ïª§Î≤ÑÎ¶¨ÏßÄ\\n\")\n",
        "for f, detail in test_cover_detail.items():\n",
        "    print(f\"üìÑ {f}\")\n",
        "    print(f\" - Ïª§Î≤Ñ ÎùºÏù∏ Ïàò: {len(detail['executed_lines'])}\")\n",
        "    print(f\" - Ïª§Î≤Ñ Ìï®Ïàò: {detail['covered_functions']}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ 3-10 Ï†ÑÏ≤¥ Î∂ÑÏÑù ÏôÑÎ£å (ÏàòÎèô Î∂ÑÎ™® Î≤ÑÏ†Ñ)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AjhNqNQ9tA9X",
        "outputId": "3602a06d-6780-4280-c7c4-cafab040053e"
      },
      "id": "AjhNqNQ9tA9X",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Í∏∞Ï§Ä Î∂ÑÎ™® Ï∏°Ï†ï ÏãúÏûë...\n",
            "üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• ÎùºÏù∏ : 235\n",
            "üìå Í∏∞Ï§Ä Ïã§ÌñâÍ∞ÄÎä• Î∏åÎûúÏπò : 72\n",
            "\n",
            "üìå ÏµúÏ¥à tests/ Ïã§Ìñâ Ï§ë...\n",
            "\n",
            "üìå Final_Test_Suite Ïã§Ìñâ Ï§ë...\n",
            "\n",
            "üìå ÌÖåÏä§Ìä∏Î≥Ñ Ïª§Î≤ÑÎ¶¨ÏßÄ Î∂ÑÏÑù Ï§ë...\n",
            "\n",
            "üìä 3-10 ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º\n",
            "\n",
            "1Ô∏è‚É£ Ïã§ÌñâÎêú ÌÖåÏä§Ìä∏ Ìï®Ïàò Ïàò\n",
            " - ÏµúÏ¥à tests/: 0Í∞ú\n",
            " - ÏµúÏ¢Ö Final_Test_Suite: 0Í∞ú\n",
            " - Œî ÌÖåÏä§Ìä∏ Ï¶ùÍ∞Ä: 0Í∞ú\n",
            "\n",
            "2Ô∏è‚É£ ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\n",
            " - ÏµúÏ¥à: 43.4% (102/235)\n",
            " - ÏµúÏ¢Ö: 59.15% (139/235)\n",
            " - Œî ÎùºÏù∏ Ïª§Î≤ÑÎ¶¨ÏßÄ: 15.74%p\n",
            "\n",
            "3Ô∏è‚É£ Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ (Í∏∞Ï§Ä Î∂ÑÎ™® ÎåÄÎπÑ)\n",
            " - ÏµúÏ¥à: 44.44% (32/72)\n",
            " - ÏµúÏ¢Ö: 75.0% (54/72)\n",
            " - Œî Î∏åÎûúÏπò Ïª§Î≤ÑÎ¶¨ÏßÄ: 30.56%p\n",
            "\n",
            "4Ô∏è‚É£ ÏÉàÎ°≠Í≤å Ïã§ÌñâÎêú ÏÜåÏä§ÏΩîÎìú ÎùºÏù∏\n",
            " - Ï¥ù 37Í∞ú Ï¶ùÍ∞Ä\n",
            "    Final_Test_Suite/test_run_worker.py:10\n",
            "    Final_Test_Suite/test_run_worker.py:11\n",
            "    Final_Test_Suite/test_run_worker.py:14\n",
            "    Final_Test_Suite/test_run_worker.py:15\n",
            "    Final_Test_Suite/test_run_worker.py:16\n",
            "    Final_Test_Suite/test_run_worker.py:17\n",
            "    Final_Test_Suite/test_run_worker.py:18\n",
            "    Final_Test_Suite/test_run_worker.py:2\n",
            "    Final_Test_Suite/test_run_worker.py:24\n",
            "    Final_Test_Suite/test_run_worker.py:25\n",
            "    Final_Test_Suite/test_run_worker.py:31\n",
            "    Final_Test_Suite/test_run_worker.py:37\n",
            "    Final_Test_Suite/test_run_worker.py:4\n",
            "    Final_Test_Suite/test_run_worker.py:40\n",
            "    Final_Test_Suite/test_run_worker.py:41\n",
            "    Final_Test_Suite/test_run_worker.py:42\n",
            "    Final_Test_Suite/test_run_worker.py:43\n",
            "    Final_Test_Suite/test_run_worker.py:44\n",
            "    Final_Test_Suite/test_run_worker.py:5\n",
            "    Final_Test_Suite/test_run_worker.py:51\n",
            "    Final_Test_Suite/test_run_worker.py:52\n",
            "    Final_Test_Suite/test_run_worker.py:58\n",
            "    Final_Test_Suite/test_run_worker.py:59\n",
            "    Final_Test_Suite/test_run_worker.py:6\n",
            "    Final_Test_Suite/test_run_worker.py:66\n",
            "    Final_Test_Suite/test_run_worker.py:7\n",
            "    Final_Test_Suite/test_run_worker.py:70\n",
            "    Final_Test_Suite/test_run_worker.py:71\n",
            "    Final_Test_Suite/test_run_worker.py:72\n",
            "    Final_Test_Suite/test_run_worker.py:73\n",
            "    Final_Test_Suite/test_run_worker.py:74\n",
            "    Final_Test_Suite/test_run_worker.py:81\n",
            "    Final_Test_Suite/test_run_worker.py:82\n",
            "    Final_Test_Suite/test_run_worker.py:88\n",
            "    Final_Test_Suite/test_run_worker.py:89\n",
            "    Final_Test_Suite/test_run_worker.py:9\n",
            "    Final_Test_Suite/test_run_worker.py:96\n",
            "\n",
            "5Ô∏è‚É£ ÌÖåÏä§Ìä∏Î≥Ñ ÎùºÏù∏/Ìï®Ïàò Ïª§Î≤ÑÎ¶¨ÏßÄ\n",
            "\n",
            "\n",
            "‚úÖ 3-10 Ï†ÑÏ≤¥ Î∂ÑÏÑù ÏôÑÎ£å (ÏàòÎèô Î∂ÑÎ™® Î≤ÑÏ†Ñ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "proj = pathlib.Path(PROJ)\n",
        "zip_path = proj / \"My_Research_experiment_outputs_NoCompositeVer.zip\"\n",
        "\n",
        "# ÏïïÏ∂ï ÎåÄÏÉÅ: run_artifacts, generated_tests, htmlcov, reports\n",
        "targets = [\"run_artifacts\", \"generated_tests\", \"reports\", \"htmlcov\", \"Final_Test_Suite\"]\n",
        "\n",
        "# ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞Î•º zipÏóê Îã¥Í∏∞ ÏúÑÌï¥ ÏÉà Ìè¥Îçî Íµ¨ÏÑ±\n",
        "temp_root = proj / \"_zip_bundle\"\n",
        "if temp_root.exists():\n",
        "    shutil.rmtree(temp_root)\n",
        "temp_root.mkdir()\n",
        "\n",
        "for name in targets:\n",
        "    src = proj / name\n",
        "    if src.exists():\n",
        "        shutil.copytree(src, temp_root / name)\n",
        "\n",
        "# ZIP ÏÉùÏÑ±\n",
        "shutil.make_archive(str(zip_path.with_suffix(\"\")), 'zip', temp_root)\n",
        "\n",
        "print(\"ÏïïÏ∂ï ÏÉùÏÑ± ÏôÑÎ£å:\", zip_path)\n"
      ],
      "metadata": {
        "id": "faebpWtAtmq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fe6ae785-14fa-4906-ac31-0f530c2ba269"
      },
      "id": "faebpWtAtmq3",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏïïÏ∂ï ÏÉùÏÑ± ÏôÑÎ£å: /content/money-transfer-project-template-python/My_Research_experiment_outputs_NoCompositeVer.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kww1LnA4RfHu"
      },
      "id": "Kww1LnA4RfHu",
      "execution_count": null,
      "outputs": []
    }
  ]
}